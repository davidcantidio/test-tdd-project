 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/streamlit_extension/utils/memoization.py b/streamlit_extension/utils/memoization.py
new file mode 100644
index 0000000000000000000000000000000000000000..f0a6cbe062cd091d295c2027193240e175ea3af5
--- /dev/null
+++ b/streamlit_extension/utils/memoization.py
@@ -0,0 +1,400 @@
+"""
+Streamlit Memoization Layer
+Enhanced caching system for improved performance.
+"""
+
+import streamlit as st
+import hashlib
+import json
+import time
+import logging
+from functools import wraps
+from typing import Any, Callable, Optional, Dict, Tuple, Union
+from datetime import datetime, timedelta
+from dataclasses import dataclass
+from collections import defaultdict
+from enum import Enum
+
+logger = logging.getLogger(__name__)
+
+
+class MemoizationConfig:
+    """Configuration for memoization behavior."""
+
+    # Cache TTL settings (in seconds)
+    TTL_SHORT = 60  # 1 minute for volatile data
+    TTL_MEDIUM = 600  # 10 minutes for semi-stable data
+    TTL_LONG = 3600  # 1 hour for stable data
+    TTL_VERY_LONG = 86400  # 24 hours for static data
+
+    # Cache size limits
+    MAX_ENTRIES_DEFAULT = 128
+    MAX_ENTRIES_LARGE = 512
+
+    # Cache strategies
+    STRATEGY_LRU = "lru"  # Least Recently Used
+    STRATEGY_TTL = "ttl"  # Time To Live
+    STRATEGY_HYBRID = "hybrid"  # LRU + TTL
+
+
+def memoize_data(
+    ttl: Optional[int] = MemoizationConfig.TTL_MEDIUM,
+    max_entries: int = MemoizationConfig.MAX_ENTRIES_DEFAULT,
+    show_spinner: bool = True,
+    spinner_text: str = "Loading...",
+    hash_funcs: Optional[Dict[type, Callable]] = None
+):
+    """
+    Decorator for memoizing data operations.
+    Uses st.cache_data under the hood with enhanced features.
+
+    Args:
+        ttl: Time to live in seconds
+        max_entries: Maximum cache entries
+        show_spinner: Show loading spinner
+        spinner_text: Text for spinner
+        hash_funcs: Custom hash functions for complex types
+    """
+
+    def decorator(func: Callable) -> Callable:
+        @st.cache_data(
+            ttl=ttl,
+            max_entries=max_entries,
+            show_spinner=show_spinner,
+            hash_funcs=hash_funcs
+        )
+        @wraps(func)
+        def wrapper(*args, **kwargs):
+            start_time = time.time()
+            try:
+                result = func(*args, **kwargs)
+                duration = time.time() - start_time
+                logger.debug(f"Cached function {func.__name__} executed in {duration:.2f}s")
+                return result
+            except Exception as e:
+                logger.error(f"Error in cached function {func.__name__}: {e}")
+                raise
+
+        # Add cache management methods
+        wrapper.clear = lambda: st.cache_data.clear()
+        wrapper.stats = lambda: get_cache_stats(func.__name__)
+
+        return wrapper
+
+    return decorator
+
+
+def memoize_resource(
+    ttl: Optional[int] = None,  # Resources don't expire by default
+    show_spinner: bool = True,
+    spinner_text: str = "Initializing resource...",
+    validate: Optional[Callable] = None
+):
+    """
+    Decorator for memoizing resource initialization.
+    Uses st.cache_resource for database connections, models, etc.
+
+    Args:
+        ttl: Optional TTL for resource
+        show_spinner: Show loading spinner
+        spinner_text: Text for spinner
+        validate: Optional validation function for resource
+    """
+
+    def decorator(func: Callable) -> Callable:
+        @st.cache_resource(
+            ttl=ttl,
+            show_spinner=show_spinner
+        )
+        @wraps(func)
+        def wrapper(*args, **kwargs):
+            resource = func(*args, **kwargs)
+
+            # Validate resource if validator provided
+            if validate and not validate(resource):
+                logger.warning(f"Resource validation failed for {func.__name__}")
+                st.cache_resource.clear()
+                resource = func(*args, **kwargs)
+
+            return resource
+
+        wrapper.clear = lambda: st.cache_resource.clear()
+        return wrapper
+
+    return decorator
+
+
+class CacheKeyGenerator:
+    """Generate intelligent cache keys for complex objects."""
+
+    @staticmethod
+    def generate_key(*args, **kwargs) -> str:
+        """Generate unique cache key from arguments."""
+        key_parts = []
+
+        # Handle positional arguments
+        for arg in args:
+            key_parts.append(CacheKeyGenerator._hash_object(arg))
+
+        # Handle keyword arguments (sorted for consistency)
+        for key in sorted(kwargs.keys()):
+            key_parts.append(f"{key}:{CacheKeyGenerator._hash_object(kwargs[key])}")
+
+        # Combine and hash
+        combined = "|".join(key_parts)
+        return hashlib.sha256(combined.encode()).hexdigest()[:16]
+
+    @staticmethod
+    def _hash_object(obj: Any) -> str:
+        """Hash any object consistently."""
+        if isinstance(obj, (str, int, float, bool, type(None))):
+            return str(obj)
+        elif isinstance(obj, (list, tuple)):
+            return f"[{','.join(CacheKeyGenerator._hash_object(item) for item in obj)}]"
+        elif isinstance(obj, dict):
+            items = sorted(obj.items())
+            return f"{{{','.join(f'{k}:{CacheKeyGenerator._hash_object(v)}' for k, v in items)}}}"
+        elif hasattr(obj, '__dict__'):
+            # Custom objects
+            return CacheKeyGenerator._hash_object(obj.__dict__)
+        else:
+            # Fallback to JSON-based hash (SECURITY FIX: replaced pickle+MD5 with JSON+SHA256)
+            try:
+                json_str = json.dumps(obj, sort_keys=True, default=str)
+                return hashlib.sha256(json_str.encode()).hexdigest()[:8]
+            except Exception:
+                return str(id(obj))
+
+
+@dataclass
+class CacheStatistics:
+    """Statistics for cache performance monitoring."""
+    hits: int = 0
+    misses: int = 0
+    evictions: int = 0
+    total_size: int = 0
+    avg_computation_time: float = 0.0
+    last_accessed: Optional[datetime] = None
+
+    @property
+    def hit_rate(self) -> float:
+        """Calculate cache hit rate."""
+        total = self.hits + self.misses
+        return (self.hits / total * 100) if total > 0 else 0.0
+
+
+class CacheMonitor:
+    """Monitor and track cache performance."""
+
+    def __init__(self):
+        self.stats = defaultdict(CacheStatistics)
+
+    def record_hit(self, cache_name: str):
+        """Record a cache hit."""
+        self.stats[cache_name].hits += 1
+        self.stats[cache_name].last_accessed = datetime.now()
+
+    def record_miss(self, cache_name: str, computation_time: float):
+        """Record a cache miss."""
+        stats = self.stats[cache_name]
+        stats.misses += 1
+        stats.last_accessed = datetime.now()
+
+        # Update average computation time
+        total = stats.hits + stats.misses
+        stats.avg_computation_time = (
+            (stats.avg_computation_time * (total - 1) + computation_time) / total
+        )
+
+    def get_report(self) -> str:
+        """Generate cache performance report."""
+        report_lines = ["Cache Performance Report", "=" * 50]
+
+        for cache_name, stats in self.stats.items():
+            report_lines.extend([
+                f"\nCache: {cache_name}",
+                f"  Hit Rate: {stats.hit_rate:.1f}%",
+                f"  Hits: {stats.hits}, Misses: {stats.misses}",
+                f"  Avg Computation Time: {stats.avg_computation_time:.3f}s",
+                f"  Last Accessed: {stats.last_accessed}"
+            ])
+
+        return "\n".join(report_lines)
+
+
+# Global cache monitor instance
+cache_monitor = CacheMonitor()
+
+
+# Database query memoization
+@memoize_data(ttl=MemoizationConfig.TTL_SHORT)
+def get_clients_cached(db_manager, include_inactive: bool = False):
+    """Cached version of get_clients."""
+    return db_manager.get_clients(include_inactive=include_inactive)
+
+
+@memoize_data(ttl=MemoizationConfig.TTL_SHORT)
+def get_projects_cached(db_manager, client_id: Optional[int] = None):
+    """Cached version of get_projects."""
+    return db_manager.get_projects(client_id=client_id)
+
+
+@memoize_data(ttl=MemoizationConfig.TTL_MEDIUM)
+def get_epic_analytics_cached(db_manager, epic_id: int):
+    """Cached version of epic analytics (expensive operation)."""
+    return db_manager.get_epic_analytics(epic_id)
+
+
+# Resource initialization memoization
+@memoize_resource()
+def get_database_connection():
+    """Get cached database connection."""
+    from streamlit_extension.utils.database import DatabaseManager
+    from streamlit_extension.config import load_config
+
+    config = load_config()
+    return DatabaseManager(
+        framework_db_path=str(config.get_database_path()),
+        timer_db_path=str(config.get_timer_database_path())
+    )
+
+
+# Heavy computation memoization
+@memoize_data(ttl=MemoizationConfig.TTL_LONG)
+def calculate_project_metrics(project_id: int) -> Dict[str, Any]:
+    """Calculate expensive project metrics."""
+    # This would contain expensive calculations
+    pass
+
+
+@memoize_data(ttl=MemoizationConfig.TTL_VERY_LONG)
+def get_historical_analytics(start_date: str, end_date: str):
+    """Get historical analytics (very expensive, rarely changes)."""
+    # Historical data analysis
+    pass
+
+
+class CacheInvalidator:
+    """Manage cache invalidation strategies."""
+
+    @staticmethod
+    def invalidate_on_write(entity_type: str):
+        """Decorator to invalidate cache after write operations."""
+
+        def decorator(func: Callable) -> Callable:
+            @wraps(func)
+            def wrapper(*args, **kwargs):
+                result = func(*args, **kwargs)
+
+                # Invalidate related caches
+                if entity_type == "client":
+                    get_clients_cached.clear()
+                elif entity_type == "project":
+                    get_projects_cached.clear()
+                    get_clients_cached.clear()  # Projects affect client metrics
+                elif entity_type == "epic":
+                    get_epic_analytics_cached.clear()
+
+                logger.info(f"Cache invalidated for {entity_type} after write operation")
+                return result
+
+            return wrapper
+
+        return decorator
+
+    @staticmethod
+    def invalidate_all():
+        """Clear all caches."""
+        st.cache_data.clear()
+        st.cache_resource.clear()
+        logger.warning("All caches cleared")
+
+
+# Session-Specific Caching
+def memoize_session(
+    key: str,
+    ttl: Optional[int] = None
+):
+    """
+    Memoize data in session state (user-specific).
+
+    Args:
+        key: Unique key for session storage
+        ttl: Optional TTL in seconds
+    """
+
+    def decorator(func: Callable) -> Callable:
+        @wraps(func)
+        def wrapper(*args, **kwargs):
+            # Generate cache key
+            cache_key = f"cache_{key}_{CacheKeyGenerator.generate_key(*args, **kwargs)}"
+            timestamp_key = f"{cache_key}_timestamp"
+
+            # Check if cached value exists and is valid
+            if cache_key in st.session_state:
+                if ttl is None:
+                    # No TTL, return cached value
+                    cache_monitor.record_hit(f"session_{key}")
+                    return st.session_state[cache_key]
+
+                # Check TTL
+                if timestamp_key in st.session_state:
+                    elapsed = time.time() - st.session_state[timestamp_key]
+                    if elapsed < ttl:
+                        cache_monitor.record_hit(f"session_{key}")
+                        return st.session_state[cache_key]
+
+            # Cache miss, compute value
+            start_time = time.time()
+            result = func(*args, **kwargs)
+            computation_time = time.time() - start_time
+
+            # Store in session
+            st.session_state[cache_key] = result
+            if ttl is not None:
+                st.session_state[timestamp_key] = time.time()
+
+            cache_monitor.record_miss(f"session_{key}", computation_time)
+            return result
+
+        # Add clear method
+        wrapper.clear = lambda: clear_session_cache(key)
+        return wrapper
+
+    return decorator
+
+
+def clear_session_cache(pattern: Optional[str] = None):
+    """Clear session cache entries."""
+    if pattern is None:
+        # Clear all cache entries
+        keys_to_remove = [k for k in st.session_state.keys() if k.startswith("cache_")]
+    else:
+        # Clear entries matching pattern
+        keys_to_remove = [k for k in st.session_state.keys()
+                          if k.startswith(f"cache_{pattern}")]
+
+    for key in keys_to_remove:
+        del st.session_state[key]
+
+    logger.info(f"Cleared {len(keys_to_remove)} session cache entries")
+
+
+# Utility functions
+def get_cache_stats(cache_name: Optional[str] = None) -> Dict[str, Any]:
+    """Get cache statistics."""
+    if cache_name:
+        return cache_monitor.stats.get(cache_name, CacheStatistics()).__dict__
+    return {name: stats.__dict__ for name, stats in cache_monitor.stats.items()}
+
+
+__all__ = [
+    'memoize_data',
+    'memoize_resource',
+    'memoize_session',
+    'CacheInvalidator',
+    'clear_session_cache',
+    'get_cache_stats',
+    'cache_monitor',
+    'MemoizationConfig'
+]
+
+
+# Initialize cache system
+def init_memoization():
+    """Initialize memoization system."""
+    logger.info("Memoization system initialized")
+
+    # Set up periodic cache maintenance
+    if 'cache_initialized' not in st.session_state:
+        st.session_state.cache_initialized = True
+        logger.info("Session cache initialized")
+
+
+# Auto-initialize on import
+init_memoization()
+
+EOF
)