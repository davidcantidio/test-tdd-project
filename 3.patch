 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/scripts/health_check.py b/scripts/health_check.py
new file mode 100644
index 0000000000000000000000000000000000000000..7201def1cfc90d726ff284b7ab318ae86a2c38a2
--- /dev/null
+++ b/scripts/health_check.py
@@ -0,0 +1,14 @@
+"""Standalone script that prints a basic health payload."""
+from __future__ import annotations
+
+import json
+from streamlit_extension.endpoints import HealthCheckEndpoint
+
+
+def main() -> None:
+    endpoint = HealthCheckEndpoint()
+    print(json.dumps(endpoint.basic_health()))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/streamlit_extension/endpoints/__init__.py b/streamlit_extension/endpoints/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..d65962ce2ee933e441a620b619bd18c223b91bf2
--- /dev/null
+++ b/streamlit_extension/endpoints/__init__.py
@@ -0,0 +1,8 @@
+"""Endpoint modules for Streamlit extension."""
+from .health import HealthCheckEndpoint, HealthStatus, ComponentChecker
+
+__all__ = [
+    "HealthCheckEndpoint",
+    "HealthStatus",
+    "ComponentChecker",
+]
diff --git a/streamlit_extension/endpoints/health.py b/streamlit_extension/endpoints/health.py
new file mode 100644
index 0000000000000000000000000000000000000000..f0a1947e65f9fc113c37cb2d42d5491354698bcf
--- /dev/null
+++ b/streamlit_extension/endpoints/health.py
@@ -0,0 +1,145 @@
+"""Health check endpoints for orchestration tools.
+
+This module provides simple health-check utilities that can be used by
+monitoring systems or orchestration tools such as Kubernetes.  The
+implementation intentionally keeps the logic lightweight – the goal of
+these utilities is to offer predictable structures that are easy to
+consume in tests and during development.
+"""
+
+from __future__ import annotations
+
+from dataclasses import dataclass
+from datetime import datetime, timedelta
+from enum import Enum
+import time
+from typing import Dict, Tuple
+
+
+class HealthStatus(str, Enum):
+    """Enum representing component health states."""
+
+    HEALTHY = "healthy"
+    DEGRADED = "degraded"
+    UNHEALTHY = "unhealthy"
+    MAINTENANCE = "maintenance"
+
+
+@dataclass
+class ComponentChecker:
+    """Collection of helper methods used by the health endpoints.
+
+    The real project would integrate with databases, caches and other
+    dependencies.  For the purposes of the tests we implement simple
+    placeholders that return deterministic data which can easily be
+    monkey‑patched to simulate different scenarios.
+    """
+
+    def check_database(self) -> Dict[str, str]:
+        return {"status": HealthStatus.HEALTHY.value, "response_time": "1ms"}
+
+    def check_cache(self) -> Dict[str, str]:
+        return {
+            "status": HealthStatus.HEALTHY.value,
+            "hit_rate": "100%",
+            "memory_usage": "0%",
+        }
+
+    def check_external_apis(self) -> Dict[str, str]:
+        return {"status": HealthStatus.HEALTHY.value, "latency": "0ms"}
+
+    def check_disk_space(self) -> Dict[str, str]:
+        return {"status": HealthStatus.HEALTHY.value, "disk_usage": "0%"}
+
+    def check_memory_usage(self) -> Dict[str, str]:
+        return {
+            "status": HealthStatus.HEALTHY.value,
+            "cpu_usage": "0%",
+            "memory_usage": "0%",
+        }
+
+
+class HealthCheckEndpoint:
+    """Implements the different health‑check endpoints."""
+
+    def __init__(self, start_time: float | None = None, version: str = "1.0.0", checker: ComponentChecker | None = None) -> None:
+        self.start_time = start_time or time.time()
+        self.version = version
+        self.checker = checker or ComponentChecker()
+
+    # ------------------------------------------------------------------
+    # Helpers
+    def _current_timestamp(self) -> str:
+        return datetime.utcnow().isoformat() + "Z"
+
+    # ------------------------------------------------------------------
+    def basic_health(self) -> Dict[str, str]:
+        """Return a very small health payload."""
+
+        uptime_seconds = int(time.time() - self.start_time)
+        uptime = str(timedelta(seconds=uptime_seconds))
+        return {
+            "status": HealthStatus.HEALTHY.value,
+            "timestamp": self._current_timestamp(),
+            "uptime": uptime,
+            "version": self.version,
+        }
+
+    def detailed_health(self) -> Dict[str, object]:
+        """Return detailed component information.
+
+        The overall status is derived from the status of individual
+        components – ``unhealthy`` takes precedence over ``degraded``
+        which in turn takes precedence over ``healthy``.
+        """
+
+        database = self.checker.check_database()
+        cache = self.checker.check_cache()
+        memory = self.checker.check_memory_usage()
+        disk = self.checker.check_disk_space()
+
+        system = {
+            "cpu_usage": memory.get("cpu_usage", "0%"),
+            "memory_usage": memory.get("memory_usage", "0%"),
+            "disk_usage": disk.get("disk_usage", "0%"),
+        }
+
+        components = {
+            "database": database,
+            "cache": cache,
+            "system": system,
+        }
+
+        statuses = [database.get("status"), cache.get("status"), memory.get("status"), disk.get("status")]
+        overall = HealthStatus.HEALTHY.value
+        if HealthStatus.UNHEALTHY.value in statuses:
+            overall = HealthStatus.UNHEALTHY.value
+        elif HealthStatus.DEGRADED.value in statuses:
+            overall = HealthStatus.DEGRADED.value
+
+        return {
+            "status": overall,
+            "timestamp": self._current_timestamp(),
+            "components": components,
+        }
+
+    def readiness_check(self) -> Tuple[int, Dict[str, object]]:
+        """Readiness probe suitable for Kubernetes."""
+
+        db = self.checker.check_database()
+        cache = self.checker.check_cache()
+        ready = db.get("status") == HealthStatus.HEALTHY.value and cache.get("status") == HealthStatus.HEALTHY.value
+        status_code = 200 if ready else 503
+        payload = {
+            "ready": ready,
+            "dependencies": {
+                "database": "connected" if db.get("status") == HealthStatus.HEALTHY.value else "unavailable",
+                "cache": "available" if cache.get("status") == HealthStatus.HEALTHY.value else "unavailable",
+            },
+        }
+        return status_code, payload
+
+    def liveness_check(self) -> Tuple[int, Dict[str, object]]:
+        """Simple liveness probe."""
+
+        return 200, {"alive": True, "last_heartbeat": self._current_timestamp()}
diff --git a/streamlit_extension/utils/graceful_shutdown.py b/streamlit_extension/utils/graceful_shutdown.py
new file mode 100644
index 0000000000000000000000000000000000000000..d62e508746e53fc5fe01574bb66bdc0d2bae4f5a
--- /dev/null
+++ b/streamlit_extension/utils/graceful_shutdown.py
@@ -0,0 +1,53 @@
+"""Simple graceful shutdown helpers.
+
+This is a lightweight abstraction that mimics the behaviour of a more
+sophisticated shutdown handler.  It is intentionally small – the
+existing project already contains a comprehensive implementation in the
+``monitoring`` package.  This variant exists so that components within
+``streamlit_extension`` can depend on a minimal interface during tests.
+"""
+
+from __future__ import annotations
+
+from dataclasses import dataclass
+from enum import Enum, auto
+from typing import Callable, Dict, List
+
+
+class ShutdownPhase(Enum):
+    STOP_ACCEPTING_REQUESTS = auto()
+    DRAIN_CONNECTIONS = auto()
+    CLEANUP_RESOURCES = auto()
+    FINAL_SHUTDOWN = auto()
+
+
+@dataclass
+class GracefulShutdownHandler:
+    """Coordinate a simple shutdown sequence."""
+
+    cleanup_handlers: Dict[str, Callable[[], None]]
+
+    def __init__(self) -> None:
+        self.cleanup_handlers = {}
+        self._sequence: List[ShutdownPhase] = []
+
+    def register_signal_handlers(self) -> None:  # pragma: no cover - OS interaction
+        """Placeholder for installing signal handlers."""
+        self._sequence.append(ShutdownPhase.STOP_ACCEPTING_REQUESTS)
+
+    def shutdown_sequence(self) -> List[ShutdownPhase]:
+        self._sequence.append(ShutdownPhase.DRAIN_CONNECTIONS)
+        self._sequence.append(ShutdownPhase.CLEANUP_RESOURCES)
+        self.cleanup_resources()
+        self._sequence.append(ShutdownPhase.FINAL_SHUTDOWN)
+        return list(self._sequence)
+
+    def cleanup_resources(self) -> None:
+        for handler in self.cleanup_handlers.values():
+            handler()
+
+    def wait_for_completion(self) -> bool:
+        return True
+
+    def add_cleanup_handler(self, name: str, handler: Callable[[], None]) -> None:
+        self.cleanup_handlers[name] = handler
diff --git a/streamlit_extension/utils/monitoring.py b/streamlit_extension/utils/monitoring.py
new file mode 100644
index 0000000000000000000000000000000000000000..f40fa79310dba4d3970ffd4875df5fccbd9c1be8
--- /dev/null
+++ b/streamlit_extension/utils/monitoring.py
@@ -0,0 +1,115 @@
+"""Light‑weight monitoring helpers used for unit tests.
+
+The goal of this module is to provide a minimal yet functional monitoring
+framework.  It gathers basic metrics, evaluates them against configured
+thresholds and can export the data in a Prometheus friendly plain text
+format.  The implementation is intentionally simple so that tests can
+exercise behaviour without needing heavy dependencies or external
+services.
+"""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Dict, List
+import os
+import shutil
+
+
+# ---------------------------------------------------------------------------
+# Metrics collection
+
+@dataclass
+class MetricsCollector:
+    """Collect basic system and application metrics."""
+
+    def cpu_usage(self) -> float:
+        # ``os.getloadavg`` is not available on Windows; fall back to 0.0
+        try:
+            load1, _, _ = os.getloadavg()
+            # normalise to a 0-100 percentage assuming 1 CPU
+            return float(min(load1 * 100.0, 100.0))
+        except OSError:  # pragma: no cover - platform specific
+            return 0.0
+
+    def memory_usage(self) -> float:
+        # Process memory usage via ``resource`` would be possible but is
+        # avoided to keep things portable.  Instead we use the total
+        # virtual memory from ``os.sysconf`` when available.
+        try:
+            used = os.sysconf("SC_PAGE_SIZE") * os.sysconf("SC_PHYS_PAGES")
+            return float(used / 1024 / 1024)
+        except (ValueError, AttributeError, OSError):  # pragma: no cover
+            return 0.0
+
+    def disk_usage(self) -> float:
+        usage = shutil.disk_usage("/")
+        return float(usage.used) / float(usage.total) * 100.0
+
+    def connection_pool_stats(self) -> Dict[str, int]:
+        return {"active": 0, "idle": 0, "max": 0}
+
+    def response_time_metrics(self) -> Dict[str, float]:
+        return {"average_ms": 0.0, "p95_ms": 0.0}
+
+
+# ---------------------------------------------------------------------------
+# Alerting
+
+@dataclass
+class AlertManager:
+    """Basic threshold based alert manager."""
+
+    thresholds: Dict[str, float] = field(default_factory=dict)
+    notifications: List[str] = field(default_factory=list)
+
+    def define_thresholds(self, thresholds: Dict[str, float]) -> None:
+        self.thresholds.update(thresholds)
+
+    def check_alerts(self, metrics: Dict[str, float]) -> List[str]:
+        alerts: List[str] = []
+        for key, value in metrics.items():
+            threshold = self.thresholds.get(key)
+            if threshold is not None and value > threshold:
+                alerts.append(f"{key} threshold exceeded: {value} > {threshold}")
+        return alerts
+
+    def send_notifications(self, alerts: List[str]) -> None:
+        self.notifications.extend(alerts)
+
+
+# ---------------------------------------------------------------------------
+# System monitor facade
+
+@dataclass
+class SystemMonitor:
+    """Facade coordinating metrics collection and alerting."""
+
+    collector: MetricsCollector = field(default_factory=MetricsCollector)
+    alert_manager: AlertManager = field(default_factory=AlertManager)
+
+    def collect_metrics(self) -> Dict[str, float]:
+        return {
+            "cpu_usage": self.collector.cpu_usage(),
+            "memory_usage": self.collector.memory_usage(),
+            "disk_usage": self.collector.disk_usage(),
+        }
+
+    def check_performance(self) -> Dict[str, str]:
+        metrics = self.collect_metrics()
+        status: Dict[str, str] = {}
+        for key, value in metrics.items():
+            threshold = self.alert_manager.thresholds.get(key, float("inf"))
+            status[key] = "healthy" if value <= threshold else "degraded"
+        return status
+
+    def generate_alerts(self) -> List[str]:
+        metrics = self.collect_metrics()
+        alerts = self.alert_manager.check_alerts(metrics)
+        self.alert_manager.send_notifications(alerts)
+        return alerts
+
+    def export_metrics(self) -> str:
+        metrics = self.collect_metrics()
+        # Export in a simple Prometheus-style text format
+        return "\n".join(f"{key} {value}" for key, value in metrics.items())
diff --git a/streamlit_extension/utils/structured_logging.py b/streamlit_extension/utils/structured_logging.py
new file mode 100644
index 0000000000000000000000000000000000000000..857f7bf0d128c5776037e12ca4193e0524225020
--- /dev/null
+++ b/streamlit_extension/utils/structured_logging.py
@@ -0,0 +1,89 @@
+"""Small structured logging helper.
+
+This module implements a tiny structured logger capable of producing JSON
+formatted log records.  It is deliberately simplified but provides the
+hooks required for tests: correlation identifiers, performance metrics
+and dedicated security/business event helpers.
+"""
+
+from __future__ import annotations
+
+import json
+import logging
+from dataclasses import dataclass
+from datetime import datetime
+from enum import Enum
+from typing import Any, Dict
+
+
+class LogFormat(str, Enum):
+    JSON = "json"
+    GELF = "gelf"
+    PLAIN_TEXT = "plain_text"
+
+
+class LogLevel(int, Enum):
+    DEBUG = logging.DEBUG
+    INFO = logging.INFO
+    WARNING = logging.WARNING
+    ERROR = logging.ERROR
+    CRITICAL = logging.CRITICAL
+
+
+class _JSONFormatter(logging.Formatter):
+    def format(self, record: logging.LogRecord) -> str:  # pragma: no cover - trivial
+        data = {
+            "timestamp": datetime.utcnow().isoformat() + "Z",
+            "level": record.levelname,
+            "logger": record.name,
+            "message": record.getMessage(),
+        }
+        for field in ("correlation_id", "performance", "security", "business"):
+            if hasattr(record, field):
+                data[field] = getattr(record, field)
+        return json.dumps(data)
+
+
+@dataclass
+class StructuredLogger:
+    """Wrapper around :mod:`logging` providing helper methods."""
+
+    name: str = "app"
+    level: LogLevel = LogLevel.INFO
+    fmt: LogFormat = LogFormat.JSON
+
+    def __post_init__(self) -> None:
+        self.logger = logging.getLogger(self.name)
+        self.logger.setLevel(self.level.value)
+        handler = logging.StreamHandler()
+        formatter: logging.Formatter
+        if self.fmt is LogFormat.JSON:
+            formatter = _JSONFormatter()
+        else:  # pragma: no cover - not used in tests
+            formatter = logging.Formatter("%(levelname)s: %(message)s")
+        handler.setFormatter(formatter)
+        self.logger.handlers = [handler]
+
+    # ------------------------------------------------------------------
+    def log_with_correlation(self, level: LogLevel, message: str, correlation_id: str) -> None:
+        extra = {"correlation_id": correlation_id}
+        self.logger.log(level.value, message, extra=extra)
+
+    def log_performance(self, message: str, **metrics: Any) -> None:
+        extra = {"performance": metrics}
+        self.logger.info(message, extra=extra)
+
+    def log_security_event(self, message: str, **details: Any) -> None:
+        extra = {"security": details}
+        self.logger.warning(message, extra=extra)
+
+    def log_business_event(self, message: str, **details: Any) -> None:
+        extra = {"business": details}
+        self.logger.info(message, extra=extra)
+
+
+__all__ = [
+    "StructuredLogger",
+    "LogFormat",
+    "LogLevel",
+]
diff --git a/tests/test_health_endpoint.py b/tests/test_health_endpoint.py
new file mode 100644
index 0000000000000000000000000000000000000000..9d4b4881d3331514ca22d5cd6a24ea65e710bc7f
--- /dev/null
+++ b/tests/test_health_endpoint.py
@@ -0,0 +1,57 @@
+import sys
+from pathlib import Path
+sys.path.append(str(Path(__file__).resolve().parents[1]))
+from streamlit_extension.endpoints import HealthCheckEndpoint, HealthStatus, ComponentChecker
+
+
+def test_basic_health_check():
+    endpoint = HealthCheckEndpoint()
+    data = endpoint.basic_health()
+    assert data["status"] == HealthStatus.HEALTHY.value
+    assert "timestamp" in data and "uptime" in data
+
+
+def test_detailed_health_check():
+    endpoint = HealthCheckEndpoint()
+    data = endpoint.detailed_health()
+    assert data["status"] == HealthStatus.HEALTHY.value
+    components = data["components"]
+    assert set(["database", "cache", "system"]).issubset(components.keys())
+
+
+def test_readiness_check():
+    endpoint = HealthCheckEndpoint()
+    code, payload = endpoint.readiness_check()
+    assert code == 200
+    assert payload["ready"] is True
+
+
+def test_liveness_check():
+    endpoint = HealthCheckEndpoint()
+    code, payload = endpoint.liveness_check()
+    assert code == 200
+    assert payload["alive"] is True
+
+
+def test_health_check_with_database_down(monkeypatch):
+    def failed_db(self):
+        return {"status": HealthStatus.UNHEALTHY.value}
+
+    monkeypatch.setattr(ComponentChecker, "check_database", failed_db, raising=False)
+    endpoint = HealthCheckEndpoint()
+    data = endpoint.detailed_health()
+    assert data["status"] == HealthStatus.UNHEALTHY.value
+
+
+def test_health_check_with_degraded_performance(monkeypatch):
+    def degraded_memory(self):
+        return {
+            "status": HealthStatus.DEGRADED.value,
+            "cpu_usage": "95%",
+            "memory_usage": "95%",
+        }
+
+    monkeypatch.setattr(ComponentChecker, "check_memory_usage", degraded_memory, raising=False)
+    endpoint = HealthCheckEndpoint()
+    data = endpoint.detailed_health()
+    assert data["status"] == HealthStatus.DEGRADED.value
diff --git a/tests/test_monitoring.py b/tests/test_monitoring.py
new file mode 100644
index 0000000000000000000000000000000000000000..2658d528c5bb03a4f7d28f74de0dfead746083c5
--- /dev/null
+++ b/tests/test_monitoring.py
@@ -0,0 +1,42 @@
+import sys
+from pathlib import Path
+import importlib.util
+
+ROOT = Path(__file__).resolve().parents[1]
+module_path = ROOT / "streamlit_extension" / "utils" / "monitoring.py"
+spec = importlib.util.spec_from_file_location("streamlit_extension.utils.monitoring", module_path)
+monitoring = importlib.util.module_from_spec(spec)
+sys.modules[spec.name] = monitoring
+spec.loader.exec_module(monitoring)
+SystemMonitor = monitoring.SystemMonitor
+MetricsCollector = monitoring.MetricsCollector
+AlertManager = monitoring.AlertManager
+
+
+def test_metrics_collection():
+    monitor = SystemMonitor()
+    metrics = monitor.collect_metrics()
+    assert set(["cpu_usage", "memory_usage", "disk_usage"]).issubset(metrics.keys())
+
+
+def test_performance_monitoring():
+    monitor = SystemMonitor()
+    monitor.alert_manager.define_thresholds({"cpu_usage": 1000, "memory_usage": 1e9, "disk_usage": 1000})
+    status = monitor.check_performance()
+    assert status["cpu_usage"] == "healthy"
+    assert status["memory_usage"] == "healthy"
+
+
+def test_alert_generation():
+    collector = MetricsCollector()
+    monitor = SystemMonitor(collector=collector, alert_manager=AlertManager())
+    monitor.alert_manager.define_thresholds({"cpu_usage": 0})
+    alerts = monitor.generate_alerts()
+    assert alerts
+    assert monitor.alert_manager.notifications == alerts
+
+
+def test_metrics_export():
+    monitor = SystemMonitor()
+    export = monitor.export_metrics()
+    assert "cpu_usage" in export
 
EOF
)