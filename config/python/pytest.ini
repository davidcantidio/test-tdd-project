[tool:pytest]
# Basic pytest configuration for TDD projects

# Minimum version required
minversion = 7.0

# Test discovery paths
testpaths = 
    tests
    src

# Python files to consider as tests
python_files = 
    test_*.py
    *_test.py
    tests.py

# Python classes to consider as test classes  
python_classes = 
    Test*
    *Tests
    *TestCase

# Python functions to consider as tests
python_functions = 
    test_*

# Default command line options
addopts = 
    # Verbose output with test names
    -v
    
    # Show local variables in tracebacks
    --tb=short
    
    # Coverage reporting
    --cov=src
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-fail-under=90
    
    # Show durations of slowest tests
    --durations=10
    
    # Strict mode - treat warnings as errors
    --strict-markers
    --strict-config
    
    # Colored output
    --color=yes
    
    # Show captured output for failed tests
    --capture=no
    
    # Exit after first failure (useful for TDD red phase)
    # --exitfirst  # Uncomment for red phase development
    
    # Run tests in parallel (uncomment if needed)
    # -n auto

# Test markers for categorization
markers =
    # TDD phases
    red: RED phase - tests that should fail initially
    green: GREEN phase - tests that should pass after implementation  
    refactor: REFACTOR phase - tests that should remain green during refactoring
    
    # Test types
    unit: Unit tests - test individual functions/classes in isolation
    integration: Integration tests - test interaction between components
    e2e: End-to-end tests - test complete user workflows
    
    # Performance and speed
    slow: Slow tests - may take more than 1 second
    fast: Fast tests - should complete in under 100ms
    performance: Performance tests - measure speed/memory usage
    
    # Environment specific
    database: Tests that require database connection
    network: Tests that require network/internet access
    api: Tests that call external APIs
    
    # Epic and feature specific  
    epic: Tests related to specific epics
    feature: Tests for specific features
    
    # Quality gates
    smoke: Smoke tests - basic functionality checks
    regression: Regression tests - prevent previously fixed bugs
    
    # Platform specific
    windows: Tests that only run on Windows
    linux: Tests that only run on Linux  
    macos: Tests that only run on macOS

# Test timeout (in seconds)
timeout = 300
timeout_method = thread

# Warning filters
filterwarnings =
    # Treat warnings as errors by default
    error
    
    # Ignore specific warnings
    ignore::UserWarning
    ignore::DeprecationWarning:django.*
    ignore::PendingDeprecationWarning
    
    # Ignore warnings from third-party libraries
    ignore::DeprecationWarning:plotly.*
    ignore::UserWarning:pandas.*

# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Log file configuration  
log_file = tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_file_date_format = %Y-%m-%d %H:%M:%S

# Test session scoped fixtures
console_output_style = progress

# Empty parameter sets behavior
empty_parameter_set_mark = skip

# Consider these directories for test collection
collect_ignore = [
    "setup.py",
    "conftest.py"
]

# Minimum required coverage percentage (can be overridden by --cov-fail-under)
required_coverage = 90

# TDD specific configuration
# Uncomment these for specific TDD phases:

# RED Phase - Stop on first failure
# addopts = -x --tb=short --no-cov

# GREEN Phase - Run all tests with coverage
# addopts = -v --cov=src --cov-report=term-missing

# REFACTOR Phase - Run with performance monitoring
# addopts = -v --cov=src --durations=5 --benchmark-skip