 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/streamlit_extension/utils/constants.py b/streamlit_extension/utils/constants.py
new file mode 100644
index 0000000000000000000000000000000000000000..f3c0e7a1dba0ff300477162d81e591b135b77248
--- /dev/null
+++ b/streamlit_extension/utils/constants.py
@@ -0,0 +1,117 @@
+from enum import Enum, IntEnum
+from typing import Final
+
+class TableNames:
+    """Database table names."""
+    CLIENTS: Final[str] = "framework_clients"
+    PROJECTS: Final[str] = "framework_projects"
+    EPICS: Final[str] = "framework_epics"
+    TASKS: Final[str] = "framework_tasks"
+    USERS: Final[str] = "framework_users"
+    WORK_SESSIONS: Final[str] = "work_sessions"
+    ACHIEVEMENTS: Final[str] = "user_achievements"
+    STREAKS: Final[str] = "user_streaks"
+
+
+class FieldNames:
+    """Common database field names."""
+    ID: Final[str] = "id"
+    NAME: Final[str] = "name"
+    EMAIL: Final[str] = "email"
+    STATUS: Final[str] = "status"
+    CREATED_AT: Final[str] = "created_at"
+    UPDATED_AT: Final[str] = "updated_at"
+    CLIENT_ID: Final[str] = "client_id"
+    PROJECT_ID: Final[str] = "project_id"
+    EPIC_ID: Final[str] = "epic_id"
+
+
+class ClientStatus(Enum):
+    """Client status values."""
+    ACTIVE = "active"
+    INACTIVE = "inactive"
+    PROSPECT = "prospect"
+    ARCHIVED = "archived"
+
+
+class ProjectStatus(Enum):
+    """Project status values."""
+    PLANNING = "planning"
+    ACTIVE = "active"
+    ON_HOLD = "on_hold"
+    COMPLETED = "completed"
+    CANCELLED = "cancelled"
+
+
+class TaskStatus(Enum):
+    """Task status values."""
+    TODO = "todo"
+    IN_PROGRESS = "in_progress"
+    TESTING = "testing"
+    DONE = "done"
+    BLOCKED = "blocked"
+
+
+class EpicStatus(Enum):
+    """Epic status values."""
+    DRAFT = "draft"
+    READY = "ready"
+    IN_PROGRESS = "in_progress"
+    REVIEW = "review"
+    DONE = "done"
+
+
+class TDDPhase(Enum):
+    """TDD development phases."""
+    RED = "red"
+    GREEN = "green"
+    REFACTOR = "refactor"
+    COMPLETE = "complete"
+
+
+class Priority(IntEnum):
+    """Priority levels for tasks and epics."""
+    LOW = 1
+    MEDIUM = 2
+    HIGH = 3
+    URGENT = 4
+    CRITICAL = 5
+
+
+class Complexity(IntEnum):
+    """Complexity levels for estimation."""
+    TRIVIAL = 1
+    SIMPLE = 2
+    MODERATE = 3
+    COMPLEX = 4
+    EXPERT = 5
+
+
+class UIConstants:
+    """UI-related constants."""
+    DEFAULT_PAGE_SIZE: Final[int] = 10
+    MAX_PAGE_SIZE: Final[int] = 100
+    DEFAULT_SEARCH_PLACEHOLDER: Final[str] = "Digite para buscar..."
+    EMPTY_STATE_MESSAGE: Final[str] = "Nenhum registro encontrado"
+    LOADING_MESSAGE: Final[str] = "Carregando..."
+    SUCCESS_MESSAGE: Final[str] = "OperaÃ§Ã£o realizada com sucesso!"
+    ERROR_MESSAGE: Final[str] = "Erro ao executar operaÃ§Ã£o"
+
+
+class ValidationRules:
+    """Validation constants."""
+    MIN_PASSWORD_LENGTH: Final[int] = 8
+    MAX_NAME_LENGTH: Final[int] = 100
+    MAX_EMAIL_LENGTH: Final[int] = 255
+    MAX_DESCRIPTION_LENGTH: Final[int] = 1000
+    MIN_SEARCH_LENGTH: Final[int] = 2
+
+
+class TimeConstants:
+    """Time-related constants."""
+    SESSION_TIMEOUT_MINUTES: Final[int] = 30
+    CACHE_TTL_SECONDS: Final[int] = 300
+    DB_CONNECTION_TIMEOUT: Final[int] = 30
+    POMODORO_MINUTES: Final[int] = 25
+    SHORT_BREAK_MINUTES: Final[int] = 5
+    LONG_BREAK_MINUTES: Final[int] = 15
diff --git a/streamlit_extension/utils/database.py b/streamlit_extension/utils/database.py
index c85cd289fb81cf17840f601843fd55bf12ea9a89..2e1c57291b35897267c43a1156fac091baebce79 100644
--- a/streamlit_extension/utils/database.py
+++ b/streamlit_extension/utils/database.py
@@ -1,99 +1,119 @@
 """
 ðŸ—„ï¸ Database Management Utilities
 
 Streamlit-optimized database operations with:
 - Connection pooling
 - Caching strategies
 - SQLAlchemy integration
 - Error handling
 """
 
 import sqlite3
 from pathlib import Path
-from typing import Optional, Dict, Any, List, Union, Iterator, Tuple, Generator
+from typing import (
+    Optional,
+    Dict,
+    Any,
+    List,
+    Union,
+    Iterator,
+    Tuple,
+    Generator,
+)
 from contextlib import contextmanager
 from datetime import datetime
 import json
 import logging
 
 # Graceful imports
 try:
     import sqlalchemy as sa
     from sqlalchemy import create_engine, text
     from sqlalchemy.pool import StaticPool
-    from sqlalchemy.engine import Connection
+    from sqlalchemy.engine import Connection, Result
     SQLALCHEMY_AVAILABLE = True
 except ImportError:
     sa = None
     create_engine = None
     text = None
     StaticPool = None
     Connection = None
+    Result = Any  # type: ignore[assignment]
     SQLALCHEMY_AVAILABLE = False
 
 try:
     import pandas as pd
     PANDAS_AVAILABLE = True
 except ImportError:
     pd = None
     PANDAS_AVAILABLE = False
 
 try:
     import streamlit as st
     STREAMLIT_AVAILABLE = True
 except ImportError:
     st = None
     STREAMLIT_AVAILABLE = False
 
 # Import timezone utilities
 try:
     from ..config.streamlit_config import format_datetime_user_tz, format_time_ago_user_tz
     TIMEZONE_UTILS_AVAILABLE = True
 except ImportError:
     TIMEZONE_UTILS_AVAILABLE = False
     format_datetime_user_tz = None
     format_time_ago_user_tz = None
 
 # Import duration system for FASE 2.3 extension
 try:
     from duration_system.duration_calculator import DurationCalculator
     from duration_system.duration_formatter import DurationFormatter
     DURATION_SYSTEM_AVAILABLE = True
 except ImportError:
     DurationCalculator = None
     DurationFormatter = None
     DURATION_SYSTEM_AVAILABLE = False
 
 # Import caching system
 try:
     from .cache import cache_database_query, invalidate_cache_on_change, get_cache
     CACHE_AVAILABLE = True
 except ImportError:
     CACHE_AVAILABLE = False
     cache_database_query = invalidate_cache_on_change = get_cache = None
 
+from .constants import (
+    TableNames,
+    FieldNames,
+    ClientStatus,
+    ProjectStatus,
+    TaskStatus,
+    EpicStatus,
+    TDDPhase,
+)
+
 logger = logging.getLogger(__name__)
 
 class DatabaseManager:
     """
     Enterprise-grade database manager with connection pooling and error handling.
 
     This class provides a centralized interface for database operations with:
     - Connection pooling for performance
     - Transaction management
     - Error handling and logging
     - Circuit breaker integration
     - Health monitoring
 
     Examples:
         Basic usage:
         >>> db = DatabaseManager()
         >>> with db.get_connection() as conn:
         ...     result = conn.execute("SELECT * FROM users")
 
         Transaction usage:
         >>> with db.get_connection() as conn:
         ...     with db.transaction(conn):
         ...         conn.execute("INSERT INTO users ...")
 
     Attributes:
@@ -198,202 +218,243 @@ class DatabaseManager:
             conn.row_factory = sqlite3.Row
             conn.execute("PRAGMA foreign_keys = ON")
             try:
                 yield conn
             finally:
                 conn.close()
 
     def release_connection(self, connection: Union[Connection, sqlite3.Connection]) -> None:
         """Return connection to pool with cleanup.
 
         This method is provided for cases where a connection obtained via
         :meth:`get_connection` needs to be closed manually instead of using the
         context manager protocol.
 
         Args:
             connection: Connection instance to be returned.
 
         Example:
             >>> conn = next(db_manager.get_connection())
             >>> db_manager.release_connection(conn)
         """
         try:
             connection.close()
         except Exception:  # pragma: no cover - best effort
             logger.warning("Failed to close connection", exc_info=True)
+
+    def execute_query(
+        self,
+        query: str,
+        params: Optional[Dict[str, Any]] = None,
+        database_name: str = "framework",
+    ) -> Union[Result, List[Dict[str, Any]]]:
+        """Execute a raw SQL query.
+
+        Args:
+            query: SQL query string to execute.
+            params: Optional mapping of parameters.
+            database_name: Database identifier, defaults to ``framework``.
+
+        Returns:
+            SQLAlchemy ``Result`` when available or list of row dictionaries.
+        """
+        params = params or {}
+        with self.get_connection(database_name) as conn:
+            if SQLALCHEMY_AVAILABLE:
+                return conn.execute(text(query), params)
+            cursor = conn.cursor()
+            cursor.execute(query, params)
+            return [dict(row) for row in cursor.fetchall()]
     
     @cache_database_query("get_epics", ttl=300) if CACHE_AVAILABLE else lambda f: f
-    def get_epics(self, page: int = 1, page_size: int = 50, 
-                 status_filter: str = "", project_id: Optional[int] = None) -> Dict[str, Any]:
+    def get_epics(
+        self,
+        page: int = 1,
+        page_size: int = 50,
+        status_filter: Optional[Union[EpicStatus, str]] = None,
+        project_id: Optional[int] = None,
+    ) -> Dict[str, Any]:
         """Get epics with intelligent caching and pagination.
         
         Args:
             page: Page number (1-based)
             page_size: Number of items per page
             status_filter: Filter by specific status
             project_id: Filter by specific project ID
             
         Returns:
             Dictionary with 'data' (list of epics), 'total', 'page', 'total_pages'
         """
         try:
             with self.get_connection("framework") as conn:
                 # Build WHERE conditions
                 where_conditions = ["deleted_at IS NULL"]
                 params: Dict[str, Any] = {}
                 
                 if status_filter:
-                    where_conditions.append("status = :status_filter")
-                    params["status_filter"] = status_filter
+                    where_conditions.append(f"{FieldNames.STATUS} = :status_filter")
+                    params["status_filter"] = (
+                        status_filter.value if isinstance(status_filter, EpicStatus) else status_filter
+                    )
                 
                 if project_id:
                     where_conditions.append("project_id = :project_id")
                     params["project_id"] = project_id
                 
                 where_clause = " AND ".join(where_conditions)
                 
                 # Count total records
-                count_query = f"SELECT COUNT(*) FROM framework_epics WHERE {where_clause}"
+                count_query = f"SELECT COUNT(*) FROM {TableNames.EPICS} WHERE {where_clause}"
                 
                 if SQLALCHEMY_AVAILABLE:
                     count_result = conn.execute(text(count_query), params)
                     total = count_result.scalar()
                 else:
                     cursor = conn.cursor()
                     cursor.execute(count_query, params)
                     total = cursor.fetchone()[0]
                 
                 # Calculate pagination
                 total_pages = (total + page_size - 1) // page_size
                 offset = (page - 1) * page_size
                 
                 # Get paginated data
                 data_query = f"""
                     SELECT id, epic_key, name, description, status, 
                            created_at, updated_at, completed_at,
                            points_earned, difficulty_level, project_id
-                    FROM framework_epics
+                    FROM {TableNames.EPICS}
                     WHERE {where_clause}
                     ORDER BY created_at DESC
                     LIMIT :limit OFFSET :offset
                 """
                 params["limit"] = page_size
                 params["offset"] = offset
                 
                 if SQLALCHEMY_AVAILABLE:
                     result = conn.execute(text(data_query), params)
                     data = [dict(row._mapping) for row in result]
                 else:
                     cursor = conn.cursor()
                     cursor.execute(data_query, params)
                     data = [dict(row) for row in cursor.fetchall()]
                 
                 return {
                     "data": data,
                     "total": total,
                     "page": page,
                     "page_size": page_size,
                     "total_pages": total_pages
                 }
         except Exception as e:
             print(f"Error loading epics: {e}")
             return {
                 "data": [],
                 "total": 0,
                 "page": page,
                 "page_size": page_size,
                 "total_pages": 0
             }
     
     def get_all_epics(self) -> List[Dict[str, Any]]:
         """Backward compatibility method - get all epics without pagination."""
         result = self.get_epics(page=1, page_size=1000)  # Large page size to get all
         return result["data"] if isinstance(result, dict) else result
     
     @cache_database_query("get_tasks", ttl=300) if CACHE_AVAILABLE else lambda f: f
-    def get_tasks(self, epic_id: Optional[int] = None, page: int = 1, page_size: int = 100,
-                 status_filter: str = "", tdd_phase_filter: str = "") -> Dict[str, Any]:
+    def get_tasks(
+        self,
+        epic_id: Optional[int] = None,
+        page: int = 1,
+        page_size: int = 100,
+        status_filter: Optional[Union[TaskStatus, str]] = None,
+        tdd_phase_filter: Optional[Union[TDDPhase, str]] = None,
+    ) -> Dict[str, Any]:
         """Get tasks with intelligent caching, pagination, and filtering.
         
         Args:
             epic_id: Filter by specific epic ID
             page: Page number (1-based)
             page_size: Number of items per page
             status_filter: Filter by specific status
             tdd_phase_filter: Filter by TDD phase
             
         Returns:
             Dictionary with 'data' (list of tasks), 'total', 'page', 'total_pages'
         """
         try:
             with self.get_connection("framework") as conn:
                 # Build WHERE conditions
                 where_conditions = ["1=1"]
                 params: Dict[str, Any] = {}
                 
                 if epic_id:
                     where_conditions.append("t.epic_id = :epic_id")
                     params["epic_id"] = epic_id
                 
                 if status_filter:
                     where_conditions.append("t.status = :status_filter")
-                    params["status_filter"] = status_filter
-                
+                    params["status_filter"] = (
+                        status_filter.value if isinstance(status_filter, TaskStatus) else status_filter
+                    )
+
                 if tdd_phase_filter:
                     where_conditions.append("t.tdd_phase = :tdd_phase_filter")
-                    params["tdd_phase_filter"] = tdd_phase_filter
+                    params["tdd_phase_filter"] = (
+                        tdd_phase_filter.value if isinstance(tdd_phase_filter, TDDPhase) else tdd_phase_filter
+                    )
                 
                 where_clause = " AND ".join(where_conditions)
                 
                 # Count total records
                 count_query = f"""
-                    SELECT COUNT(*) 
-                    FROM framework_tasks t
-                    JOIN framework_epics e ON t.epic_id = e.id
+                    SELECT COUNT(*)
+                    FROM {TableNames.TASKS} t
+                    JOIN {TableNames.EPICS} e ON t.epic_id = e.id
                     WHERE {where_clause}
                 """
                 
                 if SQLALCHEMY_AVAILABLE:
                     count_result = conn.execute(text(count_query), params)
                     total = count_result.scalar()
                 else:
                     cursor = conn.cursor()
                     cursor.execute(count_query, list(params.values()))
                     total = cursor.fetchone()[0]
                 
                 # Calculate pagination
                 total_pages = (total + page_size - 1) // page_size
                 offset = (page - 1) * page_size
                 
                 # Get paginated data
                 data_query = f"""
                     SELECT t.id, t.epic_id, t.title, t.description, t.status,
                            t.estimate_minutes, t.tdd_phase, t.position,
                            t.created_at, t.updated_at, t.completed_at,
                            e.name as epic_name, e.epic_key, t.task_key
-                    FROM framework_tasks t
-                    JOIN framework_epics e ON t.epic_id = e.id
+                    FROM {TableNames.TASKS} t
+                    JOIN {TableNames.EPICS} e ON t.epic_id = e.id
                     WHERE {where_clause}
                     ORDER BY t.position ASC, t.created_at DESC
                     LIMIT :limit OFFSET :offset
                 """
                 params["limit"] = page_size
                 params["offset"] = offset
                 
                 if SQLALCHEMY_AVAILABLE:
                     result = conn.execute(text(data_query), params)
                     data = [dict(row._mapping) for row in result]
                 else:
                     cursor = conn.cursor()
                     cursor.execute(data_query, list(params.values()))
                     data = [dict(row) for row in cursor.fetchall()]
                 
                 return {
                     "data": data,
                     "total": total,
                     "page": page,
                     "page_size": page_size,
                     "total_pages": total_pages
                 }
         except Exception as e:
             logger.error(f"Error loading tasks: {e}")
             if STREAMLIT_AVAILABLE and st:
@@ -1917,130 +1978,132 @@ class DatabaseManager:
                     cursor.execute("""
                         SELECT id, title, status, tdd_phase, estimate_minutes,
                                created_at, updated_at, completed_at, priority
                         FROM framework_tasks 
                         WHERE epic_id = ? AND deleted_at IS NULL
                         ORDER BY priority ASC, created_at ASC
                     """, (epic_id,))
                     
                     return [dict(zip([col[0] for col in cursor.description], row)) 
                            for row in cursor.fetchall()]
                            
         except Exception:
             return []
     
     # ==================================================================================
     # HIERARCHY SYSTEM METHODS (CLIENT â†’ PROJECT â†’ EPIC â†’ TASK) - SCHEMA V6
     # ==================================================================================
     
     @cache_database_query("get_clients", ttl=300) if CACHE_AVAILABLE else lambda f: f
     def get_clients(
         self,
         include_inactive: bool = True,
         page: int = 1,
         page_size: int = 20,
         name_filter: str = "",
-        status_filter: str = "",
+        status_filter: Optional[Union[ClientStatus, str]] = None,
     ) -> Dict[str, Any]:
         """Retrieve clients with filtering and pagination support.
 
         Performs optimized client queries with multiple filter options and
         pagination. Results are cached for performance.
 
         Args:
             include_inactive: Include inactive clients. Defaults to ``True``.
             page: Page number (1-based).
             page_size: Number of items per page.
             name_filter: Search term for client name using ``LIKE`` matching.
             status_filter: Filter by client status (e.g. ``"active"``).
 
         Returns:
             Dict[str, Any]: Dictionary containing:
                 - ``data`` (List[Dict]): List of client records.
                 - ``total`` (int): Total count of matching clients.
                 - ``page`` (int): Current page number.
                 - ``page_size`` (int): Results per page.
                 - ``total_pages`` (int): Total pages available.
 
         Raises:
             DatabaseError: If query execution fails.
 
         Performance:
             - Cached results: ~1ms response time.
             - Uncached results: ~10-50ms depending on dataset size.
 
         Thread Safety:
             This method is thread-safe and can be called concurrently.
 
         Example:
             >>> result = db_manager.get_clients(include_inactive=False, page=1)
             >>> clients = result["data"]
         """
         try:
             with self.get_connection("framework") as conn:
                 # Build WHERE conditions
                 where_conditions = ["deleted_at IS NULL"]
                 params: Dict[str, Any] = {}
-                
+
                 if not include_inactive:
-                    where_conditions.append("status = :status")
-                    params["status"] = "active"
-                
+                    where_conditions.append(f"{FieldNames.STATUS} = :status")
+                    params["status"] = ClientStatus.ACTIVE.value
+
                 if name_filter:
                     where_conditions.append("name LIKE :name_filter")
                     params["name_filter"] = f"%{name_filter}%"
-                
+
                 if status_filter:
-                    where_conditions.append("status = :status_filter")
-                    params["status_filter"] = status_filter
+                    where_conditions.append(f"{FieldNames.STATUS} = :status_filter")
+                    params["status_filter"] = (
+                        status_filter.value if isinstance(status_filter, ClientStatus) else status_filter
+                    )
                 
                 where_clause = " AND ".join(where_conditions)
                 
                 # Count total records
-                count_query = f"SELECT COUNT(*) FROM framework_clients WHERE {where_clause}"  # nosec B608
+                count_query = f"SELECT COUNT(*) FROM {TableNames.CLIENTS} WHERE {where_clause}"  # nosec B608
                 
                 if SQLALCHEMY_AVAILABLE:
                     count_result = conn.execute(text(count_query), params)
                     total = count_result.scalar()
                 else:
                     cursor = conn.cursor()
                     cursor.execute(count_query, params)
                     total = cursor.fetchone()[0]
                 
                 # Calculate pagination
                 total_pages = (total + page_size - 1) // page_size
                 offset = (page - 1) * page_size
                 
                 # Get paginated data
                 data_query = f"""
                     SELECT id, client_key, name, description, industry, company_size,
                            primary_contact_name, primary_contact_email,
                            timezone, currency, preferred_language,
                            hourly_rate, contract_type, status, client_tier,
                            priority_level, account_manager_id, technical_lead_id,
                            created_at, updated_at, last_contact_date
-                    FROM framework_clients
+                    FROM {TableNames.CLIENTS}
                     WHERE {where_clause}
                     ORDER BY priority_level DESC, name ASC
                     LIMIT :limit OFFSET :offset
                 """  # nosec B608
                 params["limit"] = page_size
                 params["offset"] = offset
                 
                 if SQLALCHEMY_AVAILABLE:
                     result = conn.execute(text(data_query), params)
                     data = [dict(row._mapping) for row in result]
                 else:
                     cursor = conn.cursor()
                     cursor.execute(data_query, params)
                     data = [dict(row) for row in cursor.fetchall()]
                 
                 return {
                     "data": data,
                     "total": total,
                     "page": page,
                     "page_size": page_size,
                     "total_pages": total_pages
                 }
                 
         except Exception as e:
             logger.error(f"Error loading clients: {e}")
@@ -2082,125 +2145,133 @@ class DatabaseManager:
                             """
                             SELECT * FROM framework_clients
                             WHERE id = :client_id AND deleted_at IS NULL
                             """
                         ),
                         {"client_id": client_id},
                     )
                     row = result.fetchone()
                     return dict(row._mapping) if row else None
                 else:
                     cursor = conn.cursor()
                     cursor.execute(
                         """
                             SELECT * FROM framework_clients
                             WHERE id = ? AND deleted_at IS NULL
                         """,
                         (client_id,),
                     )
                     row = cursor.fetchone()
                     return dict(row) if row else None
         except Exception as e:
             logger.error(f"Error getting client: {e}")
             return None
     
     @cache_database_query("get_projects", ttl=300) if CACHE_AVAILABLE else lambda f: f
-    def get_projects(self, client_id: Optional[int] = None, include_inactive: bool = False,
-                    page: int = 1, page_size: int = 50, status_filter: str = "",
-                    project_type_filter: str = "") -> Dict[str, Any]:
+    def get_projects(
+        self,
+        client_id: Optional[int] = None,
+        include_inactive: bool = False,
+        page: int = 1,
+        page_size: int = 50,
+        status_filter: Optional[Union[ProjectStatus, str]] = None,
+        project_type_filter: str = "",
+    ) -> Dict[str, Any]:
         """Get projects with caching support and pagination.
         
         Args:
             client_id: Filter by specific client ID (optional)
             include_inactive: If True, include inactive/archived projects
             page: Page number (1-based)
             page_size: Number of items per page
             status_filter: Filter by specific status
             project_type_filter: Filter by project type
             
         Returns:
             Dictionary with 'data' (list of projects), 'total', 'page', 'total_pages'
         """
         try:
             with self.get_connection("framework") as conn:
                 # Build WHERE conditions
                 where_conditions = ["p.deleted_at IS NULL"]
                 params: Dict[str, Any] = {}
                 
                 if client_id:
                     where_conditions.append("p.client_id = :client_id")
                     params["client_id"] = client_id
 
                 if not include_inactive:
                     where_conditions.append("p.status NOT IN ('cancelled', 'archived')")
                 
                 if status_filter:
                     where_conditions.append("p.status = :status_filter")
-                    params["status_filter"] = status_filter
+                    params["status_filter"] = (
+                        status_filter.value if isinstance(status_filter, ProjectStatus) else status_filter
+                    )
                 
                 if project_type_filter:
                     where_conditions.append("p.project_type = :project_type_filter")
                     params["project_type_filter"] = project_type_filter
                 
                 where_clause = " AND ".join(where_conditions)
                 
                 # Count total records
                 count_query = f"""
-                    SELECT COUNT(*) 
-                    FROM framework_projects p
-                    INNER JOIN framework_clients c ON p.client_id = c.id AND c.deleted_at IS NULL
+                    SELECT COUNT(*)
+                    FROM {TableNames.PROJECTS} p
+                    INNER JOIN {TableNames.CLIENTS} c ON p.client_id = c.id AND c.deleted_at IS NULL
                     WHERE {where_clause}
                 """
                 
                 if SQLALCHEMY_AVAILABLE:
                     count_result = conn.execute(text(count_query), params)
                     total = count_result.scalar()
                 else:
                     cursor = conn.cursor()
                     cursor.execute(count_query, list(params.values()))
                     total = cursor.fetchone()[0]
                 
                 # Calculate pagination
                 total_pages = (total + page_size - 1) // page_size
                 offset = (page - 1) * page_size
                 
                 # Get paginated data
                 data_query = f"""
                     SELECT p.id, p.client_id, p.project_key, p.name, p.description,
                            p.summary, p.project_type, p.methodology, p.status,
                            p.priority, p.health_status, p.completion_percentage,
                            p.planned_start_date, p.planned_end_date,
                            p.actual_start_date, p.actual_end_date,
                            p.estimated_hours, p.actual_hours,
                            p.budget_amount, p.budget_currency, p.hourly_rate,
                            p.project_manager_id, p.technical_lead_id,
                            p.repository_url, p.deployment_url, p.documentation_url,
                            p.created_at, p.updated_at,
                            c.name as client_name, c.client_key as client_key,
                            c.status as client_status, c.client_tier
-                    FROM framework_projects p
-                    INNER JOIN framework_clients c ON p.client_id = c.id AND c.deleted_at IS NULL
+                    FROM {TableNames.PROJECTS} p
+                    INNER JOIN {TableNames.CLIENTS} c ON p.client_id = c.id AND c.deleted_at IS NULL
                     WHERE {where_clause}
                     ORDER BY p.priority DESC, p.name ASC
                     LIMIT :limit OFFSET :offset
                 """
                 params["limit"] = page_size
                 params["offset"] = offset
 
                 if SQLALCHEMY_AVAILABLE:
                     result = conn.execute(text(data_query), params)
                     data = [dict(row._mapping) for row in result]
                 else:
                     cursor = conn.cursor()
                     cursor.execute(data_query, list(params.values()))
                     data = [dict(row) for row in cursor.fetchall()]
                 
                 return {
                     "data": data,
                     "total": total,
                     "page": page,
                     "page_size": page_size,
                     "total_pages": total_pages
                 }
         except Exception as e:
             logger.error(f"Error loading projects: {e}")
             if STREAMLIT_AVAILABLE and st:
@@ -2442,209 +2513,224 @@ class DatabaseManager:
                 if project_id:
                     query += " AND project_id = ?"
                     params.append(project_id)
                 elif client_id:
                     query += " AND client_id = ?"
                     params.append(client_id)
                 
                 query += " ORDER BY client_name, project_name"
                 
                 if SQLALCHEMY_AVAILABLE:
                     result = conn.execute(text(query), params)
                     return [dict(row._mapping) for row in result]
                 else:
                     cursor = conn.cursor()
                     cursor.execute(query, params)
                     return [dict(row) for row in cursor.fetchall()]
         except Exception as e:
             logger.error(f"Error loading project dashboard: {e}")
             return []
     
     # ==================================================================================
     # HIERARCHY CRUD OPERATIONS
     # ==================================================================================
     
     @invalidate_cache_on_change("db_query:get_clients:", "db_query:get_client_dashboard:") if CACHE_AVAILABLE else lambda f: f
-    def create_client(self, client_key: str, name: str, description: str = "",
-                     industry: str = "", company_size: str = "startup",
-                     primary_contact_name: str = "", primary_contact_email: str = "",
-                     hourly_rate: float = 0.0, **kwargs) -> Optional[int]:
+    def create_client(
+        self,
+        client_key: str,
+        name: str,
+        description: str = "",
+        industry: str = "",
+        company_size: str = "startup",
+        primary_contact_name: str = "",
+        primary_contact_email: str = "",
+        hourly_rate: float = 0.0,
+        **kwargs: Any,
+    ) -> Optional[int]:
         """Create new client record.
 
         Creates client with full validation and automatic timestamp assignment.
         Invalidates related caches and triggers audit logging.
 
         Args:
             client_key: Unique client identifier string (3-20 chars).
             name: Client display name (1-100 characters).
             description: Client description (max 500 chars).
             industry: Industry classification.
             company_size: Company size category.
             primary_contact_name: Primary contact name.
             primary_contact_email: Primary contact email.
             hourly_rate: Billing rate per hour.
             **kwargs: Additional optional fields like ``status`` or
                 ``client_tier``.
 
         Returns:
             Optional[int]: New client ID if successful, ``None`` if failed.
 
         Raises:
             ValueError: If required fields are missing or invalid.
             IntegrityError: If ``client_key`` already exists.
             DatabaseError: If insert operation fails.
 
         Side Effects:
             - Invalidates client list caches.
             - Creates audit log entry.
 
         Performance:
             - Insert operation: ~5ms.
 
         Example:
             >>> client_id = db_manager.create_client(
             ...     client_key="acme_corp", name="ACME Corporation"
             ... )
         """
         try:
             client_data = {
                 'client_key': client_key,
                 'name': name,
                 'description': description,
                 'industry': industry,
                 'company_size': company_size,
                 'primary_contact_name': primary_contact_name,
                 'primary_contact_email': primary_contact_email,
                 'hourly_rate': hourly_rate,
-                'status': kwargs.get('status', 'active'),
+                'status': kwargs.get('status', ClientStatus.ACTIVE.value),
                 'client_tier': kwargs.get('client_tier', 'standard'),
                 'priority_level': kwargs.get('priority_level', 5),
                 'timezone': kwargs.get('timezone', 'America/Sao_Paulo'),
                 'currency': kwargs.get('currency', 'BRL'),
                 'preferred_language': kwargs.get('preferred_language', 'pt-BR'),
                 'contract_type': kwargs.get('contract_type', 'time_and_materials'),
                 'created_by': kwargs.get('created_by', 1)
             }
             
             with self.get_connection("framework") as conn:
                 placeholders = ', '.join(['?' for _ in client_data])
                 columns = ', '.join(client_data.keys())
                 
                 if SQLALCHEMY_AVAILABLE:
                     # Convert to named parameters for SQLAlchemy
                     named_placeholders = ', '.join([f':{key}' for key in client_data.keys()])
                     result = conn.execute(
-                        text(f"INSERT INTO framework_clients ({columns}) VALUES ({named_placeholders})"),  # nosec B608
+                        text(f"INSERT INTO {TableNames.CLIENTS} ({columns}) VALUES ({named_placeholders})"),  # nosec B608
                         client_data
                     )
                     conn.commit()
                     return result.lastrowid
                 else:
                     cursor = conn.cursor()
                     cursor.execute(
-                        f"INSERT INTO framework_clients ({columns}) VALUES ({placeholders})",  # nosec B608
+                        f"INSERT INTO {TableNames.CLIENTS} ({columns}) VALUES ({placeholders})",  # nosec B608
                         list(client_data.values())
                     )
                     conn.commit()
                     return cursor.lastrowid
                     
         except Exception as e:
             logger.error(f"Error creating client: {e}")
             if STREAMLIT_AVAILABLE and st:
                 st.error(f"âŒ Error creating client: {e}")
             return None
     
     @invalidate_cache_on_change(
         "db_query:get_projects:",
         "db_query:get_hierarchy_overview:",
         "db_query:get_client_dashboard:",
         "db_query:get_project_dashboard:"
     ) if CACHE_AVAILABLE else lambda f: f
-    def create_project(self, client_id: int, project_key: str, name: str,
-                      description: str = "", project_type: str = "development",
-                      methodology: str = "agile", **kwargs) -> Optional[int]:
+    def create_project(
+        self,
+        client_id: int,
+        project_key: str,
+        name: str,
+        description: str = "",
+        project_type: str = "development",
+        methodology: str = "agile",
+        **kwargs: Any,
+    ) -> Optional[int]:
         """Create a new project.
         
         Args:
             client_id: ID of the client who owns this project
             project_key: Unique project identifier within client
             name: Project name
             description: Project description
             project_type: Type of project (development, maintenance, etc.)
             methodology: Development methodology (agile, waterfall, etc.)
             **kwargs: Additional project fields
             
         Returns:
             Project ID if successful, None otherwise
         """
         try:
             project_data = {
                 'client_id': client_id,
                 'project_key': project_key,
                 'name': name,
                 'description': description,
                 'project_type': project_type,
                 'methodology': methodology,
-                'status': kwargs.get('status', 'planning'),
+                'status': kwargs.get('status', ProjectStatus.PLANNING.value),
                 'priority': kwargs.get('priority', 5),
                 'health_status': kwargs.get('health_status', 'green'),
                 'completion_percentage': kwargs.get('completion_percentage', 0),
                 'planned_start_date': kwargs.get('planned_start_date'),
                 'planned_end_date': kwargs.get('planned_end_date'),
                 'estimated_hours': kwargs.get('estimated_hours', 0),
                 'budget_amount': kwargs.get('budget_amount', 0),
                 'budget_currency': kwargs.get('budget_currency', 'BRL'),
                 'hourly_rate': kwargs.get('hourly_rate'),
                 'project_manager_id': kwargs.get('project_manager_id', 1),
                 'technical_lead_id': kwargs.get('technical_lead_id', 1),
                 'repository_url': kwargs.get('repository_url', ''),
                 'visibility': kwargs.get('visibility', 'client'),
                 'access_level': kwargs.get('access_level', 'standard'),
                 'complexity_score': kwargs.get('complexity_score', 5.0),
                 'quality_score': kwargs.get('quality_score', 8.0),
                 'created_by': kwargs.get('created_by', 1)
             }
             
             with self.get_connection("framework") as conn:
                 # Remove None values
                 project_data = {k: v for k, v in project_data.items() if v is not None}
                 
                 placeholders = ', '.join(['?' for _ in project_data])
                 columns = ', '.join(project_data.keys())
-                
+
                 if SQLALCHEMY_AVAILABLE:
                     named_placeholders = ', '.join([f':{key}' for key in project_data.keys()])
                     result = conn.execute(
-                        text(f"INSERT INTO framework_projects ({columns}) VALUES ({named_placeholders})"),  # nosec B608
+                        text(f"INSERT INTO {TableNames.PROJECTS} ({columns}) VALUES ({named_placeholders})"),  # nosec B608
                         project_data
                     )
                     conn.commit()
                     return result.lastrowid
                 else:
                     cursor = conn.cursor()
                     cursor.execute(
-                        f"INSERT INTO framework_projects ({columns}) VALUES ({placeholders})",
+                        f"INSERT INTO {TableNames.PROJECTS} ({columns}) VALUES ({placeholders})",
                         list(project_data.values())
                     )
                     conn.commit()
                     return cursor.lastrowid
                     
         except Exception as e:
             logger.error(f"Error creating project: {e}")
             if STREAMLIT_AVAILABLE and st:
                 st.error(f"âŒ Error creating project: {e}")
             return None
     
     @invalidate_cache_on_change("db_query:get_epics:", "db_query:get_epics_with_hierarchy:") if CACHE_AVAILABLE else lambda f: f
     def update_epic_project(self, epic_id: int, project_id: int) -> bool:
         """Update the project assignment for an epic.
         
         Args:
             epic_id: ID of the epic to update
             project_id: ID of the new project
             
         Returns:
             True if successful, False otherwise
         """
         try:
             with self.get_connection("framework") as conn:
                 if SQLALCHEMY_AVAILABLE:
@@ -2712,288 +2798,288 @@ class DatabaseManager:
         """
         try:
             with self.get_connection("framework") as conn:
                 if SQLALCHEMY_AVAILABLE:
                     result = conn.execute(text("""
                         SELECT * FROM framework_projects 
                         WHERE client_id = :client_id AND project_key = :project_key 
                         AND deleted_at IS NULL
                     """), {"client_id": client_id, "project_key": project_key})
                     row = result.fetchone()
                     return dict(row._mapping) if row else None
                 else:
                     cursor = conn.cursor()
                     cursor.execute("""
                         SELECT * FROM framework_projects 
                         WHERE client_id = ? AND project_key = ? AND deleted_at IS NULL
                     """, (client_id, project_key))
                     row = cursor.fetchone()
                     return dict(row) if row else None
                     
         except Exception as e:
             logger.error(f"Error getting project by key: {e}")
             return None
     
     @invalidate_cache_on_change("db_query:get_clients:", "db_query:get_client_dashboard:") if CACHE_AVAILABLE else lambda f: f
-    def update_client(self, client_id: int, **fields) -> bool:
+    def update_client(self, client_id: int, **fields: Any) -> bool:
         """Update existing client record.
 
         Updates specified fields while preserving others. Validates all input
         and maintains data integrity. Supports partial updates.
 
         Args:
             client_id: Client ID to update. Must exist.
             **fields: Fields to update. Same validation as ``create_client``.
 
         Returns:
             bool: ``True`` if update successful, ``False`` if failed or no
                 changes.
 
         Raises:
             ValueError: If ``client_id`` invalid or field validation fails.
             DatabaseError: If update operation fails.
 
         Side Effects:
             - Invalidates client caches for this client.
             - Updates ``updated_at`` timestamp.
 
         Performance:
             - Update operation: ~3ms.
 
         Example:
             >>> db_manager.update_client(123, name="New Name")
         """
         try:
             if not fields:
                 return True
                 
             # Add updated_at timestamp
             fields['updated_at'] = 'CURRENT_TIMESTAMP'
             
             # Build SET clause
             set_clauses = []
             values = {}
             
             for key, value in fields.items():
                 if key == 'updated_at':
                     set_clauses.append(f"{key} = CURRENT_TIMESTAMP")
                 else:
                     set_clauses.append(f"{key} = :{key}")
                     values[key] = value
             
             values['client_id'] = client_id
             
             with self.get_connection("framework") as conn:
                 if SQLALCHEMY_AVAILABLE:
                     conn.execute(text(f"""
-                        UPDATE framework_clients 
+                        UPDATE {TableNames.CLIENTS}
                         SET {', '.join(set_clauses)}
                         WHERE id = :client_id AND deleted_at IS NULL
                     """), values)  # nosec B608
                     conn.commit()
                 else:
                     cursor = conn.cursor()
                     # Convert to positional parameters for sqlite
                     positional_values = [values[key] for key in values.keys() if key != 'client_id']
                     positional_values.append(client_id)
                     
                     sqlite_clauses = [clause.replace(f':{key}', '?') for clause in set_clauses if f':{key}' in clause]
                     sqlite_clauses.extend([clause for clause in set_clauses if '?' not in clause and ':' not in clause])
                     
                     cursor.execute(f"""
-                        UPDATE framework_clients 
+                        UPDATE {TableNames.CLIENTS}
                         SET {', '.join(sqlite_clauses)}
                         WHERE id = ? AND deleted_at IS NULL
                     """, positional_values)  # nosec B608
                     conn.commit()
                 
                 return True
                 
         except Exception as e:
             logger.error(f"Error updating client: {e}")
             if STREAMLIT_AVAILABLE and st:
                 st.error(f"âŒ Error updating client: {e}")
             return False
     
     @invalidate_cache_on_change("db_query:get_clients:", "db_query:get_client_dashboard:") if CACHE_AVAILABLE else lambda f: f
     def delete_client(self, client_id: int, soft_delete: bool = True) -> bool:
         """Delete client record (soft or hard delete).
 
         Removes client from active use. Soft delete preserves data for audit
         purposes. Hard delete permanently removes all data.
 
         Args:
             client_id: Client ID to delete. Must exist.
             soft_delete: Use soft delete. Defaults to ``True``.
 
         Returns:
             bool: ``True`` if deletion successful, ``False`` if failed.
 
         Raises:
             ValueError: If ``client_id`` invalid.
             DatabaseError: If delete operation fails.
 
         Side Effects:
             - Invalidates all client-related caches.
 
         Performance:
             - Soft delete: ~2ms.
             - Hard delete: ~10-100ms (depends on related data).
 
         Example:
             >>> db_manager.delete_client(123, soft_delete=True)
         """
         try:
             with self.get_connection("framework") as conn:
                 if soft_delete:
                     if SQLALCHEMY_AVAILABLE:
                         conn.execute(text("""
-                            UPDATE framework_clients 
+                            UPDATE {TableNames.CLIENTS}
                             SET deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                             WHERE id = :client_id
                         """), {"client_id": client_id})
                         conn.commit()
                     else:
                         cursor = conn.cursor()
                         cursor.execute("""
-                            UPDATE framework_clients 
+                            UPDATE {TableNames.CLIENTS}
                             SET deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                             WHERE id = ?
                         """, (client_id,))
                         conn.commit()
                 else:
                     if SQLALCHEMY_AVAILABLE:
-                        conn.execute(text("DELETE FROM framework_clients WHERE id = :client_id"), 
+                        conn.execute(text(f"DELETE FROM {TableNames.CLIENTS} WHERE id = :client_id"),
                                    {"client_id": client_id})
                         conn.commit()
                     else:
                         cursor = conn.cursor()
-                        cursor.execute("DELETE FROM framework_clients WHERE id = ?", (client_id,))
+                        cursor.execute(f"DELETE FROM {TableNames.CLIENTS} WHERE id = ?", (client_id,))
                         conn.commit()
                 
                 return True
                 
         except Exception as e:
             logger.error(f"Error deleting client: {e}")
             if STREAMLIT_AVAILABLE and st:
                 st.error(f"âŒ Error deleting client: {e}")
             return False
     
     @invalidate_cache_on_change(
         "db_query:get_projects:",
         "db_query:get_hierarchy_overview:",
         "db_query:get_client_dashboard:",
         "db_query:get_project_dashboard:"
     ) if CACHE_AVAILABLE else lambda f: f
-    def update_project(self, project_id: int, **fields) -> bool:
+    def update_project(self, project_id: int, **fields: Any) -> bool:
         """Update an existing project.
         
         Args:
             project_id: ID of the project to update
             **fields: Fields to update
             
         Returns:
             True if successful, False otherwise
         """
         try:
             if not fields:
                 return True
                 
             # Add updated_at timestamp
             fields['updated_at'] = 'CURRENT_TIMESTAMP'
             
             # Build SET clause
             set_clauses = []
             values = {}
             
             for key, value in fields.items():
                 if key == 'updated_at':
                     set_clauses.append(f"{key} = CURRENT_TIMESTAMP")
                 else:
                     set_clauses.append(f"{key} = :{key}")
                     values[key] = value
             
             values['project_id'] = project_id
             
             with self.get_connection("framework") as conn:
                 if SQLALCHEMY_AVAILABLE:
                     conn.execute(text(f"""
-                        UPDATE framework_projects 
+                        UPDATE {TableNames.PROJECTS}
                         SET {', '.join(set_clauses)}
                         WHERE id = :project_id AND deleted_at IS NULL
                     """), values)  # nosec B608
                     conn.commit()
                 else:
                     cursor = conn.cursor()
                     # Convert to positional parameters for sqlite
                     positional_values = [values[key] for key in values.keys() if key != 'project_id']
                     positional_values.append(project_id)
                     
                     sqlite_clauses = [clause.replace(f':{key}', '?') for clause in set_clauses if f':{key}' in clause]
                     sqlite_clauses.extend([clause for clause in set_clauses if '?' not in clause and ':' not in clause])
                     
                     cursor.execute(f"""
-                        UPDATE framework_projects 
+                        UPDATE {TableNames.PROJECTS}
                         SET {', '.join(sqlite_clauses)}
                         WHERE id = ? AND deleted_at IS NULL
                     """, positional_values)  # nosec B608
                     conn.commit()
                 
                 return True
                 
         except Exception as e:
             logger.error(f"Error updating project: {e}")
             if STREAMLIT_AVAILABLE and st:
                 st.error(f"âŒ Error updating project: {e}")
             return False
     
     @invalidate_cache_on_change(
         "db_query:get_projects:",
         "db_query:get_hierarchy_overview:",
         "db_query:get_client_dashboard:",
         "db_query:get_project_dashboard:"
     ) if CACHE_AVAILABLE else lambda f: f
     def delete_project(self, project_id: int, soft_delete: bool = True) -> bool:
         """Delete a project (soft delete by default).
         
         Args:
             project_id: ID of the project to delete
             soft_delete: If True, mark as deleted instead of removing
             
         Returns:
             True if successful, False otherwise
         """
         try:
             with self.get_connection("framework") as conn:
                 if soft_delete:
                     if SQLALCHEMY_AVAILABLE:
                         conn.execute(text("""
-                            UPDATE framework_projects 
+                            UPDATE {TableNames.PROJECTS}
                             SET deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                             WHERE id = :project_id
                         """), {"project_id": project_id})
                         conn.commit()
                     else:
                         cursor = conn.cursor()
                         cursor.execute("""
-                            UPDATE framework_projects 
+                            UPDATE {TableNames.PROJECTS}
                             SET deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                             WHERE id = ?
                         """, (project_id,))
                         conn.commit()
                 else:
                     if SQLALCHEMY_AVAILABLE:
-                        conn.execute(text("DELETE FROM framework_projects WHERE id = :project_id"), 
+                        conn.execute(text(f"DELETE FROM {TableNames.PROJECTS} WHERE id = :project_id"),
                                    {"project_id": project_id})
                         conn.commit()
                     else:
                         cursor = conn.cursor()
-                        cursor.execute("DELETE FROM framework_projects WHERE id = ?", (project_id,))
+                        cursor.execute(f"DELETE FROM {TableNames.PROJECTS} WHERE id = ?", (project_id,))
                         conn.commit()
                 
                 return True
                 
         except Exception as e:
             logger.error(f"Error deleting project: {e}")
             if STREAMLIT_AVAILABLE and st:
                 st.error(f"âŒ Error deleting project: {e}")
             return False
\ No newline at end of file
diff --git a/tests/test_constants_system.py b/tests/test_constants_system.py
new file mode 100644
index 0000000000000000000000000000000000000000..47d041639596752b69ce4114133eec3dce0e2bfb
--- /dev/null
+++ b/tests/test_constants_system.py
@@ -0,0 +1,37 @@
+import sys
+from pathlib import Path
+import types
+
+sys.path.append(str(Path(__file__).resolve().parents[1]))
+
+sys.modules.setdefault("psutil", types.ModuleType("psutil"))
+
+from streamlit_extension.utils.constants import (
+    TableNames,
+    FieldNames,
+    ClientStatus,
+    ProjectStatus,
+    TaskStatus,
+    EpicStatus,
+    TDDPhase,
+    Priority,
+    Complexity,
+)
+
+def test_table_constants():
+    assert TableNames.CLIENTS == "framework_clients"
+    assert TableNames.PROJECTS == "framework_projects"
+    assert FieldNames.STATUS == "status"
+
+
+def test_enum_values():
+    assert ClientStatus.ACTIVE.value == "active"
+    assert ProjectStatus.PLANNING.value == "planning"
+    assert TaskStatus.TODO.value == "todo"
+    assert EpicStatus.DRAFT.value == "draft"
+    assert TDDPhase.RED.value == "red"
+
+
+def test_int_enums():
+    assert Priority.HIGH == 3
+    assert Complexity.COMPLEX == 4
diff --git a/tests/test_type_hints_database_manager.py b/tests/test_type_hints_database_manager.py
new file mode 100644
index 0000000000000000000000000000000000000000..ae6ed7eaa35340db84226ee87501259436fac979
--- /dev/null
+++ b/tests/test_type_hints_database_manager.py
@@ -0,0 +1,35 @@
+import inspect
+import sys
+from pathlib import Path
+import types
+
+sys.path.append(str(Path(__file__).resolve().parents[1]))
+
+sys.modules.setdefault("psutil", types.ModuleType("psutil"))
+
+from streamlit_extension.utils.database import DatabaseManager
+
+METHODS = [
+    "get_connection",
+    "execute_query",
+    "get_clients",
+    "create_client",
+    "update_client",
+    "delete_client",
+    "get_projects",
+    "create_project",
+    "update_project",
+    "delete_project",
+    "get_epics",
+    "get_tasks",
+]
+
+def test_database_manager_annotations():
+    for name in METHODS:
+        func = getattr(DatabaseManager, name)
+        sig = inspect.signature(func)
+        assert sig.return_annotation is not inspect._empty, f"{name} missing return annotation"
+        for param in sig.parameters.values():
+            if param.name == "self":
+                continue
+            assert param.annotation is not inspect._empty, f"{name} missing annotation for {param.name}"
 
EOF
)