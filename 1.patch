 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/streamlit_extension/utils/circuit_breaker.py b/streamlit_extension/utils/circuit_breaker.py
new file mode 100644
index 0000000000000000000000000000000000000000..289cf9a5843c8cd3d999efd5f0e71bbfc0abdcf3
--- /dev/null
+++ b/streamlit_extension/utils/circuit_breaker.py
@@ -0,0 +1,64 @@
+"""Circuit breaker utility for database resilience."""
+import time
+import logging
+from typing import Callable, Type, Any
+
+
+class CircuitBreakerOpenError(Exception):
+    """Raised when the circuit breaker is open."""
+
+
+class CircuitBreaker:
+    """Simple circuit breaker implementation.
+
+    Args:
+        failure_threshold: Number of consecutive failures to open the circuit.
+        recovery_timeout: Seconds to keep the circuit open before allowing
+            another attempt.
+        expected_exception: Exception type that counts as a failure.
+    """
+
+    def __init__(
+        self,
+        failure_threshold: int = 5,
+        recovery_timeout: float = 60.0,
+        expected_exception: Type[Exception] = Exception,
+    ) -> None:
+        self.failure_threshold = failure_threshold
+        self.recovery_timeout = recovery_timeout
+        self.expected_exception = expected_exception
+        self.fail_counter = 0
+        self.opened_at: float | None = None
+        self.logger = logging.getLogger(__name__)
+
+    def is_open(self) -> bool:
+        """Check if the circuit is currently open."""
+        if self.opened_at is None:
+            return False
+        if (time.time() - self.opened_at) >= self.recovery_timeout:
+            # Allow retries after timeout (half-open)
+            return False
+        return True
+
+    def _record_failure(self) -> None:
+        self.fail_counter += 1
+        if self.fail_counter >= self.failure_threshold:
+            self.opened_at = time.time()
+            self.logger.warning("Circuit opened after %s failures", self.fail_counter)
+
+    def _reset(self) -> None:
+        self.fail_counter = 0
+        self.opened_at = None
+
+    def call(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:
+        """Execute ``func`` while applying circuit breaker logic."""
+        if self.is_open():
+            raise CircuitBreakerOpenError("Circuit is open")
+        try:
+            result = func(*args, **kwargs)
+        except self.expected_exception:
+            self._record_failure()
+            raise
+        else:
+            self._reset()
+            return result
diff --git a/streamlit_extension/utils/db_resilience.py b/streamlit_extension/utils/db_resilience.py
new file mode 100644
index 0000000000000000000000000000000000000000..ed9a6bcc994c5a524b416e3137b970609cfecbe6
--- /dev/null
+++ b/streamlit_extension/utils/db_resilience.py
@@ -0,0 +1,53 @@
+"""Database resilience helpers with retry and circuit breaker."""
+from __future__ import annotations
+
+import time
+import logging
+from contextlib import contextmanager
+from typing import Generator, Optional
+
+from .circuit_breaker import CircuitBreaker, CircuitBreakerOpenError
+
+logger = logging.getLogger(__name__)
+
+
+@contextmanager
+def resilient_connection(
+    db_manager,
+    db_name: str = "framework",
+    retries: int = 3,
+    delay: float = 0.1,
+    breaker: Optional[CircuitBreaker] = None,
+) -> Generator:
+    """Context manager that provides a resilient database connection.
+
+    Retries transient failures and opens a circuit after repeated errors to
+    prevent resource exhaustion.
+
+    Args:
+        db_manager: ``DatabaseManager`` instance.
+        db_name: Target database name.
+        retries: Number of retry attempts per call.
+        delay: Delay between retries in seconds.
+        breaker: Optional :class:`CircuitBreaker` instance.
+    """
+
+    breaker = breaker or CircuitBreaker(failure_threshold=retries)
+    last_exc = None
+    for attempt in range(1, retries + 1):
+        try:
+            conn_cm = breaker.call(db_manager.get_connection, db_name)
+            with conn_cm as conn:
+                yield conn
+                return
+        except CircuitBreakerOpenError:
+            logger.error("Circuit breaker open; aborting connection attempts")
+            raise
+        except Exception as exc:  # pragma: no cover - generic
+            last_exc = exc
+            logger.warning("Connection attempt %s failed: %s", attempt, exc)
+            if attempt == retries:
+                raise
+            time.sleep(delay)
+    if last_exc:
+        raise last_exc
diff --git a/tests/test_connection_pool_fix.py b/tests/test_connection_pool_fix.py
new file mode 100644
index 0000000000000000000000000000000000000000..a05f269badb8112ec7d2f71fe9f91ee6fbefcdfd
--- /dev/null
+++ b/tests/test_connection_pool_fix.py
@@ -0,0 +1,52 @@
+import pathlib
+import sys
+import sqlite3
+from contextlib import contextmanager
+
+import pytest
+
+sys.path.append(str(pathlib.Path(__file__).resolve().parents[1]))
+from streamlit_extension.utils.db_resilience import resilient_connection
+from streamlit_extension.utils.circuit_breaker import CircuitBreaker, CircuitBreakerOpenError
+
+
+class FlakyDBManager:
+    """DB manager that fails twice before succeeding."""
+
+    def __init__(self):
+        self.attempts = 0
+
+    @contextmanager
+    def get_connection(self, db_name: str = "framework"):
+        self.attempts += 1
+        if self.attempts <= 2:
+            raise sqlite3.OperationalError("temporary failure")
+        conn = sqlite3.connect(":memory:")
+        try:
+            yield conn
+        finally:
+            conn.close()
+
+
+class FailingDBManager:
+    @contextmanager
+    def get_connection(self, db_name: str = "framework"):
+        raise sqlite3.OperationalError("fail")
+
+
+def test_resilient_connection_recovers_from_transient_errors():
+    manager = FlakyDBManager()
+    with resilient_connection(manager) as conn:
+        assert conn.execute("SELECT 1").fetchone()[0] == 1
+
+
+def test_circuit_breaker_opens_after_failures():
+    manager = FailingDBManager()
+    breaker = CircuitBreaker(failure_threshold=2, recovery_timeout=60)
+    for _ in range(2):
+        with pytest.raises(sqlite3.OperationalError):
+            with resilient_connection(manager, retries=1, breaker=breaker):
+                pass
+    with pytest.raises(CircuitBreakerOpenError):
+        with resilient_connection(manager, retries=1, breaker=breaker):
+            pass
 
EOF
)