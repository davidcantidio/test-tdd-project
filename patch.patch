 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/migration/bidirectional_sync.py b/migration/bidirectional_sync.py
index 7f7525fe15ff4acf3b81cff970eebda574d51ace..6131bea8367029733335dd98dcc218711cbe2100 100644
--- a/migration/bidirectional_sync.py
+++ b/migration/bidirectional_sync.py
@@ -9,50 +9,51 @@ CAMADA 1 - CORE DATA (Sync Bidirecional):
   JSON ↔ Database: name, summary, goals, definition_of_done, labels, tasks
   
 CAMADA 2 - CALCULATED FIELDS (Database → JSON):
   Database → JSON: planned_dates, calculated_duration_days, stats
   
 CAMADA 3 - SYSTEM FIELDS (Database Only):
   Database: created_at, id, assigned_to, points_earned, github_*, etc.
 
 Features:
 - Conflict detection and resolution
 - Change tracking with checksums
 - Rollback capability
 - Comprehensive logging
 """
 
 import sqlite3
 import json
 import hashlib
 from datetime import datetime, date
 from pathlib import Path
 from typing import Dict, Any, List, Optional, Tuple, Set
 from dataclasses import dataclass, asdict
 from enum import Enum
 import sys
 import traceback
+import time
 
 # Add migration to path
 sys.path.append(str(Path(__file__).parent))
 
 from data_base_strategy import DataBaseCalculator, DateBaseStrategy
 from json_enrichment import JSONEnrichmentEngine
 
 # Import the fixed connection pool
 sys.path.append(str(Path(__file__).parent.parent))
 try:
     from duration_system.database_transactions import DatabaseConnectionPool
     CONNECTION_POOL_AVAILABLE = True
 except ImportError:
     CONNECTION_POOL_AVAILABLE = False
 
 
 class SyncDirection(Enum):
     """Direções de sincronização."""
     JSON_TO_DB = "json_to_db"
     DB_TO_JSON = "db_to_json"
     BIDIRECTIONAL = "bidirectional"
 
 
 class ConflictResolution(Enum):
     """Estratégias de resolução de conflitos."""
@@ -135,192 +136,211 @@ class FieldMapping:
         'actual_end_date'
     }
     
     # CAMADA 3: System Fields - Apenas Database (nunca exportados)
     SYSTEM_FIELDS = {
         'id', 'created_at', 'updated_at', 'completed_at', 'deleted_at',
         'assigned_to', 'created_by', 'reviewer_id',
         'points_earned', 'completion_bonus', 'difficulty_level',
         'github_issue_id', 'github_milestone_id', 'github_project_id',
         'estimated_hours', 'actual_hours', 'actual_minutes',
         'sync_status', 'last_json_sync', 'json_checksum'
     }
 
 
 class BidirectionalSyncEngine:
     """
     Engine para sincronização bidirecional entre JSONs e banco de dados.
     
     Funcionalidades:
     - Sincronização inteligente por camadas
     - Detecção e resolução de conflitos
     - Change tracking com checksums
     - Rollback automático em caso de erro
     """
     
-    def __init__(self, db_path: str = "framework.db", 
-                 conflict_resolution: ConflictResolution = ConflictResolution.TIMESTAMP_WINS):
+    def __init__(self, db_path: str = "framework.db",
+                 conflict_resolution: ConflictResolution = ConflictResolution.TIMESTAMP_WINS,
+                 max_retries: int = 3,
+                 retry_delay: float = 0.1):
         self.db_path = db_path
         self.conflict_resolution = conflict_resolution
+        self.max_retries = max_retries
+        self.retry_delay = retry_delay
         
         # Initialize components
         self.enrichment_engine = JSONEnrichmentEngine(DateBaseStrategy.NEXT_MONDAY)
         self.field_mapping = FieldMapping()
         
         # Initialize connection pool if available
         if CONNECTION_POOL_AVAILABLE:
             self.connection_pool = DatabaseConnectionPool(db_path, max_connections=5)
         else:
             self.connection_pool = None
         
         # Sync tracking
         self.sync_session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
     
     def calculate_checksum(self, data: Dict[str, Any]) -> str:
         """Calculate SHA-256 checksum of data for change detection."""
         json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
         return hashlib.sha256(json_str.encode()).hexdigest()
     
     def get_database_connection(self):
         """Get database connection using fixed connection pool."""
         if self.connection_pool:
             # Connection pool connections may not have row_factory set
             class ConnectionWrapper:
                 def __init__(self, pool_conn):
                     self.conn = pool_conn.__enter__()
                     self.conn.row_factory = sqlite3.Row
                     self.pool_conn = pool_conn
                 
                 def __enter__(self):
                     return self.conn
                 
                 def __exit__(self, *args):
                     self.pool_conn.__exit__(*args)
                     
                 def execute(self, *args, **kwargs):
                     return self.conn.execute(*args, **kwargs)
                     
                 def commit(self):
                     return self.conn.commit()
-            
+
+                def rollback(self):
+                    return self.conn.rollback()
+
             return ConnectionWrapper(self.connection_pool.get_connection())
         else:
-            # Fallback to direct connection with improved settings
-            conn = sqlite3.connect(self.db_path, timeout=30.0)
-            conn.row_factory = sqlite3.Row  # Enable column access by name
-            conn.execute("PRAGMA journal_mode=WAL")  # Enable WAL mode for better concurrency
-            conn.execute("PRAGMA busy_timeout=30000")  # 30 second timeout
-            return conn
+            # Fallback to direct connection with retry logic
+            attempts = 0
+            while True:
+                try:
+                    conn = sqlite3.connect(self.db_path, timeout=30.0)
+                    conn.row_factory = sqlite3.Row  # Enable column access by name
+                    conn.execute("PRAGMA journal_mode=WAL")  # Enable WAL mode for better concurrency
+                    conn.execute("PRAGMA busy_timeout=30000")  # 30 second timeout
+                    return conn
+                except sqlite3.OperationalError:
+                    if attempts >= self.max_retries:
+                        raise
+                    attempts += 1
+                    time.sleep(self.retry_delay)
     
     def epic_exists_in_db(self, epic_key: str) -> bool:
         """Check if epic exists in database."""
         with self.get_database_connection() as conn:
             cursor = conn.execute(
                 "SELECT COUNT(*) FROM framework_epics WHERE epic_key = ? AND deleted_at IS NULL",
                 (epic_key,)
             )
             return cursor.fetchone()[0] > 0
     
     def get_epic_from_db(self, epic_key: str) -> Optional[Dict[str, Any]]:
         """Load epic data from database."""
         with self.get_database_connection() as conn:
             # Get epic data
             cursor = conn.execute("""
                 SELECT * FROM framework_epics 
                 WHERE epic_key = ? AND deleted_at IS NULL
             """, (epic_key,))
             
             epic_row = cursor.fetchone()
             if not epic_row:
                 return None
             
             epic_data = dict(epic_row)
             
             # Get tasks for this epic
             cursor = conn.execute("""
                 SELECT * FROM framework_tasks 
                 WHERE epic_id = ? AND status != 'deleted'
                 ORDER BY task_sequence, task_key
             """, (epic_data['id'],))
             
             tasks = [dict(row) for row in cursor.fetchall()]
             epic_data['tasks'] = tasks
             
             return epic_data
     
     def insert_epic_to_db(self, epic_data: Dict[str, Any]) -> int:
         """Insert new epic into database using a single connection."""
         with self.get_database_connection() as conn:
-            # Increase timeout for complex operations
-            conn.execute("PRAGMA busy_timeout=60000")
-
-            # Prepare epic fields
-            epic_key = epic_data.get('id') or epic_data.get('epic_key', '')
-            name = epic_data.get('name', '')
-            summary = epic_data.get('summary', epic_data.get('description', ''))
-            duration_desc = epic_data.get('duration', '')
-
-            # JSON fields
-            goals = json.dumps(epic_data.get('goals', []), ensure_ascii=False)
-            definition_of_done = json.dumps(epic_data.get('definition_of_done', []), ensure_ascii=False)
-            labels = json.dumps(epic_data.get('labels', []), ensure_ascii=False)
-
-            # Calculate dates using enrichment engine
-            enriched = self.enrichment_engine.enrich_epic(epic_data)
-            calc_fields = enriched.get('calculated_fields', {})
-
-            # Insert epic
-            cursor = conn.execute("""
-                INSERT INTO framework_epics (
-                    epic_key, name, summary, duration_description,
+            try:
+                # Increase timeout for complex operations
+                conn.execute("PRAGMA busy_timeout=60000")
+
+                # Prepare epic fields
+                epic_key = epic_data.get('id') or epic_data.get('epic_key', '')
+                name = epic_data.get('name', '')
+                summary = epic_data.get('summary', epic_data.get('description', ''))
+                duration_desc = epic_data.get('duration', '')
+
+                # JSON fields
+                goals = json.dumps(epic_data.get('goals', []), ensure_ascii=False)
+                definition_of_done = json.dumps(epic_data.get('definition_of_done', []), ensure_ascii=False)
+                labels = json.dumps(epic_data.get('labels', []), ensure_ascii=False)
+
+                # Calculate dates using enrichment engine
+                enriched = self.enrichment_engine.enrich_epic(epic_data)
+                calc_fields = enriched.get('calculated_fields', {})
+
+                # Insert epic
+                cursor = conn.execute("""
+                    INSERT INTO framework_epics (
+                        epic_key, name, summary, duration_description,
+                        goals, definition_of_done, labels,
+                        planned_start_date, planned_end_date, calculated_duration_days,
+                        tdd_enabled, methodology,
+                        performance_constraints, quality_gates, automation_hooks, checklist_epic_level,
+                        sync_status, json_checksum, created_at, updated_at
+                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'synced', ?, ?, ?)
+                """, (
+                    epic_key, name, summary, duration_desc,
                     goals, definition_of_done, labels,
-                    planned_start_date, planned_end_date, calculated_duration_days,
-                    tdd_enabled, methodology,
-                    performance_constraints, quality_gates, automation_hooks, checklist_epic_level,
-                    sync_status, json_checksum, created_at, updated_at
-                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'synced', ?, ?, ?)
-            """, (
-                epic_key, name, summary, duration_desc,
-                goals, definition_of_done, labels,
-                calc_fields.get('planned_start_date'), calc_fields.get('planned_end_date'),
-                calc_fields.get('calculated_duration_days', 0),
-                epic_data.get('tdd_enabled', True), epic_data.get('methodology', 'Test-Driven Development'),
-                json.dumps(epic_data.get('performance_constraints', {}), ensure_ascii=False),
-                json.dumps(epic_data.get('quality_gates', {}), ensure_ascii=False),
-                json.dumps(epic_data.get('automation_hooks', {}), ensure_ascii=False),
-                json.dumps(epic_data.get('checklist_epic_level', []), ensure_ascii=False),
-                self.calculate_checksum(epic_data),
-                datetime.now().isoformat(), datetime.now().isoformat()
-            ))
-
-            epic_id = cursor.lastrowid
-
-            # Insert tasks using the same connection
-            self.insert_tasks_to_db(epic_id, epic_data.get('tasks', []), conn=conn)
-
-            conn.commit()
-            return epic_id
+                    calc_fields.get('planned_start_date'), calc_fields.get('planned_end_date'),
+                    calc_fields.get('calculated_duration_days', 0),
+                    epic_data.get('tdd_enabled', True), epic_data.get('methodology', 'Test-Driven Development'),
+                    json.dumps(epic_data.get('performance_constraints', {}), ensure_ascii=False),
+                    json.dumps(epic_data.get('quality_gates', {}), ensure_ascii=False),
+                    json.dumps(epic_data.get('automation_hooks', {}), ensure_ascii=False),
+                    json.dumps(epic_data.get('checklist_epic_level', []), ensure_ascii=False),
+                    self.calculate_checksum(epic_data),
+                    datetime.now().isoformat(), datetime.now().isoformat()
+                ))
+
+                epic_id = cursor.lastrowid
+
+                # Insert tasks using the same connection
+                self.insert_tasks_to_db(epic_id, epic_data.get('tasks', []), conn=conn)
+
+                conn.commit()
+                return epic_id
+            except Exception:
+                conn.rollback()
+                raise
 
     def insert_tasks_to_db(self, epic_id: int, tasks: List[Dict[str, Any]], conn: sqlite3.Connection | None = None):
         """Insert tasks for an epic into database.
 
         Uses batch insertion and supports connection reuse to avoid nested
         transactions that could lead to deadlocks.
         """
 
         # Prepare data for batch insertion
         task_rows: List[Tuple[Any, ...]] = []
         for i, task in enumerate(tasks):
             task_key = task.get('id', f"{i+1}")
             title = task.get('title', '')
             description = task.get('description', '')
             tdd_phase = task.get('tdd_phase')
             estimate_minutes = task.get('estimate_minutes', 60)
             story_points = task.get('story_points', 1)
             branch = task.get('branch', '')
 
             # JSON fields
             test_specs = json.dumps(task.get('test_specs', []), ensure_ascii=False)
             acceptance_criteria = json.dumps(task.get('acceptance_criteria', []), ensure_ascii=False)
             deliverables = json.dumps(task.get('deliverables', []), ensure_ascii=False)
             files_touched = json.dumps(task.get('files_touched', []), ensure_ascii=False)
             test_plan = json.dumps(task.get('test_plan', []), ensure_ascii=False)
 
EOF
)