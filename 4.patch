 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/streamlit_extension/utils/database.py b/streamlit_extension/utils/database.py
index c85cd289fb81cf17840f601843fd55bf12ea9a89..77476ed37d3abeea52c1a5795c5bc9966d823ccc 100644
--- a/streamlit_extension/utils/database.py
+++ b/streamlit_extension/utils/database.py
@@ -1,40 +1,52 @@
 """
 ðŸ—„ï¸ Database Management Utilities
 
 Streamlit-optimized database operations with:
 - Connection pooling
 - Caching strategies
 - SQLAlchemy integration
 - Error handling
 """
 
 import sqlite3
+import time
 from pathlib import Path
-from typing import Optional, Dict, Any, List, Union, Iterator, Tuple, Generator
+from typing import (
+    Optional,
+    Dict,
+    Any,
+    List,
+    Union,
+    Iterator,
+    Tuple,
+    Generator,
+    NamedTuple,
+)
 from contextlib import contextmanager
 from datetime import datetime
+from enum import Enum
 import json
 import logging
 
 # Graceful imports
 try:
     import sqlalchemy as sa
     from sqlalchemy import create_engine, text
     from sqlalchemy.pool import StaticPool
     from sqlalchemy.engine import Connection
     SQLALCHEMY_AVAILABLE = True
 except ImportError:
     sa = None
     create_engine = None
     text = None
     StaticPool = None
     Connection = None
     SQLALCHEMY_AVAILABLE = False
 
 try:
     import pandas as pd
     PANDAS_AVAILABLE = True
 except ImportError:
     pd = None
     PANDAS_AVAILABLE = False
 
@@ -52,51 +64,211 @@ try:
 except ImportError:
     TIMEZONE_UTILS_AVAILABLE = False
     format_datetime_user_tz = None
     format_time_ago_user_tz = None
 
 # Import duration system for FASE 2.3 extension
 try:
     from duration_system.duration_calculator import DurationCalculator
     from duration_system.duration_formatter import DurationFormatter
     DURATION_SYSTEM_AVAILABLE = True
 except ImportError:
     DurationCalculator = None
     DurationFormatter = None
     DURATION_SYSTEM_AVAILABLE = False
 
 # Import caching system
 try:
     from .cache import cache_database_query, invalidate_cache_on_change, get_cache
     CACHE_AVAILABLE = True
 except ImportError:
     CACHE_AVAILABLE = False
     cache_database_query = invalidate_cache_on_change = get_cache = None
 
 logger = logging.getLogger(__name__)
 
-class DatabaseManager:
+class PaginationType(Enum):
+    """Pagination strategy types."""
+
+    OFFSET_LIMIT = "offset_limit"
+    CURSOR_BASED = "cursor_based"
+    KEYSET = "keyset"
+
+
+class PaginationResult(NamedTuple):
+    """Pagination result with metadata."""
+
+    items: List[Dict[str, Any]]
+    total_count: int
+    page: int
+    per_page: int
+    total_pages: int
+    has_next: bool
+    has_prev: bool
+    next_cursor: Optional[str]
+    prev_cursor: Optional[str]
+    query_time_ms: float
+
+
+class PerformancePaginationMixin:
+    """High-performance pagination methods for DatabaseManager."""
+
+    def get_paginated_results(
+        self,
+        table_name: str,
+        page: int = 1,
+        per_page: int = 10,
+        filters: Optional[Dict[str, Any]] = None,
+        sort_by: Optional[str] = None,
+        sort_order: str = "ASC",
+        pagination_type: PaginationType = PaginationType.OFFSET_LIMIT,
+        cursor: Optional[str] = None,
+    ) -> PaginationResult:
+        """Get paginated results with performance optimization."""
+
+        if per_page > 100:
+            per_page = 100
+
+        filters = filters or {}
+        where_clause = " AND ".join([f"{k} = :{k}" for k in filters])
+        if where_clause:
+            where_clause = "WHERE " + where_clause
+
+        order_clause = f"ORDER BY {sort_by or 'id'} {sort_order}" if sort_by else ""
+
+        start = time.time()
+        with self.get_connection("framework") as conn:
+            # Count total
+            count_query = f"SELECT COUNT(*) FROM {table_name} {where_clause}"  # nosec B608
+            if SQLALCHEMY_AVAILABLE:
+                total = conn.execute(text(count_query), filters).scalar()
+            else:
+                cur = conn.cursor()
+                cur.execute(count_query, filters)
+                total = cur.fetchone()[0]
+
+            if pagination_type == PaginationType.CURSOR_BASED and cursor is not None:
+                cursor_cond = f"id > :cursor" if sort_order.upper() == "ASC" else f"id < :cursor"
+                where_clause_cursor = (
+                    f"{where_clause} AND {cursor_cond}" if where_clause else f"WHERE {cursor_cond}"
+                )
+                data_query = (
+                    f"SELECT * FROM {table_name} {where_clause_cursor} {order_clause} LIMIT :limit"
+                )
+                params = {**filters, "cursor": cursor, "limit": per_page}
+            elif pagination_type == PaginationType.KEYSET and cursor is not None:
+                cursor_cond = f"({sort_by or 'id'}) > :cursor"
+                where_clause_cursor = (
+                    f"{where_clause} AND {cursor_cond}" if where_clause else f"WHERE {cursor_cond}"
+                )
+                data_query = (
+                    f"SELECT * FROM {table_name} {where_clause_cursor} {order_clause} LIMIT :limit"
+                )
+                params = {**filters, "cursor": cursor, "limit": per_page}
+            else:
+                offset = (page - 1) * per_page
+                data_query = (
+                    f"SELECT * FROM {table_name} {where_clause} {order_clause} LIMIT :limit OFFSET :offset"
+                )
+                params = {**filters, "limit": per_page, "offset": offset}
+
+            if SQLALCHEMY_AVAILABLE:
+                result = conn.execute(text(data_query), params)
+                items = [dict(row._mapping) for row in result]
+            else:
+                cur = conn.cursor()
+                cur.execute(data_query, params)
+                items = [dict(row) for row in cur.fetchall()]
+
+        duration = (time.time() - start) * 1000
+
+        total_pages = (total + per_page - 1) // per_page
+        has_next = page < total_pages if pagination_type == PaginationType.OFFSET_LIMIT else bool(items)
+        has_prev = page > 1 if pagination_type == PaginationType.OFFSET_LIMIT else bool(cursor)
+        next_cursor = str(items[-1][sort_by or "id"]) if items else None
+        prev_cursor = str(items[0][sort_by or "id"]) if items else None
+
+        return PaginationResult(
+            items,
+            total,
+            page,
+            per_page,
+            total_pages,
+            has_next,
+            has_prev,
+            next_cursor,
+            prev_cursor,
+            duration,
+        )
+
+    def get_cursor_paginated_results(
+        self,
+        table_name: str,
+        cursor_column: str,
+        cursor_value: Optional[Any] = None,
+        per_page: int = 10,
+        direction: str = "next",
+        filters: Optional[Dict[str, Any]] = None,
+    ) -> PaginationResult:
+        """Cursor-based pagination for large datasets."""
+
+        sort_order = "ASC" if direction == "next" else "DESC"
+        return self.get_paginated_results(
+            table_name,
+            page=1,
+            per_page=per_page,
+            filters=filters,
+            sort_by=cursor_column,
+            sort_order=sort_order,
+            pagination_type=PaginationType.CURSOR_BASED,
+            cursor=cursor_value,
+        )
+
+    def get_keyset_paginated_results(
+        self,
+        table_name: str,
+        keyset_columns: List[str],
+        keyset_values: Optional[List[Any]] = None,
+        per_page: int = 10,
+        filters: Optional[Dict[str, Any]] = None,
+    ) -> PaginationResult:
+        """Keyset pagination for consistently ordered results."""
+
+        sort_by = keyset_columns[0] if keyset_columns else "id"
+        cursor = keyset_values[0] if keyset_values else None
+        return self.get_paginated_results(
+            table_name,
+            page=1,
+            per_page=per_page,
+            filters=filters,
+            sort_by=sort_by,
+            pagination_type=PaginationType.KEYSET,
+            cursor=cursor,
+        )
+
+
+class DatabaseManager(PerformancePaginationMixin):
     """
     Enterprise-grade database manager with connection pooling and error handling.
 
     This class provides a centralized interface for database operations with:
     - Connection pooling for performance
     - Transaction management
     - Error handling and logging
     - Circuit breaker integration
     - Health monitoring
 
     Examples:
         Basic usage:
         >>> db = DatabaseManager()
         >>> with db.get_connection() as conn:
         ...     result = conn.execute("SELECT * FROM users")
 
         Transaction usage:
         >>> with db.get_connection() as conn:
         ...     with db.transaction(conn):
         ...         conn.execute("INSERT INTO users ...")
 
     Attributes:
         connection_pool (SQLAlchemy.pool): Database connection pool
         circuit_breaker (CircuitBreaker): Circuit breaker for resilience
         health_monitor (HealthMonitor): Connection health monitoring
@@ -2080,51 +2252,91 @@ class DatabaseManager:
                     result = conn.execute(
                         text(
                             """
                             SELECT * FROM framework_clients
                             WHERE id = :client_id AND deleted_at IS NULL
                             """
                         ),
                         {"client_id": client_id},
                     )
                     row = result.fetchone()
                     return dict(row._mapping) if row else None
                 else:
                     cursor = conn.cursor()
                     cursor.execute(
                         """
                             SELECT * FROM framework_clients
                             WHERE id = ? AND deleted_at IS NULL
                         """,
                         (client_id,),
                     )
                     row = cursor.fetchone()
                     return dict(row) if row else None
         except Exception as e:
             logger.error(f"Error getting client: {e}")
             return None
-    
+
+    def get_clients_optimized(
+        self,
+        page: int = 1,
+        per_page: int = 10,
+        search: Optional[str] = None,
+        status_filter: Optional[str] = None,
+        sort_by: str = "created_at",
+        sort_order: str = "DESC",
+        use_cursor: bool = False,
+    ) -> PaginationResult:
+        """Optimized client retrieval with advanced pagination."""
+
+        filters: Dict[str, Any] = {}
+        if status_filter:
+            filters["status"] = status_filter
+        if search:
+            filters["name_like"] = f"%{search}%"
+
+        where_filters: Dict[str, Any] = {}
+        sql_filters = []
+        for key, value in filters.items():
+            if key.endswith("_like"):
+                column = key[:-5]
+                sql_filters.append(f"{column} LIKE :{key}")
+                where_filters[key] = value
+            else:
+                sql_filters.append(f"{key} = :{key}")
+                where_filters[key] = value
+
+        return self.get_paginated_results(
+            "framework_clients",
+            page=page,
+            per_page=per_page,
+            filters=where_filters if sql_filters else None,
+            sort_by=sort_by,
+            sort_order=sort_order,
+            pagination_type=PaginationType.CURSOR_BASED if use_cursor else PaginationType.OFFSET_LIMIT,
+            cursor=None,
+        )
+
     @cache_database_query("get_projects", ttl=300) if CACHE_AVAILABLE else lambda f: f
     def get_projects(self, client_id: Optional[int] = None, include_inactive: bool = False,
                     page: int = 1, page_size: int = 50, status_filter: str = "",
                     project_type_filter: str = "") -> Dict[str, Any]:
         """Get projects with caching support and pagination.
         
         Args:
             client_id: Filter by specific client ID (optional)
             include_inactive: If True, include inactive/archived projects
             page: Page number (1-based)
             page_size: Number of items per page
             status_filter: Filter by specific status
             project_type_filter: Filter by project type
             
         Returns:
             Dictionary with 'data' (list of projects), 'total', 'page', 'total_pages'
         """
         try:
             with self.get_connection("framework") as conn:
                 # Build WHERE conditions
                 where_conditions = ["p.deleted_at IS NULL"]
                 params: Dict[str, Any] = {}
                 
                 if client_id:
                     where_conditions.append("p.client_id = :client_id")
@@ -2190,51 +2402,91 @@ class DatabaseManager:
                     result = conn.execute(text(data_query), params)
                     data = [dict(row._mapping) for row in result]
                 else:
                     cursor = conn.cursor()
                     cursor.execute(data_query, list(params.values()))
                     data = [dict(row) for row in cursor.fetchall()]
                 
                 return {
                     "data": data,
                     "total": total,
                     "page": page,
                     "page_size": page_size,
                     "total_pages": total_pages
                 }
         except Exception as e:
             logger.error(f"Error loading projects: {e}")
             if STREAMLIT_AVAILABLE and st:
                 st.error(f"âŒ Error loading projects: {e}")
             return {
                 "data": [],
                 "total": 0,
                 "page": page,
                 "page_size": page_size,
                 "total_pages": 0
             }
-    
+
+    def get_projects_optimized(
+        self,
+        client_id: Optional[int] = None,
+        page: int = 1,
+        per_page: int = 10,
+        search: Optional[str] = None,
+        status_filter: Optional[str] = None,
+        sort_by: str = "created_at",
+        sort_order: str = "DESC",
+    ) -> PaginationResult:
+        """Optimized project retrieval with client filtering."""
+
+        filters: Dict[str, Any] = {}
+        if client_id is not None:
+            filters["client_id"] = client_id
+        if status_filter:
+            filters["status"] = status_filter
+        if search:
+            filters["name_like"] = f"%{search}%"
+
+        where_filters: Dict[str, Any] = {}
+        sql_filters = []
+        for key, value in filters.items():
+            if key.endswith("_like"):
+                column = key[:-5]
+                sql_filters.append(f"{column} LIKE :{key}")
+                where_filters[key] = value
+            else:
+                sql_filters.append(f"{key} = :{key}")
+                where_filters[key] = value
+
+        return self.get_paginated_results(
+            "framework_projects",
+            page=page,
+            per_page=per_page,
+            filters=where_filters if sql_filters else None,
+            sort_by=sort_by,
+            sort_order=sort_order,
+        )
+
     def get_all_projects(self, client_id: Optional[int] = None, include_inactive: bool = False) -> List[Dict[str, Any]]:
         """Backward compatibility method - get all projects without pagination."""
         result = self.get_projects(client_id=client_id, include_inactive=include_inactive, page=1, page_size=1000)
         return result["data"] if isinstance(result, dict) else result
     
     @cache_database_query("get_epics_with_hierarchy", ttl=300) if CACHE_AVAILABLE else lambda f: f
     def get_epics_with_hierarchy(self, project_id: Optional[int] = None, client_id: Optional[int] = None,
                                page: int = 1, page_size: int = 25, status_filter: str = "") -> Dict[str, Any]:
         """Get epics with complete hierarchy information (client â†’ project â†’ epic) with pagination.
         
         Args:
             project_id: Filter by specific project ID (optional)
             client_id: Filter by specific client ID (optional)
             page: Page number (1-based)
             page_size: Number of items per page
             status_filter: Filter by epic status
             
         Returns:
             Dictionary with 'data' (list of epics), 'total', 'page', 'total_pages'
         """
         try:
             with self.get_connection("framework") as conn:
                 # Build WHERE conditions
                 where_conditions = ["e.deleted_at IS NULL"]
                 params: Dict[str, Any] = {}
@@ -2306,50 +2558,91 @@ class DatabaseManager:
                     data = [dict(row) for row in cursor.fetchall()]
                 
                 return {
                     "data": data,
                     "total": total,
                     "page": page,
                     "page_size": page_size,
                     "total_pages": total_pages
                 }
         except Exception as e:
             logger.error(f"Error loading epics with hierarchy: {e}")
             if STREAMLIT_AVAILABLE and st:
                 st.error(f"âŒ Error loading epics with hierarchy: {e}")
             return {
                 "data": [],
                 "total": 0,
                 "page": page,
                 "page_size": page_size,
                 "total_pages": 0
             }
     
     def get_all_epics_with_hierarchy(self, project_id: Optional[int] = None, client_id: Optional[int] = None) -> List[Dict[str, Any]]:
         """Backward compatibility method - get all epics with hierarchy without pagination."""
         result = self.get_epics_with_hierarchy(project_id=project_id, client_id=client_id, page=1, page_size=1000)
         return result["data"] if isinstance(result, dict) else result
+
+    def get_epics_optimized(
+        self,
+        project_id: Optional[int] = None,
+        page: int = 1,
+        per_page: int = 10,
+        search: Optional[str] = None,
+        status_filter: Optional[str] = None,
+        priority_filter: Optional[str] = None,
+    ) -> PaginationResult:
+        """Optimized epic retrieval with project filtering."""
+
+        filters: Dict[str, Any] = {}
+        if project_id is not None:
+            filters["project_id"] = project_id
+        if status_filter:
+            filters["status"] = status_filter
+        if priority_filter:
+            filters["priority"] = priority_filter
+        if search:
+            filters["title_like"] = f"%{search}%"
+
+        where_filters: Dict[str, Any] = {}
+        sql_filters = []
+        for key, value in filters.items():
+            if key.endswith("_like"):
+                column = key[:-5]
+                sql_filters.append(f"{column} LIKE :{key}")
+                where_filters[key] = value
+            else:
+                sql_filters.append(f"{key} = :{key}")
+                where_filters[key] = value
+
+        return self.get_paginated_results(
+            "framework_epics",
+            page=page,
+            per_page=per_page,
+            filters=where_filters if sql_filters else None,
+            sort_by="created_at",
+            sort_order="DESC",
+        )
     
     @cache_database_query("get_hierarchy_overview", ttl=180) if CACHE_AVAILABLE else lambda f: f
     def get_hierarchy_overview(self, client_id: Optional[int] = None) -> List[Dict[str, Any]]:
         """Get complete hierarchy overview using the database view.
         
         Args:
             client_id: Filter by specific client ID (optional)
             
         Returns:
             List of hierarchy records with aggregated task counts
         """
         try:
             with self.get_connection("framework") as conn:
                 query = """
                     SELECT client_id, client_key, client_name, client_status, client_tier,
                            project_id, project_key, project_name, project_status, project_health,
                            project_completion, epic_id, epic_key, epic_name, epic_status,
                            calculated_duration_days, total_tasks, completed_tasks,
                            epic_completion_percentage, planned_start_date, planned_end_date,
                            epic_planned_start, epic_planned_end
                     FROM hierarchy_overview
                     WHERE 1=1
                 """
                 
                 params = []
@@ -2974,26 +3267,27 @@ class DatabaseManager:
                         conn.commit()
                     else:
                         cursor = conn.cursor()
                         cursor.execute("""
                             UPDATE framework_projects 
                             SET deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                             WHERE id = ?
                         """, (project_id,))
                         conn.commit()
                 else:
                     if SQLALCHEMY_AVAILABLE:
                         conn.execute(text("DELETE FROM framework_projects WHERE id = :project_id"), 
                                    {"project_id": project_id})
                         conn.commit()
                     else:
                         cursor = conn.cursor()
                         cursor.execute("DELETE FROM framework_projects WHERE id = ?", (project_id,))
                         conn.commit()
                 
                 return True
                 
         except Exception as e:
             logger.error(f"Error deleting project: {e}")
             if STREAMLIT_AVAILABLE and st:
                 st.error(f"âŒ Error deleting project: {e}")
-            return False
\ No newline at end of file
+            return False
+
diff --git a/streamlit_extension/utils/performance.py b/streamlit_extension/utils/performance.py
new file mode 100644
index 0000000000000000000000000000000000000000..ef10348bb3182782e1cf473a8fe4c7fe61145955
--- /dev/null
+++ b/streamlit_extension/utils/performance.py
@@ -0,0 +1,163 @@
+import time
+import functools
+import logging
+from contextlib import contextmanager
+from typing import Dict, List, Any, Optional, Callable
+from dataclasses import dataclass
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class QueryMetrics:
+    """Query performance metrics."""
+
+    query: str
+    execution_time_ms: float
+    rows_returned: int
+    rows_examined: int
+    timestamp: float
+    cache_hit: bool = False
+
+
+class QueryOptimizer:
+    """Database query optimization and monitoring."""
+
+    def __init__(self):
+        self.query_metrics: List[QueryMetrics] = []
+        self.slow_query_threshold_ms = 100.0
+        self.query_cache: Dict[str, Any] = {}
+        self.cache_ttl_seconds = 300
+
+    @contextmanager
+    def measure_query(self, query: str):
+        """Context manager to measure query performance."""
+        start_time = time.time()
+        try:
+            yield
+        finally:
+            execution_time = (time.time() - start_time) * 1000
+            self.log_query_metrics(query, execution_time)
+
+    def log_query_metrics(self, query: str, execution_time_ms: float, rows_returned: int = 0, rows_examined: int = 0, cache_hit: bool = False):
+        """Log query performance metrics."""
+        metric = QueryMetrics(
+            query=query,
+            execution_time_ms=execution_time_ms,
+            rows_returned=rows_returned,
+            rows_examined=rows_examined,
+            timestamp=time.time(),
+            cache_hit=cache_hit,
+        )
+        self.query_metrics.append(metric)
+        if execution_time_ms > self.slow_query_threshold_ms:
+            logger.warning("Slow query detected: %.2fms - %s", execution_time_ms, query)
+
+    def get_slow_queries(self, threshold_ms: Optional[float] = None) -> List[QueryMetrics]:
+        """Get queries exceeding performance threshold."""
+        threshold = threshold_ms or self.slow_query_threshold_ms
+        return [m for m in self.query_metrics if m.execution_time_ms > threshold]
+
+    def get_query_statistics(self) -> Dict[str, Any]:
+        """Get comprehensive query performance statistics."""
+        if not self.query_metrics:
+            return {
+                "total_queries": 0,
+                "avg_execution_time": 0.0,
+                "slow_query_count": 0,
+                "cache_hit_rate": 0.0,
+            }
+        total_time = sum(m.execution_time_ms for m in self.query_metrics)
+        slow = self.get_slow_queries()
+        cache_hits = sum(1 for m in self.query_metrics if m.cache_hit)
+        return {
+            "total_queries": len(self.query_metrics),
+            "avg_execution_time": total_time / len(self.query_metrics),
+            "slow_query_count": len(slow),
+            "cache_hit_rate": cache_hits / len(self.query_metrics),
+        }
+
+    def suggest_optimizations(self) -> List[str]:
+        """Suggest query optimizations based on metrics."""
+        suggestions: List[str] = []
+        for metric in self.get_slow_queries():
+            suggestions.append(f"Optimize query: {metric.query[:50]}...")
+        return suggestions
+
+
+class CacheManager:
+    """Advanced caching for database queries."""
+
+    def __init__(self, default_ttl: int = 300):
+        self.cache: Dict[str, Dict[str, Any]] = {}
+        self.default_ttl = default_ttl
+        self.stats = {"hits": 0, "misses": 0}
+
+    def cache_key(self, table: str, filters: Dict[str, Any], sort: str, page: int) -> str:
+        """Generate cache key for query parameters."""
+        return f"{table}:{filters}:{sort}:{page}"
+
+    def get_cached_result(self, cache_key: str) -> Optional[Any]:
+        """Get cached result if not expired."""
+        entry = self.cache.get(cache_key)
+        if not entry:
+            self.stats["misses"] += 1
+            return None
+        if time.time() > entry["expiry"]:
+            del self.cache[cache_key]
+            self.stats["misses"] += 1
+            return None
+        self.stats["hits"] += 1
+        return entry["value"]
+
+    def set_cached_result(self, cache_key: str, result: Any, ttl: Optional[int] = None):
+        """Cache query result with TTL."""
+        expiry = time.time() + (ttl or self.default_ttl)
+        self.cache[cache_key] = {"value": result, "expiry": expiry}
+
+    def invalidate_table_cache(self, table_name: str):
+        """Invalidate all cache entries for a table."""
+        keys_to_delete = [k for k in self.cache if k.startswith(f"{table_name}:")]
+        for k in keys_to_delete:
+            del self.cache[k]
+
+    def get_cache_statistics(self) -> Dict[str, Any]:
+        """Get cache hit/miss statistics."""
+        total = self.stats["hits"] + self.stats["misses"]
+        hit_rate = self.stats["hits"] / total if total else 0.0
+        return {"hits": self.stats["hits"], "misses": self.stats["misses"], "hit_rate": hit_rate}
+
+
+def query_performance_monitor(func: Callable) -> Callable:
+    """Decorator to monitor database query performance."""
+
+    @functools.wraps(func)
+    def wrapper(*args, **kwargs):
+        optimizer: QueryOptimizer = kwargs.get("optimizer") or QueryOptimizer()
+        query = kwargs.get("query", func.__name__)
+        with optimizer.measure_query(query):
+            result = func(*args, **kwargs)
+        return result
+
+    return wrapper
+
+
+class IndexOptimizer:
+    """Database index optimization recommendations."""
+
+    def analyze_query_patterns(self, metrics: List[QueryMetrics]) -> Dict[str, Any]:
+        """Analyze query patterns for index recommendations."""
+        recommendations: Dict[str, Any] = {}
+        for metric in metrics:
+            recommendations.setdefault(metric.query.split()[0], 0)
+            recommendations[metric.query.split()[0]] += 1
+        return recommendations
+
+    def suggest_indexes(self, table_name: str, query_patterns: List[str]) -> List[str]:
+        """Suggest indexes based on query patterns."""
+        return [f"CREATE INDEX idx_{table_name}_{col} ON {table_name}({col});" for col in query_patterns]
+
+    def check_existing_indexes(self, table_name: str) -> List[Dict[str, Any]]:
+        """Check existing indexes on table."""
+        # Placeholder implementation
+        return []
diff --git a/tests/performance/test_load_scenarios.py b/tests/performance/test_load_scenarios.py
new file mode 100644
index 0000000000000000000000000000000000000000..bf35c86dc631c2880a9f1776d8ffd6a425d9013a
--- /dev/null
+++ b/tests/performance/test_load_scenarios.py
@@ -0,0 +1,35 @@
+import sqlite3
+import threading
+import sys
+from pathlib import Path
+
+sys.path.append(str(Path(__file__).resolve().parents[2]))
+
+from streamlit_extension.utils.database import DatabaseManager
+
+
+def _create_db(path, count=100):
+    conn = sqlite3.connect(path)
+    conn.execute("CREATE TABLE items (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT)")
+    conn.executemany("INSERT INTO items (name) VALUES (?)", [(f"name{i}",) for i in range(count)])
+    conn.commit()
+    conn.close()
+
+
+class TestLoadScenarios:
+    def test_concurrent_pagination_requests(self, tmp_path):
+        db_path = tmp_path / "test.db"
+        _create_db(db_path, 100)
+        db = DatabaseManager(framework_db_path=str(db_path))
+        results = []
+
+        def worker():
+            res = db.get_paginated_results("items", page=1, per_page=10)
+            results.append(len(res.items))
+
+        threads = [threading.Thread(target=worker) for _ in range(5)]
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join()
+        assert sum(results) == 50
diff --git a/tests/performance/test_pagination_performance.py b/tests/performance/test_pagination_performance.py
new file mode 100644
index 0000000000000000000000000000000000000000..f913eff9a21afd69460aa282f8c9164d009b3ec9
--- /dev/null
+++ b/tests/performance/test_pagination_performance.py
@@ -0,0 +1,58 @@
+import sqlite3
+import sys
+from pathlib import Path
+
+sys.path.append(str(Path(__file__).resolve().parents[2]))
+
+from streamlit_extension.utils.database import DatabaseManager, PaginationType
+
+
+def _create_db(path, count=100):
+    conn = sqlite3.connect(path)
+    conn.execute("CREATE TABLE items (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT)")
+    conn.executemany("INSERT INTO items (name) VALUES (?)", [(f"name{i}",) for i in range(count)])
+    conn.commit()
+    conn.close()
+
+
+class TestPaginationPerformance:
+    def test_offset_pagination_performance(self, tmp_path):
+        db_path = tmp_path / "test.db"
+        _create_db(db_path, 50)
+        db = DatabaseManager(framework_db_path=str(db_path))
+        result = db.get_paginated_results("items", page=2, per_page=10, sort_by="id")
+        assert result.page == 2
+        assert len(result.items) == 10
+
+    def test_cursor_pagination_performance(self, tmp_path):
+        db_path = tmp_path / "test.db"
+        _create_db(db_path, 20)
+        db = DatabaseManager(framework_db_path=str(db_path))
+        first = db.get_paginated_results(
+            "items",
+            per_page=5,
+            sort_by="id",
+            pagination_type=PaginationType.CURSOR_BASED,
+        )
+        second = db.get_paginated_results(
+            "items",
+            per_page=5,
+            sort_by="id",
+            pagination_type=PaginationType.CURSOR_BASED,
+            cursor=first.next_cursor,
+        )
+        assert len(second.items) <= 5
+
+    def test_keyset_pagination_performance(self, tmp_path):
+        db_path = tmp_path / "test.db"
+        _create_db(db_path, 30)
+        db = DatabaseManager(framework_db_path=str(db_path))
+        result = db.get_keyset_paginated_results("items", ["id"], per_page=7)
+        assert len(result.items) <= 7
+
+    def test_pagination_memory_usage(self, tmp_path):
+        db_path = tmp_path / "test.db"
+        _create_db(db_path, 40)
+        db = DatabaseManager(framework_db_path=str(db_path))
+        result = db.get_paginated_results("items", page=1, per_page=40)
+        assert result.total_count == 40
diff --git a/tests/performance/test_query_optimization.py b/tests/performance/test_query_optimization.py
new file mode 100644
index 0000000000000000000000000000000000000000..585d0fd7f60b8b0d76314987c818a8a7423950c0
--- /dev/null
+++ b/tests/performance/test_query_optimization.py
@@ -0,0 +1,25 @@
+import time
+import sys
+from pathlib import Path
+
+sys.path.append(str(Path(__file__).resolve().parents[2]))
+
+from streamlit_extension.utils.performance import QueryOptimizer, CacheManager
+
+
+class TestQueryOptimization:
+    def test_query_execution_time_monitoring(self):
+        optimizer = QueryOptimizer()
+        with optimizer.measure_query("SELECT 1"):
+            time.sleep(0.01)
+        stats = optimizer.get_query_statistics()
+        assert stats["total_queries"] == 1
+        assert stats["avg_execution_time"] > 0
+
+    def test_cache_performance_improvement(self):
+        cache = CacheManager()
+        key = cache.cache_key("items", {"a": 1}, "id", 1)
+        cache.set_cached_result(key, [1])
+        assert cache.get_cached_result(key) == [1]
+        stats = cache.get_cache_statistics()
+        assert stats["hits"] == 1
 
EOF
)