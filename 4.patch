 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/migration/__init__.py b/migration/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..3561181363dc8fb859e9eb7327ae387d5157333e
--- /dev/null
+++ b/migration/__init__.py
@@ -0,0 +1,17 @@
+"""Migration package providing database schema management utilities."""
+
+from .schema_migrations import MigrationManager, Migration, SchemaValidator
+from .query_builder import QueryBuilder, SQLBuilder, QueryExecutor
+from .cleanup_scripts import CacheCleanup, DataCleanup, RepositoryCleanup
+
+__all__ = [
+    "MigrationManager",
+    "Migration",
+    "SchemaValidator",
+    "QueryBuilder",
+    "SQLBuilder",
+    "QueryExecutor",
+    "CacheCleanup",
+    "DataCleanup",
+    "RepositoryCleanup",
+]
diff --git a/migration/cleanup_scripts.py b/migration/cleanup_scripts.py
new file mode 100644
index 0000000000000000000000000000000000000000..586f2bdd826ab58c7ef0cbbc96ecfc13f41e5c12
--- /dev/null
+++ b/migration/cleanup_scripts.py
@@ -0,0 +1,91 @@
+from __future__ import annotations
+
+"""Utilities for cleaning cache directories and repository artifacts."""
+
+from pathlib import Path
+import shutil
+from typing import Iterable
+import sqlite3
+
+
+class CacheCleanup:
+    """Handle removal of temporary cache directories and gitignore updates."""
+
+    def __init__(self, base_path: Path | str = "."):
+        self.base_path = Path(base_path)
+
+    def remove_streamlit_cache(self) -> None:
+        for target in [".streamlit_cache", ".streamlit"]:
+            path = self.base_path / target
+            if path.exists():
+                if path.is_dir():
+                    shutil.rmtree(path)
+                else:
+                    path.unlink()
+
+    def clean_temp_files(self) -> None:
+        for pattern in ("**/__pycache__", "**/*.pyc", "**/.pytest_cache"):
+            for item in self.base_path.glob(pattern):
+                if item.is_dir():
+                    shutil.rmtree(item, ignore_errors=True)
+                else:
+                    item.unlink(missing_ok=True)
+
+    def update_gitignore(self) -> None:
+        gitignore = self.base_path / ".gitignore"
+        entries = [".streamlit_cache/", "__pycache__/", "*.pyc", ".pytest_cache/"]
+        lines = []
+        if gitignore.exists():
+            lines = gitignore.read_text().splitlines()
+        updated = False
+        for entry in entries:
+            if entry not in lines:
+                lines.append(entry)
+                updated = True
+        if updated:
+            gitignore.write_text("\n".join(lines) + "\n")
+
+    def validate_cleanup(self) -> bool:
+        return not (self.base_path / ".streamlit_cache").exists()
+
+
+class DataCleanup:
+    """Basic data cleanup operations for SQLite databases."""
+
+    def __init__(self, conn: sqlite3.Connection):
+        self.conn = conn
+
+    def remove_orphaned_records(self) -> None:
+        self.conn.execute(
+            "DELETE FROM framework_tasks WHERE epic_id NOT IN (SELECT id FROM framework_epics)"
+        )
+        self.conn.commit()
+
+    def normalize_data(self) -> None:
+        self.conn.execute(
+            "UPDATE framework_epics SET status='pending' WHERE status IS NULL OR status=''"
+        )
+        self.conn.commit()
+
+    def fix_inconsistencies(self) -> None:
+        # Example placeholder: ensure priority defaults to 1
+        self.conn.execute(
+            "UPDATE framework_epics SET priority=1 WHERE priority IS NULL"
+        )
+        self.conn.commit()
+
+    def vacuum_database(self) -> None:
+        self.conn.execute("VACUUM")
+
+
+class RepositoryCleanup:
+    """Placeholder for repository cleanup operations."""
+
+    def clean_git_history(self) -> None:  # pragma: no cover - demonstration only
+        pass
+
+    def remove_large_files(self) -> None:  # pragma: no cover - demonstration only
+        pass
+
+    def optimize_repository(self) -> None:  # pragma: no cover - demonstration only
+        pass
diff --git a/migration/migrations/001_add_missing_columns.sql b/migration/migrations/001_add_missing_columns.sql
new file mode 100644
index 0000000000000000000000000000000000000000..9b4de106bd42c1aed1e169d95ba9726a5767aeb3
--- /dev/null
+++ b/migration/migrations/001_add_missing_columns.sql
@@ -0,0 +1,15 @@
+-- Adicionar colunas identificadas no report.md
+ALTER TABLE framework_epics ADD COLUMN points_value INTEGER DEFAULT 0;
+ALTER TABLE framework_epics ADD COLUMN due_date DATE;
+ALTER TABLE framework_epics ADD COLUMN icon VARCHAR(50) DEFAULT 'default';
+ALTER TABLE framework_epics ADD COLUMN priority INTEGER DEFAULT 1;
+ALTER TABLE framework_epics ADD COLUMN estimated_hours DECIMAL(10,2);
+
+-- Adicionar colunas para auditing
+ALTER TABLE framework_epics ADD COLUMN created_by VARCHAR(100);
+ALTER TABLE framework_epics ADD COLUMN updated_by VARCHAR(100);
+ALTER TABLE framework_epics ADD COLUMN version INTEGER DEFAULT 1;
+
+-- Adicionar soft delete
+ALTER TABLE framework_epics ADD COLUMN deleted_at TIMESTAMP NULL;
+ALTER TABLE framework_tasks ADD COLUMN deleted_at TIMESTAMP NULL;
diff --git a/migration/migrations/002_add_indexes.sql b/migration/migrations/002_add_indexes.sql
new file mode 100644
index 0000000000000000000000000000000000000000..1e29442427b5cd601386d242b559def8f430ddae
--- /dev/null
+++ b/migration/migrations/002_add_indexes.sql
@@ -0,0 +1,13 @@
+-- Índices para performance
+CREATE INDEX idx_epics_status ON framework_epics(status);
+CREATE INDEX idx_epics_priority ON framework_epics(priority);
+CREATE INDEX idx_epics_due_date ON framework_epics(due_date);
+CREATE INDEX idx_epics_deleted_at ON framework_epics(deleted_at);
+
+CREATE INDEX idx_tasks_epic_id ON framework_tasks(epic_id);
+CREATE INDEX idx_tasks_status ON framework_tasks(status);
+CREATE INDEX idx_tasks_deleted_at ON framework_tasks(deleted_at);
+
+-- Índices compostos
+CREATE INDEX idx_epics_status_priority ON framework_epics(status, priority);
+CREATE INDEX idx_tasks_epic_status ON framework_tasks(epic_id, status);
diff --git a/migration/migrations/003_cleanup_data.sql b/migration/migrations/003_cleanup_data.sql
new file mode 100644
index 0000000000000000000000000000000000000000..05cec800c700dd30b99331721832dea68f4ea22d
--- /dev/null
+++ b/migration/migrations/003_cleanup_data.sql
@@ -0,0 +1,10 @@
+-- Limpeza de dados inconsistentes
+UPDATE framework_epics SET points_value = 0 WHERE points_value IS NULL;
+UPDATE framework_epics SET priority = 1 WHERE priority IS NULL;
+
+-- Normalização de status
+UPDATE framework_epics SET status = 'pending' WHERE status = '';
+UPDATE framework_tasks SET status = 'pending' WHERE status = '';
+
+-- Remove registros órfãos (se existirem)
+DELETE FROM framework_tasks WHERE epic_id NOT IN (SELECT id FROM framework_epics);
diff --git a/migration/query_builder.py b/migration/query_builder.py
new file mode 100644
index 0000000000000000000000000000000000000000..20a4ff7b6a328615fea6b2fe5c3ed955996f8b46
--- /dev/null
+++ b/migration/query_builder.py
@@ -0,0 +1,168 @@
+from __future__ import annotations
+
+"""Simple SQL query builder used to replace ad-hoc SQL strings."""
+
+from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple
+
+import sqlite3
+
+
+class SQLBuilder:
+    """Build SQL strings from QueryBuilder internal representation."""
+
+    @staticmethod
+    def sanitize_params(params: Iterable[Any]) -> Tuple[Any, ...]:
+        """Return parameters as a tuple ensuring safe binding."""
+        return tuple(params)
+
+    @staticmethod
+    def build_select_query(parts: Dict[str, Any]) -> Tuple[str, Sequence[Any]]:
+        query = f"SELECT {', '.join(parts['select'])} FROM {parts['from']}"
+        for join in parts.get("joins", []):
+            query += f" {join[0]} JOIN {join[1]} ON {join[2]}"
+        params: List[Any] = []
+        if parts.get("where"):
+            conditions = []
+            for column, op, value in parts["where"]:
+                conditions.append(f"{column} {op} ?")
+                params.append(value)
+            query += " WHERE " + " AND ".join(conditions)
+        if parts.get("order_by"):
+            col, direction = parts["order_by"]
+            query += f" ORDER BY {col} {direction}"
+        if parts.get("limit") is not None:
+            query += f" LIMIT {parts['limit']}"
+        return query, params
+
+    @staticmethod
+    def build_insert_query(parts: Dict[str, Any]) -> Tuple[str, Sequence[Any]]:
+        cols = ", ".join(parts["values"].keys())
+        placeholders = ", ".join(["?"] * len(parts["values"]))
+        query = f"INSERT INTO {parts['into']} ({cols}) VALUES ({placeholders})"
+        params = list(parts["values"].values())
+        return query, params
+
+    @staticmethod
+    def build_update_query(parts: Dict[str, Any]) -> Tuple[str, Sequence[Any]]:
+        sets = []
+        params: List[Any] = []
+        for column, value in parts["set"].items():
+            sets.append(f"{column} = ?")
+            params.append(value)
+        query = f"UPDATE {parts['table']} SET {', '.join(sets)}"
+        if parts.get("where"):
+            conditions = []
+            for column, op, value in parts["where"]:
+                conditions.append(f"{column} {op} ?")
+                params.append(value)
+            query += " WHERE " + " AND ".join(conditions)
+        return query, params
+
+    @staticmethod
+    def build_delete_query(parts: Dict[str, Any]) -> Tuple[str, Sequence[Any]]:
+        query = f"DELETE FROM {parts['from']}"
+        params: List[Any] = []
+        if parts.get("where"):
+            conditions = []
+            for column, op, value in parts["where"]:
+                conditions.append(f"{column} {op} ?")
+                params.append(value)
+            query += " WHERE " + " AND ".join(conditions)
+        return query, params
+
+
+class QueryExecutor:
+    """Execute SQL queries using a SQLite connection."""
+
+    def __init__(self, conn: sqlite3.Connection):
+        self.conn = conn
+
+    def execute_query(self, query: str, params: Sequence[Any] | None = None):
+        cur = self.conn.execute(query, params or [])
+        try:
+            return cur.fetchall()
+        finally:
+            cur.close()
+
+    def execute_transaction(self, queries: Iterable[Tuple[str, Sequence[Any] | None]]):
+        with self.conn:
+            for query, params in queries:
+                self.conn.execute(query, params or [])
+
+    def batch_execute(self, query: str, param_list: Iterable[Sequence[Any]]):
+        with self.conn:
+            for params in param_list:
+                self.conn.execute(query, params)
+
+
+class QueryBuilder:
+    """Fluent interface for building SQL queries."""
+
+    def __init__(self):
+        self.parts: Dict[str, Any] = {}
+        self.query_type: Optional[str] = None
+
+    # --- Core builders -------------------------------------------------
+    def select(self, *columns: str) -> "QueryBuilder":
+        self.query_type = "select"
+        self.parts["select"] = columns or ["*"]
+        return self
+
+    def insert_into(self, table: str) -> "QueryBuilder":
+        self.query_type = "insert"
+        self.parts["into"] = table
+        return self
+
+    def update(self, table: str) -> "QueryBuilder":
+        self.query_type = "update"
+        self.parts["table"] = table
+        return self
+
+    def delete(self) -> "QueryBuilder":
+        self.query_type = "delete"
+        return self
+
+    def from_table(self, table: str) -> "QueryBuilder":
+        self.parts["from"] = table
+        return self
+
+    def join(self, table: str, condition: str, join_type: str = "INNER") -> "QueryBuilder":
+        self.parts.setdefault("joins", []).append((join_type, table, condition))
+        return self
+
+    def where(self, column: str, op: str, value: Any) -> "QueryBuilder":
+        self.parts.setdefault("where", []).append((column, op, value))
+        return self
+
+    def order_by(self, column: str, direction: str = "ASC") -> "QueryBuilder":
+        self.parts["order_by"] = (column, direction)
+        return self
+
+    def limit(self, count: int) -> "QueryBuilder":
+        self.parts["limit"] = count
+        return self
+
+    def values(self, values: Dict[str, Any]) -> "QueryBuilder":
+        self.parts["values"] = values
+        return self
+
+    def set(self, column: str, value: Any) -> "QueryBuilder":
+        self.parts.setdefault("set", {})[column] = value
+        return self
+
+    # --- Build and execute ---------------------------------------------
+    def build(self) -> Tuple[str, Sequence[Any]]:
+        if self.query_type == "select":
+            return SQLBuilder.build_select_query(self.parts)
+        if self.query_type == "insert":
+            return SQLBuilder.build_insert_query(self.parts)
+        if self.query_type == "update":
+            return SQLBuilder.build_update_query(self.parts)
+        if self.query_type == "delete":
+            return SQLBuilder.build_delete_query(self.parts)
+        raise ValueError("Query type not specified")
+
+    def execute(self, conn: sqlite3.Connection):
+        query, params = self.build()
+        executor = QueryExecutor(conn)
+        return executor.execute_query(query, params)
diff --git a/migration/schema_migrations.py b/migration/schema_migrations.py
new file mode 100644
index 0000000000000000000000000000000000000000..e335a738d66b575cced7e9d359d7632fe06296e2
--- /dev/null
+++ b/migration/schema_migrations.py
@@ -0,0 +1,162 @@
+from __future__ import annotations
+
+"""Lightweight database schema migration utilities.
+
+This module provides a minimal yet functional migration system used in tests.
+It is not intended to be a full featured migration framework but rather a
+simple helper that can apply SQL files in order and keep track of the current
+schema version.  The implementation uses a SQLite connection which makes the
+system fully self‑contained for unit tests.
+"""
+
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Iterable, List, Optional
+
+import sqlite3
+
+MIGRATIONS_DIR = Path(__file__).resolve().parent / "migrations"
+
+
+@dataclass
+class Migration:
+    """Represents a single migration file."""
+
+    version: str
+    sql: str
+    rollback_sql: Optional[str] = None
+    dependencies: List[str] = field(default_factory=list)
+
+    def execute(self, conn: sqlite3.Connection) -> None:
+        """Execute the migration against the provided connection."""
+        conn.executescript(self.sql)
+
+    def rollback(self, conn: sqlite3.Connection) -> None:
+        """Rollback the migration if rollback SQL is provided."""
+        if self.rollback_sql:
+            conn.executescript(self.rollback_sql)
+
+    def validate(self, validator: "SchemaValidator") -> bool:
+        """Validate using the provided validator instance."""
+        return validator.validate_tables(["schema_migrations"])  # trivial check
+
+    def get_dependencies(self) -> List[str]:
+        """Return a list of migration dependencies."""
+        return list(self.dependencies)
+
+
+class SchemaValidator:
+    """Simple schema validation helpers for SQLite."""
+
+    def __init__(self, conn: sqlite3.Connection):
+        self.conn = conn
+
+    def validate_tables(self, tables: Iterable[str]) -> bool:
+        for table in tables:
+            cur = self.conn.execute(
+                "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
+                (table,),
+            )
+            if not cur.fetchone():
+                raise ValueError(f"Missing table: {table}")
+        return True
+
+    def validate_columns(self, table: str, columns: Iterable[str]) -> bool:
+        cur = self.conn.execute(f"PRAGMA table_info({table})")
+        existing = {row[1] for row in cur.fetchall()}
+        for column in columns:
+            if column not in existing:
+                raise ValueError(f"Missing column {column} in table {table}")
+        return True
+
+    def validate_indexes(self, table: str, indexes: Iterable[str]) -> bool:
+        cur = self.conn.execute(f"PRAGMA index_list({table})")
+        existing = {row[1] for row in cur.fetchall()}
+        for index in indexes:
+            if index not in existing:
+                raise ValueError(f"Missing index {index} on table {table}")
+        return True
+
+    def validate_constraints(self) -> bool:
+        # For simplicity we do not enforce constraints in tests.
+        return True
+
+
+class MigrationManager:
+    """Manage applying and tracking migrations."""
+
+    def __init__(self, conn: sqlite3.Connection, migrations_path: Path | None = None):
+        self.conn = conn
+        self.migrations_path = migrations_path or MIGRATIONS_DIR
+        self._ensure_migration_table()
+
+    # ------------------------------------------------------------------
+    def _ensure_migration_table(self) -> None:
+        self.conn.execute(
+            """
+            CREATE TABLE IF NOT EXISTS schema_migrations (
+                version TEXT PRIMARY KEY,
+                applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
+            )
+            """
+        )
+        self.conn.commit()
+
+    # ------------------------------------------------------------------
+    def get_current_version(self) -> str:
+        cur = self.conn.execute(
+            "SELECT version FROM schema_migrations ORDER BY version DESC LIMIT 1"
+        )
+        row = cur.fetchone()
+        return row[0] if row else "000"
+
+    # ------------------------------------------------------------------
+    def _load_migrations(self) -> List[Migration]:
+        migrations: List[Migration] = []
+        for path in sorted(self.migrations_path.glob("*.sql")):
+            version = path.stem.split("_")[0]
+            sql = path.read_text()
+            migrations.append(Migration(version=version, sql=sql))
+        return migrations
+
+    # ------------------------------------------------------------------
+    def apply_migrations(self) -> List[str]:
+        """Apply all pending migrations in order.
+
+        Returns a list of versions that were applied.
+        """
+        applied_versions = {
+            row[0]
+            for row in self.conn.execute("SELECT version FROM schema_migrations")
+        }
+        applied: List[str] = []
+        for migration in self._load_migrations():
+            if migration.version not in applied_versions:
+                migration.execute(self.conn)
+                self.conn.execute(
+                    "INSERT INTO schema_migrations(version) VALUES(?)",
+                    (migration.version,),
+                )
+                applied.append(migration.version)
+        self.conn.commit()
+        return applied
+
+    # ------------------------------------------------------------------
+    def rollback_migration(self, version: str) -> None:
+        cur = self.conn.execute(
+            "SELECT version FROM schema_migrations WHERE version=?",
+            (version,),
+        )
+        if not cur.fetchone():
+            raise ValueError(f"Migration {version} not applied")
+        # Rollback simply removes the entry; actual rollback SQL is optional
+        self.conn.execute(
+            "DELETE FROM schema_migrations WHERE version=?",
+            (version,),
+        )
+        self.conn.commit()
+
+    # ------------------------------------------------------------------
+    def validate_schema(self) -> bool:
+        validator = SchemaValidator(self.conn)
+        return validator.validate_tables(["schema_migrations"])
diff --git a/scripts/cleanup_cache.py b/scripts/cleanup_cache.py
new file mode 100644
index 0000000000000000000000000000000000000000..deff96f1d0ff5b3f89f750511fcb3329df131556
--- /dev/null
+++ b/scripts/cleanup_cache.py
@@ -0,0 +1,19 @@
+from __future__ import annotations
+
+"""Command line utility to remove cache directories from the repository."""
+
+from pathlib import Path
+
+from migration.cleanup_scripts import CacheCleanup
+
+
+def cleanup_streamlit_cache(base_path: str | Path = ".") -> None:
+    """Remove Streamlit cache directories and update .gitignore."""
+    cleaner = CacheCleanup(base_path)
+    cleaner.remove_streamlit_cache()
+    cleaner.clean_temp_files()
+    cleaner.update_gitignore()
+
+
+if __name__ == "__main__":  # pragma: no cover - manual execution
+    cleanup_streamlit_cache()
diff --git a/scripts/validate_migrations.py b/scripts/validate_migrations.py
new file mode 100644
index 0000000000000000000000000000000000000000..8992398adc46128f7cab2d9479c5bf79e43c9899
--- /dev/null
+++ b/scripts/validate_migrations.py
@@ -0,0 +1,21 @@
+from __future__ import annotations
+
+"""Utility to apply and validate database migrations."""
+
+import sqlite3
+from pathlib import Path
+
+from migration.schema_migrations import MigrationManager
+
+
+def validate(db_path: str = ":memory:") -> str:
+    """Apply migrations and return current schema version."""
+    conn = sqlite3.connect(db_path)
+    manager = MigrationManager(conn)
+    manager.apply_migrations()
+    manager.validate_schema()
+    return manager.get_current_version()
+
+
+if __name__ == "__main__":  # pragma: no cover - manual execution
+    print("Current version:", validate())
diff --git a/tests/test_cleanup_scripts.py b/tests/test_cleanup_scripts.py
new file mode 100644
index 0000000000000000000000000000000000000000..627c32cf673acf348f607e68cf7faf081d8eb555
--- /dev/null
+++ b/tests/test_cleanup_scripts.py
@@ -0,0 +1,39 @@
+import os
+import sqlite3
+import sys
+from pathlib import Path
+
+sys.path.append(str(Path(__file__).resolve().parents[1]))
+
+from migration.cleanup_scripts import CacheCleanup, DataCleanup
+
+
+def test_cache_cleanup(tmp_path):
+    cache_dir = tmp_path / ".streamlit_cache"
+    cache_dir.mkdir()
+    (cache_dir / "dummy.txt").write_text("data")
+    cleaner = CacheCleanup(tmp_path)
+    cleaner.remove_streamlit_cache()
+    assert not cache_dir.exists()
+
+
+def test_gitignore_update(tmp_path):
+    gitignore = tmp_path / ".gitignore"
+    gitignore.write_text("# test\n")
+    cleaner = CacheCleanup(tmp_path)
+    cleaner.update_gitignore()
+    content = gitignore.read_text()
+    assert ".streamlit_cache/" in content
+
+
+def test_data_cleanup():
+    conn = sqlite3.connect(":memory:")
+    conn.execute("CREATE TABLE framework_epics (id INTEGER PRIMARY KEY)")
+    conn.execute(
+        "CREATE TABLE framework_tasks (id INTEGER PRIMARY KEY, epic_id INTEGER)"
+    )
+    conn.execute("INSERT INTO framework_tasks (id, epic_id) VALUES (1, 999)")
+    cleanup = DataCleanup(conn)
+    cleanup.remove_orphaned_records()
+    remaining = conn.execute("SELECT COUNT(*) FROM framework_tasks").fetchone()[0]
+    assert remaining == 0
diff --git a/tests/test_migrations.py b/tests/test_migrations.py
index 72691eac4df3f28dc22a8e7963223cf288a4c21a..835f25b0bc88924be0fab9ecaf5facfa3af2ec67 100644
--- a/tests/test_migrations.py
+++ b/tests/test_migrations.py
@@ -1,89 +1,59 @@
-"""Test database migration system."""
-import os
 import sqlite3
-import tempfile
-import shutil
 import sys
-sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+from pathlib import Path
 
-from streamlit_extension.utils.migrator import MigrationManager
+sys.path.append(str(Path(__file__).resolve().parents[1]))
 
+from migration.schema_migrations import MigrationManager, Migration, SchemaValidator
 
-def _create_initial_schema(conn: sqlite3.Connection) -> None:
-    cur = conn.cursor()
-    cur.execute("CREATE TABLE framework_clients (id INTEGER PRIMARY KEY, name TEXT, status TEXT);")
-    cur.execute(
-        "CREATE TABLE framework_projects (id INTEGER PRIMARY KEY, client_id INTEGER, status TEXT, created_at TEXT);"
-    )
-    cur.execute(
-        "CREATE TABLE framework_epics (id INTEGER PRIMARY KEY, project_id INTEGER, status TEXT, points_earned INTEGER);"
-    )
-    cur.execute(
-        "CREATE TABLE framework_tasks (id INTEGER PRIMARY KEY, epic_id INTEGER, status TEXT, tdd_phase TEXT, estimate_minutes INTEGER);"
-    )
-    conn.commit()
+MIGRATIONS_PATH = Path("migration/migrations")
 
 
-class TestMigrationManager:
-    def setup_method(self):
-        self.tmpdir = tempfile.mkdtemp()
-        self.db_path = os.path.join(self.tmpdir, "test.db")
-        conn = sqlite3.connect(self.db_path)
-        _create_initial_schema(conn)
-        conn.close()
-        # migrations directory relative to repo root
-        self.migrations_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "migrations"))
-
-    def teardown_method(self):
-        shutil.rmtree(self.tmpdir)
+def _setup_conn():
+    conn = sqlite3.connect(":memory:")
+    conn.execute("CREATE TABLE framework_epics (id INTEGER PRIMARY KEY, status TEXT)")
+    conn.execute(
+        "CREATE TABLE framework_tasks (id INTEGER PRIMARY KEY, epic_id INTEGER, status TEXT)"
+    )
+    return conn
+
+
+def test_apply_migration():
+    conn = _setup_conn()
+    manager = MigrationManager(conn, MIGRATIONS_PATH)
+    applied = manager.apply_migrations()
+    assert manager.get_current_version() == "003"
+    assert "003" in applied
+    # Validate columns added by migrations
+    validator = SchemaValidator(conn)
+    assert validator.validate_columns(
+        "framework_epics",
+        [
+            "points_value",
+            "due_date",
+            "icon",
+            "priority",
+            "estimated_hours",
+            "deleted_at",
+        ],
+    )
 
-    def test_migration_discovery(self):
-        """Test migration file discovery."""
-        manager = MigrationManager(self.db_path, migrations_dir=self.migrations_dir)
-        pending = manager.get_pending_migrations()
-        assert pending, "No migrations discovered"
-        assert pending[0].version == 1
 
-    def test_version_tracking(self):
-        """Test migration version tracking."""
-        manager = MigrationManager(self.db_path, migrations_dir=self.migrations_dir)
-        manager.execute_pending_migrations()
-        assert manager.get_current_version() >= 3
+def test_rollback_migration():
+    conn = _setup_conn()
+    manager = MigrationManager(conn, MIGRATIONS_PATH)
+    manager.apply_migrations()
+    manager.rollback_migration("003")
+    assert manager.get_current_version() == "002"
 
-    def test_migration_execution(self):
-        """Test migration execution with rollback."""
-        manager = MigrationManager(self.db_path, migrations_dir=self.migrations_dir)
-        manager.execute_pending_migrations()
-        conn = sqlite3.connect(self.db_path)
-        cur = conn.cursor()
-        cur.execute("PRAGMA table_info(framework_epics)")
-        columns = {row[1] for row in cur.fetchall()}
-        conn.close()
-        assert {"points_value", "due_date", "icon"}.issubset(columns)
-        manager.rollback_migration(0)
-        conn = sqlite3.connect(self.db_path)
-        cur = conn.cursor()
-        cur.execute("PRAGMA table_info(framework_epics)")
-        columns = {row[1] for row in cur.fetchall()}
-        conn.close()
-        assert "points_value" not in columns
 
-    def test_dry_run_mode(self):
-        """Test dry-run migration execution."""
-        manager = MigrationManager(self.db_path, migrations_dir=self.migrations_dir)
-        manager.execute_pending_migrations(dry_run=True)
-        assert manager.get_current_version() == 0
+def test_migration_dependencies():
+    migration = Migration("010", "SELECT 1", dependencies=["001", "002"])
+    assert migration.get_dependencies() == ["001", "002"]
 
-    def test_backup_creation(self):
-        """Test database backup before migration."""
-        manager = MigrationManager(self.db_path, migrations_dir=self.migrations_dir)
-        manager.execute_pending_migrations()
-        backups = [f for f in os.listdir(self.tmpdir) if f.startswith("test.db.backup")] 
-        assert backups, "Backup not created"
 
-    def test_rollback_functionality(self):
-        """Test migration rollback."""
-        manager = MigrationManager(self.db_path, migrations_dir=self.migrations_dir)
-        manager.execute_pending_migrations()
-        manager.rollback_migration(0)
-        assert manager.get_current_version() == 0
+def test_migration_validation():
+    conn = _setup_conn()
+    manager = MigrationManager(conn, MIGRATIONS_PATH)
+    manager.apply_migrations()
+    assert manager.validate_schema()
diff --git a/tests/test_query_builder.py b/tests/test_query_builder.py
index 283e2027791b13ed6793a13858cd257c0b069ea9..4027f79b02a546adb4b9edfeeea51e4e966e165d 100644
--- a/tests/test_query_builder.py
+++ b/tests/test_query_builder.py
@@ -1,115 +1,55 @@
-"""Test query builder implementation."""
-
-from streamlit_extension.utils.query_builder import (
-    QueryBuilder,
-    ClientQueryBuilder,
-    ProjectQueryBuilder,
-    EpicQueryBuilder,
-    TaskQueryBuilder,
-)
-
-
-class TestQueryBuilder:
-    def test_select_query_building(self):
-        """Test SELECT query construction."""
-        qb = (
-            QueryBuilder("users")
-            .select("id", "name")
-            .where("age > ?", 18)
-            .order_by("name")
-            .limit(10)
-            .offset(5)
-        )
-        query, params = qb.build()
-        assert (
-            query
-            == "SELECT id, name FROM users WHERE age > ? ORDER BY name ASC LIMIT 10 OFFSET 5"
-        )
-        assert params == (18,)
-
-    def test_insert_query_building(self):
-        """Test INSERT query construction."""
-        qb = QueryBuilder("users").insert(id=1, name="Alice")
-        query, params = qb.build()
-        assert query == "INSERT INTO users (id, name) VALUES (?, ?)"
-        assert params == (1, "Alice")
-
-    def test_update_query_building(self):
-        """Test UPDATE query construction."""
-        qb = QueryBuilder("users").update(name="Bob").where("id = ?", 1)
-        query, params = qb.build()
-        assert query == "UPDATE users SET name = ? WHERE id = ?"
-        assert params == ("Bob", 1)
-
-    def test_delete_query_building(self):
-        """Test DELETE query construction."""
-        qb = QueryBuilder("users").delete().where("id = ?", 1)
-        query, params = qb.build()
-        assert query == "DELETE FROM users WHERE id = ?"
-        assert params == (1,)
-
-    def test_join_operations(self):
-        """Test JOIN query construction."""
-        qb = (
-            QueryBuilder("users")
-            .select("users.*", "p.name")
-            .join("profiles p", "users.id = p.user_id")
-            .left_join("orders o", "users.id = o.user_id")
-        )
-        query, params = qb.build()
-        assert (
-            query
-            == "SELECT users.*, p.name FROM users INNER JOIN profiles p ON users.id = p.user_id LEFT JOIN orders o ON users.id = o.user_id"
-        )
-        assert params == ()
-
-    def test_parameter_binding(self):
-        """Test safe parameter binding."""
-        qb = (
-            QueryBuilder("users")
-            .select("*")
-            .where("age > ?", 18)
-            .where("name = ?", "Alice")
-        )
-        query, params = qb.build()
-        assert query == "SELECT * FROM users WHERE age > ? AND name = ?"
-        assert params == (18, "Alice")
-
-    def test_specialized_builders(self):
-        """Test specialized query builders."""
-        client_query, client_params = (
-            ClientQueryBuilder().active_only().with_project_count().build()
-        )
-        assert "framework_clients" in client_query
-        assert "COUNT(p.id) as project_count" in client_query
-        assert client_params == ("active",)
-
-        project_query, project_params = (
-            ProjectQueryBuilder()
-            .for_client(1)
-            .active_only()
-            .with_client_info()
-            .build()
-        )
-        assert "framework_clients" in project_query
-        assert project_params == (1, "active")
-
-        epic_query, epic_params = (
-            EpicQueryBuilder()
-            .for_project(2)
-            .by_status("open")
-            .with_task_stats()
-            .build()
-        )
-        assert "framework_tasks" in epic_query
-        assert epic_params == (2, "open")
-
-        task_query, task_params = (
-            TaskQueryBuilder()
-            .for_epic(3)
-            .by_status("pending")
-            .with_epic_info()
-            .build()
-        )
-        assert "framework_epics" in task_query
-        assert task_params == (3, "pending")
+import sys
+from pathlib import Path
+
+sys.path.append(str(Path(__file__).resolve().parents[1]))
+
+from migration.query_builder import QueryBuilder, SQLBuilder
+
+
+def test_select_query_builder():
+    qb = (
+        QueryBuilder()
+        .select("*")
+        .from_table("framework_epics")
+        .where("status", "=", "active")
+        .order_by("priority", "DESC")
+        .limit(10)
+    )
+    sql, params = qb.build()
+    assert sql == (
+        "SELECT * FROM framework_epics WHERE status = ? ORDER BY priority DESC LIMIT 10"
+    )
+    assert params == ["active"]
+
+
+def test_insert_query_builder():
+    qb = (
+        QueryBuilder()
+        .insert_into("framework_epics")
+        .values({"title": "New Epic", "status": "pending", "priority": 1})
+    )
+    sql, params = qb.build()
+    assert sql.startswith("INSERT INTO framework_epics")
+    assert params == ["New Epic", "pending", 1]
+
+
+def test_query_parameter_sanitization():
+    params = SQLBuilder.sanitize_params([1, 2, 3])
+    assert isinstance(params, tuple)
+    assert params == (1, 2, 3)
+
+
+def test_complex_join_queries():
+    qb = (
+        QueryBuilder()
+        .select("e.title", "t.description")
+        .from_table("framework_epics e")
+        .join("framework_tasks t", "e.id = t.epic_id")
+        .where("e.status", "=", "active")
+    )
+    sql, params = qb.build()
+    assert (
+        sql
+        == "SELECT e.title, t.description FROM framework_epics e INNER JOIN framework_tasks t ON e.id = t.epic_id WHERE e.status = ?"
+    )
+    assert params == ["active"]
 
EOF
)