 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/backups/migrations_backup_20250818/001_add_missing_columns.sql b/backups/migrations_backup_20250818/001_add_missing_columns.sql
deleted file mode 100644
index ed9736a0f73570d2d67b8915bf704766ab373c8f..0000000000000000000000000000000000000000
--- a/backups/migrations_backup_20250818/001_add_missing_columns.sql
+++ /dev/null
@@ -1,24 +0,0 @@
--- Migration: Add missing columns identified in report.md
--- Date: 2025-01-15
--- Description: Add points_value, due_date, icon columns to framework_epics table
-
--- Add points_value column for epic scoring
-ALTER TABLE framework_epics 
-ADD COLUMN points_value INTEGER DEFAULT 0;
-
--- Add due_date column for deadline tracking
-ALTER TABLE framework_epics 
-ADD COLUMN due_date TEXT;
-
--- Add icon column for visual identification
-ALTER TABLE framework_epics 
-ADD COLUMN icon TEXT DEFAULT 'ðŸ“‹';
-
--- Update existing records with default values
-UPDATE framework_epics 
-SET points_value = COALESCE(points_earned, 0)
-WHERE points_value IS NULL;
-
--- Add constraints
-CREATE INDEX idx_epics_due_date ON framework_epics(due_date);
-CREATE INDEX idx_epics_points_value ON framework_epics(points_value);
diff --git a/backups/migrations_backup_20250818/002_create_indexes.sql b/backups/migrations_backup_20250818/002_create_indexes.sql
deleted file mode 100644
index 40d5ef62c140f68245a08997083f90e57472df0b..0000000000000000000000000000000000000000
--- a/backups/migrations_backup_20250818/002_create_indexes.sql
+++ /dev/null
@@ -1,19 +0,0 @@
--- Migration: Create performance indexes
--- Date: 2025-01-15
--- Description: Add indexes for heavy queries identified in report.md
-
--- Client search optimization
-CREATE INDEX idx_clients_name_search ON framework_clients(name);
-CREATE INDEX idx_clients_status_active ON framework_clients(status) WHERE status = 'active';
-
--- Project filtering optimization
-CREATE INDEX idx_projects_client_status ON framework_projects(client_id, status);
-CREATE INDEX idx_projects_created_date ON framework_projects(created_at);
-
--- Epic progress calculation optimization
-CREATE INDEX idx_epics_project_status ON framework_epics(project_id, status);
-CREATE INDEX idx_epics_progress_calc ON framework_epics(project_id, status, points_earned);
-
--- Task query optimization
-CREATE INDEX idx_tasks_epic_status ON framework_tasks(epic_id, status);
-CREATE INDEX idx_tasks_tdd_phase ON framework_tasks(tdd_phase, status);
diff --git a/backups/migrations_backup_20250818/003_data_cleanup.sql b/backups/migrations_backup_20250818/003_data_cleanup.sql
deleted file mode 100644
index 00281c2f1e8f887e24ddac154e7ef4fff8ed39ad..0000000000000000000000000000000000000000
--- a/backups/migrations_backup_20250818/003_data_cleanup.sql
+++ /dev/null
@@ -1,17 +0,0 @@
--- Migration: Data cleanup and normalization
--- Date: 2025-01-15
--- Description: Clean up data inconsistencies
-
--- Normalize status values
-UPDATE framework_clients SET status = 'active' WHERE status IN ('Active', 'ACTIVE');
-UPDATE framework_projects SET status = 'active' WHERE status IN ('Active', 'ACTIVE');
-UPDATE framework_epics SET status = 'active' WHERE status IN ('Active', 'ACTIVE');
-
--- Clean up null/empty values
-UPDATE framework_epics SET icon = 'ðŸ“‹' WHERE icon IS NULL OR icon = '';
-UPDATE framework_tasks SET estimate_minutes = 60 WHERE estimate_minutes IS NULL OR estimate_minutes = 0;
-
--- Ensure referential integrity
-DELETE FROM framework_projects WHERE client_id NOT IN (SELECT id FROM framework_clients);
-DELETE FROM framework_epics WHERE project_id NOT IN (SELECT id FROM framework_projects);
-DELETE FROM framework_tasks WHERE epic_id NOT IN (SELECT id FROM framework_epics);
diff --git a/debug_services.py b/debug_services.py
deleted file mode 100644
index ebd518e45881b1f532c5c992aec9c24cc2966d45..0000000000000000000000000000000000000000
--- a/debug_services.py
+++ /dev/null
@@ -1,193 +0,0 @@
-#!/usr/bin/env python3
-"""
-ðŸ” Debug Script - Service Container Investigation
-Analyzes why services are registered but not created.
-"""
-
-import sys
-from pathlib import Path
-
-# Add project root to path
-project_root = str(Path(__file__).parent.resolve())
-if project_root not in sys.path:
-    sys.path.insert(0, project_root)
-
-import logging
-logging.basicConfig(level=logging.DEBUG, format='%(levelname)s - %(name)s - %(message)s')
-
-def test_database_connection():
-    """Test database layer connectivity"""
-    print("ðŸ—„ï¸ Testing Database Connection...")
-    try:
-        from streamlit_extension.database import get_connection, check_health
-        conn = get_connection()
-        if conn:
-            print("âœ… Database connection OK")
-            try:
-                conn.close()
-            except:
-                pass
-        
-        health = check_health()
-        print(f"âœ… Database health: {health}")
-        return True
-    except Exception as e:
-        print(f"âŒ Database connection failed: {e}")
-        import traceback
-        traceback.print_exc()
-        return False
-
-def test_database_manager():
-    """Test legacy DatabaseManager"""
-    print("\nðŸ¢ Testing DatabaseManager (Legacy)...")
-    try:
-        from streamlit_extension.utils.database import DatabaseManager
-        dbm = DatabaseManager()
-        print("âœ… DatabaseManager created successfully")
-        
-        # Test basic functionality
-        epics = dbm.get_epics()
-        print(f"âœ… DatabaseManager.get_epics() returned {len(epics)} items")
-        return dbm
-    except Exception as e:
-        print(f"âŒ DatabaseManager failed: {e}")
-        import traceback
-        traceback.print_exc()
-        return None
-
-def test_service_container_creation(dbm):
-    """Test Service Container initialization"""
-    print("\nðŸ—ï¸ Testing Service Container Creation...")
-    try:
-        from streamlit_extension.services.service_container import ServiceContainer
-        
-        # Test with legacy API
-        container = ServiceContainer(db_manager=dbm, use_modular_api=False)
-        container.initialize(lazy_loading=True)
-        print("âœ… ServiceContainer created with legacy API")
-        
-        status = container.get_service_status()
-        print(f"ðŸ“Š Container status: {status}")
-        
-        return container
-    except Exception as e:
-        print(f"âŒ ServiceContainer creation failed: {e}")
-        import traceback
-        traceback.print_exc()
-        return None
-
-def test_service_container_modular():
-    """Test Service Container with modular API"""
-    print("\nðŸ—ï¸ Testing Service Container (Modular API)...")
-    try:
-        from streamlit_extension.services.service_container import ServiceContainer
-        
-        # Test with modular API  
-        container = ServiceContainer(use_modular_api=True)
-        container.initialize(lazy_loading=True)
-        print("âœ… ServiceContainer created with modular API")
-        
-        status = container.get_service_status()
-        print(f"ðŸ“Š Container status: {status}")
-        
-        return container
-    except Exception as e:
-        print(f"âŒ ServiceContainer (modular) creation failed: {e}")
-        import traceback
-        traceback.print_exc()
-        return None
-
-def test_service_instantiation(container):
-    """Test actual service instantiation"""
-    print("\nâš™ï¸ Testing Service Instantiation...")
-    
-    services = ["client", "project", "epic", "task", "analytics", "timer"]
-    for service_name in services:
-        try:
-            print(f"  ðŸ“¦ Testing {service_name} service...")
-            if service_name == "client":
-                service = container.get_client_service()
-            elif service_name == "project":
-                service = container.get_project_service()
-            elif service_name == "epic":
-                service = container.get_epic_service()
-            elif service_name == "task":
-                service = container.get_task_service()
-            elif service_name == "analytics":
-                service = container.get_analytics_service()
-            elif service_name == "timer":
-                service = container.get_timer_service()
-            
-            print(f"    âœ… {service_name} service created: {type(service).__name__}")
-        except Exception as e:
-            print(f"    âŒ {service_name} service failed: {e}")
-            import traceback
-            traceback.print_exc()
-    
-    # Check final status
-    final_status = container.get_service_status()
-    print(f"\nðŸ“Š Final container status: {final_status}")
-
-def test_app_setup():
-    """Test the app_setup module functionality"""
-    print("\nðŸš€ Testing App Setup Module...")
-    try:
-        from streamlit_extension.utils.app_setup import (
-            get_database_manager, get_app_service_container, 
-            check_services_health
-        )
-        
-        print("  ðŸ“¦ Testing get_database_manager...")
-        dbm = get_database_manager()
-        print(f"    {'âœ…' if dbm else 'âŒ'} DatabaseManager: {dbm}")
-        
-        print("  ðŸ“¦ Testing get_app_service_container...")  
-        container = get_app_service_container()
-        print(f"    {'âœ…' if container else 'âŒ'} ServiceContainer: {container}")
-        
-        if container:
-            status = container.get_service_status()
-            print(f"    ðŸ“Š Container status: {status}")
-        
-        print("  ðŸ“¦ Testing check_services_health...")
-        health = check_services_health()
-        print(f"    ðŸ“Š Health report: {health}")
-        
-    except Exception as e:
-        print(f"âŒ App setup failed: {e}")
-        import traceback
-        traceback.print_exc()
-
-def main():
-    print("ðŸ” DEBUG SESSION - Service Container Investigation")
-    print("=" * 60)
-    
-    # Test 1: Database connectivity
-    db_ok = test_database_connection()
-    
-    # Test 2: DatabaseManager (legacy)
-    dbm = test_database_manager() if db_ok else None
-    
-    # Test 3: Service Container with legacy API
-    container_legacy = test_service_container_creation(dbm) if dbm else None
-    
-    # Test 4: Service Container with modular API
-    container_modular = test_service_container_modular() if db_ok else None
-    
-    # Test 5: Service instantiation
-    if container_legacy:
-        print("\nðŸ§ª Testing with LEGACY API container:")
-        test_service_instantiation(container_legacy)
-        
-    if container_modular:
-        print("\nðŸ§ª Testing with MODULAR API container:")
-        test_service_instantiation(container_modular)
-    
-    # Test 6: App setup module
-    test_app_setup()
-    
-    print("\n" + "=" * 60)
-    print("ðŸŽ¯ DEBUG SESSION COMPLETE")
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/migration/migrations/005_create_additional_indexes.sql b/migration/migrations/005_create_additional_indexes.sql
deleted file mode 100644
index 40d5ef62c140f68245a08997083f90e57472df0b..0000000000000000000000000000000000000000
--- a/migration/migrations/005_create_additional_indexes.sql
+++ /dev/null
@@ -1,19 +0,0 @@
--- Migration: Create performance indexes
--- Date: 2025-01-15
--- Description: Add indexes for heavy queries identified in report.md
-
--- Client search optimization
-CREATE INDEX idx_clients_name_search ON framework_clients(name);
-CREATE INDEX idx_clients_status_active ON framework_clients(status) WHERE status = 'active';
-
--- Project filtering optimization
-CREATE INDEX idx_projects_client_status ON framework_projects(client_id, status);
-CREATE INDEX idx_projects_created_date ON framework_projects(created_at);
-
--- Epic progress calculation optimization
-CREATE INDEX idx_epics_project_status ON framework_epics(project_id, status);
-CREATE INDEX idx_epics_progress_calc ON framework_epics(project_id, status, points_earned);
-
--- Task query optimization
-CREATE INDEX idx_tasks_epic_status ON framework_tasks(epic_id, status);
-CREATE INDEX idx_tasks_tdd_phase ON framework_tasks(tdd_phase, status);
diff --git a/migration/migrations/005_rollback.sql b/migration/migrations/005_rollback.sql
index 20b2a8dadf92447d5f91f02e993e84e9c11f2bf9..474893bde2c616a597fd37c554075d7b9a7403b4 100644
--- a/migration/migrations/005_rollback.sql
+++ b/migration/migrations/005_rollback.sql
@@ -1,17 +1,16 @@
 -- Rollback 005: Remove additional indexes
 -- Date: 2025-08-18
 
 -- Drop framework_tasks indexes
 DROP INDEX IF EXISTS idx_framework_tasks_epic_id;
 DROP INDEX IF EXISTS idx_framework_tasks_status;
 DROP INDEX IF EXISTS idx_framework_tasks_tdd_phase;
 
 -- Drop framework_epics indexes
 DROP INDEX IF EXISTS idx_framework_epics_status;
-DROP INDEX IF EXISTS idx_framework_epics_client_id;
 DROP INDEX IF EXISTS idx_framework_epics_project_id;
 
 -- Drop work_sessions indexes
 DROP INDEX IF EXISTS idx_work_sessions_user_id;
 DROP INDEX IF EXISTS idx_work_sessions_task_id;
 DROP INDEX IF EXISTS idx_work_sessions_start_time;
\ No newline at end of file
diff --git a/migration/migrations/006_data_cleanup.sql b/migration/migrations/006_data_cleanup.sql
deleted file mode 100644
index 00281c2f1e8f887e24ddac154e7ef4fff8ed39ad..0000000000000000000000000000000000000000
--- a/migration/migrations/006_data_cleanup.sql
+++ /dev/null
@@ -1,17 +0,0 @@
--- Migration: Data cleanup and normalization
--- Date: 2025-01-15
--- Description: Clean up data inconsistencies
-
--- Normalize status values
-UPDATE framework_clients SET status = 'active' WHERE status IN ('Active', 'ACTIVE');
-UPDATE framework_projects SET status = 'active' WHERE status IN ('Active', 'ACTIVE');
-UPDATE framework_epics SET status = 'active' WHERE status IN ('Active', 'ACTIVE');
-
--- Clean up null/empty values
-UPDATE framework_epics SET icon = 'ðŸ“‹' WHERE icon IS NULL OR icon = '';
-UPDATE framework_tasks SET estimate_minutes = 60 WHERE estimate_minutes IS NULL OR estimate_minutes = 0;
-
--- Ensure referential integrity
-DELETE FROM framework_projects WHERE client_id NOT IN (SELECT id FROM framework_clients);
-DELETE FROM framework_epics WHERE project_id NOT IN (SELECT id FROM framework_projects);
-DELETE FROM framework_tasks WHERE epic_id NOT IN (SELECT id FROM framework_epics);
diff --git a/security_patches_database.py b/security_patches_database.py
index 8adaec2932f625571ce273868529ad886c9d0302..2a81257adce37497ca47435d503df7ed55b152ac 100644
--- a/security_patches_database.py
+++ b/security_patches_database.py
@@ -35,51 +35,51 @@ def create_security_patches():
     # Security patches to apply
     patches = [
         # Patch 1: Add security validation helper
         {
             'search': 'from typing import Dict, List, Optional, Any, Union, Tuple',
             'replace': '''from typing import Dict, List, Optional, Any, Union, Tuple
 
 # SECURITY: SQL injection prevention helpers
 def _validate_table_name(table_name: str) -> str:
     """Validate table name against whitelist to prevent SQL injection."""
     allowed_tables = {
         'framework_clients', 'framework_projects', 'framework_epics', 'framework_tasks',
         'work_sessions', 'achievement_types', 'user_achievements', 'user_streaks',
         'github_sync_log', 'system_settings'
     }
     if table_name not in allowed_tables:
         raise ValueError(f"SECURITY: Invalid table name: {table_name}")
     return table_name
 
 def _validate_column_name(column_name: str, table_name: str) -> str:
     """Validate column name against whitelist to prevent SQL injection."""
     # Common allowed columns across tables
     allowed_columns = {
         'id', 'name', 'email', 'company', 'status', 'created_at', 'updated_at',
         'title', 'description', 'priority', 'estimate_minutes', 'actual_minutes',
-        'client_id', 'project_id', 'epic_id', 'task_id', 'user_id',
+        'project_id', 'epic_id', 'task_id', 'user_id',
         'start_date', 'end_date', 'budget', 'progress', 'points'
     }
     if column_name not in allowed_columns:
         raise ValueError(f"SECURITY: Invalid column name: {column_name} for table {table_name}")
     return column_name''',
             'description': 'Add security validation helpers'
         },
         
         # Patch 2: Fix order clause vulnerability (line ~211)
         {
             'search': 'order_clause = f"ORDER BY {sort_column} {sort_order}" if sort_column else ""',
             'replace': '''# SECURITY FIX: Validate column and order before building ORDER BY clause
                 if sort_column:
                     validated_column = _validate_column_name(sort_column, table_name)
                     validated_order = 'ASC' if sort_order.upper() == 'ASC' else 'DESC'
                     order_clause = f"ORDER BY {validated_column} {validated_order}"
                 else:
                     order_clause = ""''',
             'description': 'Fix ORDER BY SQL injection vulnerability'
         },
         
         # Patch 3: Fix table name validation in count query (line ~214)
         {
             'search': 'count_query = text(f"SELECT COUNT(*) FROM {table_name}") if not where_clause else text(f"SELECT COUNT(*) FROM {table_name} {where_clause}")',
             'replace': '''# SECURITY FIX: Validate table name before query construction
diff --git a/streamlit_extension.database/queries.py b/streamlit_extension.database/queries.py
deleted file mode 100644
index 99bd420fd0d3275a069d693f262e6ffab1cd2587..0000000000000000000000000000000000000000
--- a/streamlit_extension.database/queries.py
+++ /dev/null
@@ -1,704 +0,0 @@
-# Auto-gerado por tools/refactor_split_db.py â€” NÃƒO EDITAR Ã€ MÃƒO
-from __future__ import annotations
-
-# TODO: Consider extracting this block into a separate method
-# TODO: Consider extracting this block into a separate method
-def get_epics(manager, page: int=1, page_size: int=50, status_filter: Optional[Union[EpicStatus, str]]=None, project_id: Optional[int]=None) -> Dict[str, Any]:
-    """Get epics with intelligent caching and pagination.
-        
-        Args:
-            page: Page number (1-based)
-            page_size: Number of items per page
-            status_filter: Filter by specific status
-            project_id: Filter by specific project ID
-            
-        Returns:
-            Dictionary with 'data' (list of epics), 'total', 'page', 'total_pages'
-        """
-    try:
-        with manager.get_connection('framework') as conn:
-            where_conditions = ['deleted_at IS NULL']
-            params: Dict[str, Any] = {}
-            if status_filter:
-                where_conditions.append(f'{FieldNames.STATUS} = :status_filter')
-                params['status_filter'] = status_filter.value if isinstance(status_filter, EpicStatus) else status_filter
-            if project_id:
-                where_conditions.append('project_id = :project_id')
-                params['project_id'] = project_id
-            where_clause = ' AND '.join(where_conditions)
-            count_query = f'SELECT COUNT(*) FROM {TableNames.EPICS} WHERE {where_clause}'
-            if SQLALCHEMY_AVAILABLE:
-                count_result = conn.execute(text(count_query), params)
-                total = count_result.scalar()
-            else:
-                cursor = conn.cursor()
-                cursor.execute(count_query, params)
-                total = cursor.fetchone()[0]
-            total_pages = (total + page_size - 1) // page_size
-            offset = (page - 1) * page_size
-            data_query = f'\n                    SELECT id, epic_key, name, description, status, \n                           created_at, updated_at, completed_at,\n                           points_earned, difficulty_level, project_id\n                    FROM {TableNames.EPICS}\n                    WHERE {where_clause}\n                    ORDER BY created_at DESC\n                    LIMIT :limit OFFSET :offset\n                '
-            params['limit'] = page_size
-            params['offset'] = offset
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text(data_query), params)
-                data = [dict(row._mapping) for row in result]
-            else:
-                cursor = conn.cursor()
-                cursor.execute(data_query, params)
-                data = [dict(row) for row in cursor.fetchall()]
-            return {'data': data, 'total': total, 'page': page, 'page_size': page_size, 'total_pages': total_pages}
-    except Exception as e:
-        logger.error(f'Error loading epics: {e}', exc_info=True)
-        return {'data': [], 'total': 0, 'page': page, 'page_size': page_size, 'total_pages': 0}
-
-def get_all_epics(manager) -> List[Dict[str, Any]]:
-    """Backward compatibility method - get all epics without pagination."""
-    result = manager.get_epics(page=1, page_size=1000)
-    return result['data'] if isinstance(result, dict) else result
-
-# TODO: Consider extracting this block into a separate method
-
-# TODO: Consider extracting this block into a separate method
-
-def get_tasks(manager, epic_id: Optional[int]=None, page: int=1, page_size: int=100, status_filter: Optional[Union[TaskStatus, str]]=None, tdd_phase_filter: Optional[Union[TDDPhase, str]]=None) -> Dict[str, Any]:
-    """Get tasks with intelligent caching, pagination, and filtering.
-        
-        Args:
-            epic_id: Filter by specific epic ID
-            page: Page number (1-based)
-            page_size: Number of items per page
-            status_filter: Filter by specific status
-            tdd_phase_filter: Filter by TDD phase
-            
-        Returns:
-            Dictionary with 'data' (list of tasks), 'total', 'page', 'total_pages'
-        """
-    try:
-        with manager.get_connection('framework') as conn:
-            where_conditions = ['1=1']
-            params: Dict[str, Any] = {}
-            if epic_id:
-                where_conditions.append('t.epic_id = :epic_id')
-                params['epic_id'] = epic_id
-            if status_filter:
-                where_conditions.append('t.status = :status_filter')
-                params['status_filter'] = status_filter.value if isinstance(status_filter, TaskStatus) else status_filter
-            if tdd_phase_filter:
-                where_conditions.append('t.tdd_phase = :tdd_phase_filter')
-                params['tdd_phase_filter'] = tdd_phase_filter.value if isinstance(tdd_phase_filter, TDDPhase) else tdd_phase_filter
-            where_clause = ' AND '.join(where_conditions)
-            count_query = f'\n                    SELECT COUNT(*)\n                    FROM {TableNames.TASKS} t\n                    JOIN {TableNames.EPICS} e ON t.epic_id = e.id\n                    WHERE {where_clause}\n                '
-            if SQLALCHEMY_AVAILABLE:
-                count_result = conn.execute(text(count_query), params)
-                total = count_result.scalar()
-            else:
-                cursor = conn.cursor()
-                cursor.execute(count_query, list(params.values()))
-                total = cursor.fetchone()[0]
-            total_pages = (total + page_size - 1) // page_size
-            offset = (page - 1) * page_size
-            data_query = f'\n                    SELECT t.id, t.epic_id, t.title, t.description, t.status,\n                           t.estimate_minutes, t.tdd_phase, t.position,\n                           t.created_at, t.updated_at, t.completed_at,\n                           e.name as epic_name, e.epic_key, t.task_key\n                    FROM {TableNames.TASKS} t\n                    JOIN {TableNames.EPICS} e ON t.epic_id = e.id\n                    WHERE {where_clause}\n                    ORDER BY t.position ASC, t.created_at DESC\n                    LIMIT :limit OFFSET :offset\n                '
-            params['limit'] = page_size
-            params['offset'] = offset
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text(data_query), params)
-                data = [dict(row._mapping) for row in result]
-            else:
-                cursor = conn.cursor()
-                cursor.execute(data_query, list(params.values()))
-                data = [dict(row) for row in cursor.fetchall()]
-            return {'data': data, 'total': total, 'page': page, 'page_size': page_size, 'total_pages': total_pages}
-    except Exception as e:
-        logger.error(f'Error loading tasks: {e}')
-        if STREAMLIT_AVAILABLE and st:
-            st.error(f'âŒ Error loading tasks: {e}')
-        logger.error(f'Error loading tasks: {e}', exc_info=True)
-        return {'data': [], 'total': 0, 'page': page, 'page_size': page_size, 'total_pages': 0}
-
-def get_all_tasks(manager, epic_id: Optional[int]=None) -> List[Dict[str, Any]]:
-    """Backward compatibility method - get all tasks without pagination."""
-    # TODO: Consider extracting this block into a separate method
-    # TODO: Consider extracting this block into a separate method
-    result = manager.get_tasks(epic_id=epic_id, page=1, page_size=1000)
-    return result['data'] if isinstance(result, dict) else result
-
-def get_timer_sessions(manager, days: int=30) -> List[Dict[str, Any]]:
-    """Get recent timer sessions with short-term caching."""
-    if not manager.timer_db_path.exists():
-        return []
-    try:
-        with manager.get_connection('timer') as conn:
-            query = "\n                    SELECT task_reference, user_identifier, started_at, ended_at,\n                           planned_duration_minutes, actual_duration_minutes,\n                           focus_rating, energy_level, mood_rating,\n                           interruptions_count,\n                           created_at\n                    FROM timer_sessions\n                    WHERE created_at >= DATE('now', ? || ' days')\n                    ORDER BY created_at DESC\n                    LIMIT 1000\n                "
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text(query), [f'-{days}'])
-                return [dict(row._mapping) for row in result]
-            else:
-                cursor = conn.cursor()
-                cursor.execute(query, [f'-{days}'])
-                return [dict(row) for row in cursor.fetchall()]
-    # TODO: Consider extracting this block into a separate method
-    # TODO: Consider extracting this block into a separate method
-    except Exception as e:
-        logger.error(f'Error loading timer sessions: {e}', exc_info=True)
-        return []
-
-def get_user_stats(manager, user_id: int=1) -> Dict[str, Any]:
-    """Get user statistics and gamification data."""
-    try:
-        with manager.get_connection('framework') as conn:
-            stats = {}
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text("\n                        SELECT COUNT(*) as completed_tasks\n                        FROM framework_tasks\n                        WHERE status = 'completed' AND deleted_at IS NULL\n                    "))
-                stats['completed_tasks'] = result.scalar() or 0
-                result = conn.execute(text('\n                        SELECT COALESCE(SUM(points_earned), 0) as total_points\n                        FROM framework_epics WHERE deleted_at IS NULL\n                    '))
-                stats['total_points'] = result.scalar() or 0
-                result = conn.execute(text('\n                        SELECT COUNT(*) as active_streaks\n                        FROM user_streaks\n                        WHERE user_id = ? AND current_count > 0\n                    '), [user_id])
-                stats['active_streaks'] = result.scalar() or 0
-            else:
-                cursor = conn.cursor()
-                cursor.execute("\n                        SELECT COUNT(*) FROM framework_tasks\n                        WHERE status = 'completed' AND deleted_at IS NULL\n                    ")
-                row = cursor.fetchone()
-                stats['completed_tasks'] = row[0] if row and row[0] is not None else 0
-                cursor.execute('\n                        SELECT COALESCE(SUM(points_earned), 0)\n                        FROM framework_epics WHERE deleted_at IS NULL\n                    ')
-                row = cursor.fetchone()
-                stats['total_points'] = row[0] if row and row[0] is not None else 0
-                cursor.execute('\n                        SELECT COUNT(*) FROM user_streaks\n                        WHERE user_id = ? AND current_count > 0\n                    ', [user_id])
-                row = cursor.fetchone()
-                stats['active_streaks'] = row[0] if row and row[0] is not None else 0
-            # TODO: Consider extracting this block into a separate method
-            # TODO: Consider extracting this block into a separate method
-            return stats
-    except Exception as e:
-        logger.error(f'Error loading user stats: {e}', exc_info=True)
-        return {'completed_tasks': 0, 'total_points': 0, 'active_streaks': 0}
-
-def get_achievements(manager, user_id: int=1) -> List[Dict[str, Any]]:
-    """Get user achievements."""
-    try:
-        with manager.get_connection('framework') as conn:
-            query = '\n                    SELECT at.code, at.name, at.description, at.category,\n                           at.points_reward, at.rarity, ua.unlocked_at\n                    FROM user_achievements ua\n                    JOIN achievement_types at ON ua.achievement_code = at.code\n                    WHERE ua.user_id = ?\n                    ORDER BY ua.unlocked_at DESC\n                '
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text(query), [user_id])
-                return [dict(row._mapping) for row in result]
-            else:
-                cursor = conn.cursor()
-                # TODO: Consider extracting this block into a separate method
-                # TODO: Consider extracting this block into a separate method
-                cursor.execute(query, [user_id])
-                return [dict(row) for row in cursor.fetchall()]
-    except Exception as e:
-        logger.error(f'Error loading achievements: {e}', exc_info=True)
-        return []
-
-def get_epics_with_hierarchy(manager, project_id: Optional[int]=None, page: int=1, page_size: int=25, status_filter: str='') -> Dict[str, Any]:
-    """Get epics with complete hierarchy information (project â†’ epic) with pagination.
-        
-        Args:
-            project_id: Filter by specific project ID (optional)
-            page: Page number (1-based)
-            page_size: Number of items per page
-            status_filter: Filter by epic status
-            
-        Returns:
-            Dictionary with 'data' (list of epics), 'total', 'page', 'total_pages'
-        """
-    try:
-        with manager.get_connection('framework') as conn:
-            where_conditions = ['e.deleted_at IS NULL']
-            params: Dict[str, Any] = {}
-            if project_id:
-                where_conditions.append('e.project_id = :project_id')
-                params['project_id'] = project_id
-            if status_filter:
-                where_conditions.append('e.status = :status_filter')
-                params['status_filter'] = status_filter
-            where_clause = ' AND '.join(where_conditions)
-            count_query = f'\n                    SELECT COUNT(*) \n                    FROM framework_epics e\n                    LEFT JOIN framework_projects p ON e.project_id = p.id AND p.deleted_at IS NULL\n                    WHERE {where_clause}\n                '
-            if SQLALCHEMY_AVAILABLE:
-                count_result = conn.execute(text(count_query), params)
-                total = count_result.scalar()
-            else:
-                cursor = conn.cursor()
-                cursor.execute(count_query, list(params.values()))
-                total = cursor.fetchone()[0]
-            total_pages = (total + page_size - 1) // page_size
-            offset = (page - 1) * page_size
-            data_query = f'\n                    SELECT e.id, e.epic_key, e.name, e.description, e.summary,\n                           e.status, e.priority, e.duration_days,\n                           e.points_earned, e.difficulty_level,\n                           e.planned_start_date, e.planned_end_date,\n                           e.actual_start_date, e.actual_end_date,\n                           e.calculated_duration_days, e.duration_description,\n                           e.created_at, e.updated_at, e.completed_at,\n                           e.project_id,\n                           p.name as project_name, p.project_key, p.status as project_status,\n                           p.health_status as project_health\n                    FROM framework_epics e\n                    LEFT JOIN framework_projects p ON e.project_id = p.id AND p.deleted_at IS NULL\n                    WHERE {where_clause}\n                    ORDER BY p.priority DESC, e.created_at DESC\n                    LIMIT :limit OFFSET :offset\n                '
-            params['limit'] = page_size
-            params['offset'] = offset
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text(data_query), params)
-                data = [dict(row._mapping) for row in result]
-            else:
-                cursor = conn.cursor()
-                cursor.execute(data_query, list(params.values()))
-                data = [dict(row) for row in cursor.fetchall()]
-            return {'data': data, 'total': total, 'page': page, 'page_size': page_size, 'total_pages': total_pages}
-    except Exception as e:
-        logger.error(f'Error loading epics with hierarchy: {e}')
-        if STREAMLIT_AVAILABLE and st:
-            st.error(f'âŒ Error loading epics with hierarchy: {e}')
-        # TODO: Consider extracting this block into a separate method
-        # TODO: Consider extracting this block into a separate method
-        return {'data': [], 'total': 0, 'page': page, 'page_size': page_size, 'total_pages': 0}
-
-def get_all_epics_with_hierarchy(manager, project_id: Optional[int]=None, client_id: Optional[int]=None) -> List[Dict[str, Any]]:
-    """Backward compatibility method - get all epics with hierarchy without pagination."""
-    result = manager.get_epics_with_hierarchy(project_id=project_id, client_id=client_id, page=1, page_size=1000)
-    return result['data'] if isinstance(result, dict) else result
-
-def get_hierarchy_overview(manager, client_id: Optional[int]=None) -> List[Dict[str, Any]]:
-    """Get complete hierarchy overview using the database view.
-        
-        Args:
-            client_id: Filter by specific client ID (optional)
-            
-        Returns:
-            List of hierarchy records with aggregated task counts
-        """
-    try:
-        with manager.get_connection('framework') as conn:
-            query = '\n                    SELECT client_id, client_key, client_name, client_status, client_tier,\n                           project_id, project_key, project_name, project_status, project_health,\n                           project_completion, epic_id, epic_key, epic_name, epic_status,\n                           calculated_duration_days, total_tasks, completed_tasks,\n                           epic_completion_percentage, planned_start_date, planned_end_date,\n                           epic_planned_start, epic_planned_end\n                    FROM hierarchy_overview\n                    WHERE 1=1\n                '
-            params = []
-            if client_id:
-                query += ' AND client_id = ?'
-                params.append(client_id)
-            query += ' ORDER BY client_name, project_name, epic_key'
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text(query), params)
-                return [dict(row._mapping) for row in result]
-            # TODO: Consider extracting this block into a separate method
-            # TODO: Consider extracting this block into a separate method
-            else:
-                cursor = conn.cursor()
-                cursor.execute(query, params)
-                return [dict(row) for row in cursor.fetchall()]
-    except Exception as e:
-        logger.error(f'Error loading hierarchy overview: {e}')
-        return []
-
-def get_client_dashboard(manager, client_id: Optional[int]=None) -> List[Dict[str, Any]]:
-    """Get client dashboard data using the database view.
-        
-        Args:
-            client_id: Get data for specific client (optional)
-            
-        Returns:
-            List of client dashboard records with aggregated metrics
-        """
-    try:
-        with manager.get_connection('framework') as conn:
-            query = '\n                    SELECT client_id, client_key, client_name, client_status, client_tier,\n                           hourly_rate, total_projects, active_projects, completed_projects,\n                           total_epics, active_epics, completed_epics,\n                           total_tasks, completed_tasks, in_progress_tasks,\n                           total_hours_logged, total_budget, total_points_earned,\n                           earliest_project_start, latest_project_end,\n                           projects_at_risk, avg_project_completion\n                    FROM client_dashboard\n                    WHERE 1=1\n                '
-            params = []
-            if client_id:
-                query += ' AND client_id = ?'
-                params.append(client_id)
-            query += ' ORDER BY client_name'
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text(query), params)
-                # TODO: Consider extracting this block into a separate method
-                # TODO: Consider extracting this block into a separate method
-                return [dict(row._mapping) for row in result]
-            else:
-                cursor = conn.cursor()
-                cursor.execute(query, params)
-                return [dict(row) for row in cursor.fetchall()]
-    except Exception as e:
-        logger.error(f'Error loading client dashboard: {e}')
-        return []
-
-def get_project_dashboard(manager, project_id: Optional[int]=None, client_id: Optional[int]=None) -> List[Dict[str, Any]]:
-    """Get project dashboard data using the database view.
-        
-        Args:
-            project_id: Get data for specific project (optional)
-            client_id: Get data for projects of specific client (optional)
-            
-        Returns:
-            List of project dashboard records with aggregated metrics
-        """
-    try:
-        with manager.get_connection('framework') as conn:
-            query = '\n                    SELECT project_id, project_key, project_name, project_status, health_status,\n                           completion_percentage, client_id, client_name, client_tier,\n                           total_epics, completed_epics, active_epics,\n                           total_tasks, completed_tasks, in_progress_tasks,\n                           estimated_hours, actual_hours, estimated_task_hours, actual_task_hours,\n                           budget_amount, hourly_rate, planned_start_date, planned_end_date,\n                           actual_start_date, actual_end_date, calculated_completion_percentage,\n                           total_points_earned, complexity_score, quality_score\n                    FROM project_dashboard\n                    WHERE 1=1\n                '
-            params = []
-            if project_id:
-                query += ' AND project_id = ?'
-                params.append(project_id)
-            elif client_id:
-                query += ' AND client_id = ?'
-                params.append(client_id)
-            query += ' ORDER BY client_name, project_name'
-            if SQLALCHEMY_AVAILABLE:
-                # TODO: Consider extracting this block into a separate method
-                # TODO: Consider extracting this block into a separate method
-                result = conn.execute(text(query), params)
-                return [dict(row._mapping) for row in result]
-            else:
-                cursor = conn.cursor()
-                cursor.execute(query, params)
-                return [dict(row) for row in cursor.fetchall()]
-    except Exception as e:
-        logger.error(f'Error loading project dashboard: {e}')
-        return []
-
-def create_client(manager, client_key: str, name: str, description: str='', industry: str='', company_size: str='startup', primary_contact_name: str='', primary_contact_email: str='', hourly_rate: float=0.0, **kwargs: Any) -> Optional[int]:
-    """Create new client record.
-
-        Creates client with full validation and automatic timestamp assignment.
-        Invalidates related caches and triggers audit logging.
-
-        Args:
-            client_key: Unique client identifier string (3-20 chars).
-            name: Client display name (1-100 characters).
-            description: Client description (max 500 chars).
-            industry: Industry classification.
-            company_size: Company size category.
-            primary_contact_name: Primary contact name.
-            primary_contact_email: Primary contact email.
-            hourly_rate: Billing rate per hour.
-            **kwargs: Additional optional fields like ``status`` or
-                ``client_tier``.
-
-        Returns:
-            Optional[int]: New client ID if successful, ``None`` if failed.
-
-        Raises:
-            ValueError: If required fields are missing or invalid.
-            IntegrityError: If ``client_key`` already exists.
-            DatabaseError: If insert operation fails.
-
-        Side Effects:
-            - Invalidates client list caches.
-            - Creates audit log entry.
-
-        Performance:
-            - Insert operation: ~5ms.
-
-        Example:
-            >>> client_id = db_manager.create_client(
-            ...     client_key="acme_corp", name="ACME Corporation"
-            ... )
-        """
-    try:
-        client_data = {'client_key': client_key, 'name': name, 'description': description, 'industry': industry, 'company_size': company_size, 'primary_contact_name': primary_contact_name, 'primary_contact_email': primary_contact_email, 'hourly_rate': hourly_rate, 'status': kwargs.get('status', ClientStatus.ACTIVE.value), 'client_tier': kwargs.get('client_tier', 'standard'), 'priority_level': kwargs.get('priority_level', 5), 'timezone': kwargs.get('timezone', 'America/Sao_Paulo'), 'currency': kwargs.get('currency', 'BRL'), 'preferred_language': kwargs.get('preferred_language', 'pt-BR'), 'contract_type': kwargs.get('contract_type', 'time_and_materials'), 'created_by': kwargs.get('created_by', 1)}
-        with manager.get_connection('framework') as conn:
-            placeholders = ', '.join(['?' for _ in client_data])
-            columns = ', '.join(client_data.keys())
-            if SQLALCHEMY_AVAILABLE:
-                named_placeholders = ', '.join([f':{key}' for key in client_data.keys()])
-                result = conn.execute(text(f'INSERT INTO {TableNames.CLIENTS} ({columns}) VALUES ({named_placeholders})'), client_data)
-                conn.commit()
-                return result.lastrowid
-            # TODO: Consider extracting this block into a separate method
-            # TODO: Consider extracting this block into a separate method
-            else:
-                cursor = conn.cursor()
-                cursor.execute(f'INSERT INTO {TableNames.CLIENTS} ({columns}) VALUES ({placeholders})', list(client_data.values()))
-                conn.commit()
-                return cursor.lastrowid
-    except Exception as e:
-        logger.error(f'Error creating client: {e}')
-        if STREAMLIT_AVAILABLE and st:
-            st.error(f'âŒ Error creating client: {e}')
-        return None
-
-def create_project(manager, client_id: int, project_key: str, name: str, description: str='', project_type: str='development', methodology: str='agile', **kwargs: Any) -> Optional[int]:
-    """Create a new project.
-        
-        Args:
-            client_id: ID of the client who owns this project
-            project_key: Unique project identifier within client
-            name: Project name
-            description: Project description
-            project_type: Type of project (development, maintenance, etc.)
-            methodology: Development methodology (agile, waterfall, etc.)
-            **kwargs: Additional project fields
-            
-        Returns:
-            Project ID if successful, None otherwise
-        """
-    try:
-        project_data = {'client_id': client_id, 'project_key': project_key, 'name': name, 'description': description, 'project_type': project_type, 'methodology': methodology, 'status': kwargs.get('status', ProjectStatus.PLANNING.value), 'priority': kwargs.get('priority', 5), 'health_status': kwargs.get('health_status', 'green'), 'completion_percentage': kwargs.get('completion_percentage', 0), 'planned_start_date': kwargs.get('planned_start_date'), 'planned_end_date': kwargs.get('planned_end_date'), 'estimated_hours': kwargs.get('estimated_hours', 0), 'budget_amount': kwargs.get('budget_amount', 0), 'budget_currency': kwargs.get('budget_currency', 'BRL'), 'hourly_rate': kwargs.get('hourly_rate'), 'project_manager_id': kwargs.get('project_manager_id', 1), 'technical_lead_id': kwargs.get('technical_lead_id', 1), 'repository_url': kwargs.get('repository_url', ''), 'visibility': kwargs.get('visibility', 'client'), 'access_level': kwargs.get('access_level', 'standard'), 'complexity_score': kwargs.get('complexity_score', 5.0), 'quality_score': kwargs.get('quality_score', 8.0), 'created_by': kwargs.get('created_by', 1)}
-        with manager.get_connection('framework') as conn:
-            project_data = {k: v for k, v in project_data.items() if v is not None}
-            placeholders = ', '.join(['?' for _ in project_data])
-            columns = ', '.join(project_data.keys())
-            if SQLALCHEMY_AVAILABLE:
-                named_placeholders = ', '.join([f':{key}' for key in project_data.keys()])
-                result = conn.execute(text(f'INSERT INTO {TableNames.PROJECTS} ({columns}) VALUES ({named_placeholders})'), project_data)
-                conn.commit()
-                # TODO: Consider extracting this block into a separate method
-                # TODO: Consider extracting this block into a separate method
-                return result.lastrowid
-            else:
-                cursor = conn.cursor()
-                cursor.execute(f'INSERT INTO {TableNames.PROJECTS} ({columns}) VALUES ({placeholders})', list(project_data.values()))
-                conn.commit()
-                return cursor.lastrowid
-    except Exception as e:
-        logger.error(f'Error creating project: {e}')
-        if STREAMLIT_AVAILABLE and st:
-            st.error(f'âŒ Error creating project: {e}')
-        return None
-
-def update_epic_project(manager, epic_id: int, project_id: int) -> bool:
-    """Update the project assignment for an epic.
-        
-        Args:
-            epic_id: ID of the epic to update
-            project_id: ID of the new project
-            
-        Returns:
-            True if successful, False otherwise
-        """
-    try:
-        # TODO: Consider extracting this block into a separate method
-        # TODO: Consider extracting this block into a separate method
-        with manager.get_connection('framework') as conn:
-            if SQLALCHEMY_AVAILABLE:
-                conn.execute(text('\n                        UPDATE framework_epics \n                        SET project_id = :project_id, updated_at = CURRENT_TIMESTAMP\n                        WHERE id = :epic_id\n                    '), {'project_id': project_id, 'epic_id': epic_id})
-                conn.commit()
-            else:
-                cursor = conn.cursor()
-                cursor.execute('\n                        UPDATE framework_epics \n                        SET project_id = ?, updated_at = CURRENT_TIMESTAMP\n                        WHERE id = ?\n                    ', (project_id, epic_id))
-                conn.commit()
-            return True
-    except Exception as e:
-        logger.error(f'Error updating epic project: {e}')
-        return False
-
-def get_client_by_key(manager, client_key: str) -> Optional[Dict[str, Any]]:
-    """Get client by client_key.
-        
-        Args:
-            client_key: Client key to search for
-            
-        Returns:
-            Client dictionary if found, None otherwise
-        """
-    try:
-        # TODO: Consider extracting this block into a separate method
-        # TODO: Consider extracting this block into a separate method
-        with manager.get_connection('framework') as conn:
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text('\n                        SELECT * FROM framework_clients \n                        WHERE client_key = :client_key AND deleted_at IS NULL\n                    '), {'client_key': client_key})
-                row = result.fetchone()
-                return dict(row._mapping) if row else None
-            else:
-                cursor = conn.cursor()
-                cursor.execute('\n                        SELECT * FROM framework_clients \n                        WHERE client_key = ? AND deleted_at IS NULL\n                    ', (client_key,))
-                row = cursor.fetchone()
-                return dict(row) if row else None
-    except Exception as e:
-        logger.error(f'Error getting client by key: {e}')
-        return None
-
-def get_project_by_key(manager, client_id: int, project_key: str) -> Optional[Dict[str, Any]]:
-    """Get project by client_id and project_key.
-        
-        Args:
-            client_id: Client ID
-            project_key: Project key to search for
-            
-        Returns:
-            Project dictionary if found, None otherwise
-        """
-    # TODO: Consider extracting this block into a separate method
-    # TODO: Consider extracting this block into a separate method
-    try:
-        with manager.get_connection('framework') as conn:
-            if SQLALCHEMY_AVAILABLE:
-                result = conn.execute(text('\n                        SELECT * FROM framework_projects \n                        WHERE client_id = :client_id AND project_key = :project_key \n                        AND deleted_at IS NULL\n                    '), {'client_id': client_id, 'project_key': project_key})
-                row = result.fetchone()
-                return dict(row._mapping) if row else None
-            else:
-                cursor = conn.cursor()
-                cursor.execute('\n                        SELECT * FROM framework_projects \n                        WHERE client_id = ? AND project_key = ? AND deleted_at IS NULL\n                    ', (client_id, project_key))
-                row = cursor.fetchone()
-                return dict(row) if row else None
-    except Exception as e:
-        logger.error(f'Error getting project by key: {e}')
-        return None
-
-def update_client(manager, client_id: int, **fields: Any) -> bool:
-    """Update existing client record.
-
-        Updates specified fields while preserving others. Validates all input
-        and maintains data integrity. Supports partial updates.
-
-        Args:
-            client_id: Client ID to update. Must exist.
-            **fields: Fields to update. Same validation as ``create_client``.
-
-        Returns:
-            bool: ``True`` if update successful, ``False`` if failed or no
-                changes.
-
-        Raises:
-            ValueError: If ``client_id`` invalid or field validation fails.
-            DatabaseError: If update operation fails.
-
-        Side Effects:
-            - Invalidates client caches for this client.
-            - Updates ``updated_at`` timestamp.
-
-        Performance:
-            - Update operation: ~3ms.
-
-        Example:
-            >>> db_manager.update_client(123, name="New Name")
-        """
-    try:
-        if not fields:
-            return True
-        fields['updated_at'] = 'CURRENT_TIMESTAMP'
-        set_clauses = []
-        values = {}
-        for key, value in fields.items():
-            if key == 'updated_at':
-                set_clauses.append(f'{key} = CURRENT_TIMESTAMP')
-            else:
-                set_clauses.append(f'{key} = :{key}')
-                values[key] = value
-        values['client_id'] = client_id
-        with manager.get_connection('framework') as conn:
-            if SQLALCHEMY_AVAILABLE:
-                conn.execute(text(f'\n                        UPDATE {TableNames.CLIENTS}\n                        SET {', '.join(set_clauses)}\n                        WHERE id = :client_id AND deleted_at IS NULL\n                    '), values)
-                # TODO: Consider extracting this block into a separate method
-                # TODO: Consider extracting this block into a separate method
-                conn.commit()
-            else:
-                cursor = conn.cursor()
-                positional_values = [values[key] for key in values.keys() if key != 'client_id']
-                positional_values.append(client_id)
-                sqlite_clauses = [clause.replace(f':{key}', '?') for clause in set_clauses if f':{key}' in clause]
-                sqlite_clauses.extend([clause for clause in set_clauses if '?' not in clause and ':' not in clause])
-                cursor.execute(f'\n                        UPDATE {TableNames.CLIENTS}\n                        SET {', '.join(sqlite_clauses)}\n                        WHERE id = ? AND deleted_at IS NULL\n                    ', positional_values)
-                conn.commit()
-            return True
-    except Exception as e:
-        logger.error(f'Error updating client: {e}')
-        if STREAMLIT_AVAILABLE and st:
-            st.error(f'âŒ Error updating client: {e}')
-        return False
-
-def delete_client(manager, client_id: int, soft_delete: bool=True) -> bool:
-    """Delete client record (soft or hard delete).
-
-        Removes client from active use. Soft delete preserves data for audit
-        purposes. Hard delete permanently removes all data.
-
-        Args:
-            client_id: Client ID to delete. Must exist.
-            soft_delete: Use soft delete. Defaults to ``True``.
-
-        Returns:
-            bool: ``True`` if deletion successful, ``False`` if failed.
-
-        Raises:
-            ValueError: If ``client_id`` invalid.
-            DatabaseError: If delete operation fails.
-
-        Side Effects:
-            - Invalidates all client-related caches.
-
-        Performance:
-            - Soft delete: ~2ms.
-            - Hard delete: ~10-100ms (depends on related data).
-
-        Example:
-            >>> db_manager.delete_client(123, soft_delete=True)
-        """
-    try:
-        with manager.get_connection('framework') as conn:
-            # TODO: Consider extracting this block into a separate method
-            # TODO: Consider extracting this block into a separate method
-            if soft_delete:
-                if SQLALCHEMY_AVAILABLE:
-                    conn.execute(text('\n                            UPDATE {TableNames.CLIENTS}\n                            SET deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP\n                            WHERE id = :client_id\n                        '), {'client_id': client_id})
-                    conn.commit()
-                else:
-                    cursor = conn.cursor()
-                    cursor.execute('\n                            UPDATE {TableNames.CLIENTS}\n                            SET deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP\n                            WHERE id = ?\n                        ', (client_id,))
-                    conn.commit()
-            else:
-                return manager.delete_cascade_safe(table_name='clients', record_id=client_id, cascade_tables=['projects', 'epics', 'tasks'])
-            return True
-    except Exception as e:
-        logger.error(f'Error deleting client: {e}')
-        if STREAMLIT_AVAILABLE and st:
-            st.error(f'âŒ Error deleting client: {e}')
-        return False
-
-def update_project(manager, project_id: int, **fields: Any) -> bool:
-    """Update an existing project.
-        
-        Args:
-            project_id: ID of the project to update
-            **fields: Fields to update
-            
-        Returns:
-            True if successful, False otherwise
-        """
-    try:
-        if not fields:
-            return True
-        fields['updated_at'] = 'CURRENT_TIMESTAMP'
-        set_clauses = []
-        values = {}
-        for key, value in fields.items():
-            if key == 'updated_at':
-                set_clauses.append(f'{key} = CURRENT_TIMESTAMP')
-            else:
-                set_clauses.append(f'{key} = :{key}')
-                values[key] = value
-        values['project_id'] = project_id
-        with manager.get_connection('framework') as conn:
-            # TODO: Consider extracting this block into a separate method
-            # TODO: Consider extracting this block into a separate method
-            if SQLALCHEMY_AVAILABLE:
-                conn.execute(text(f'\n                        UPDATE {TableNames.PROJECTS}\n                        SET {', '.join(set_clauses)}\n                        WHERE id = :project_id AND deleted_at IS NULL\n                    '), values)
-                conn.commit()
-            else:
-                cursor = conn.cursor()
-                positional_values = [values[key] for key in values.keys() if key != 'project_id']
-                positional_values.append(project_id)
-                sqlite_clauses = [clause.replace(f':{key}', '?') for clause in set_clauses if f':{key}' in clause]
-                sqlite_clauses.extend([clause for clause in set_clauses if '?' not in clause and ':' not in clause])
-                cursor.execute(f'\n                        UPDATE {TableNames.PROJECTS}\n                        SET {', '.join(sqlite_clauses)}\n                        WHERE id = ? AND deleted_at IS NULL\n                    ', positional_values)
-                conn.commit()
-            return True
-    except Exception as e:
-        logger.error(f'Error updating project: {e}')
-        if STREAMLIT_AVAILABLE and st:
-            st.error(f'âŒ Error updating project: {e}')
-        return False
-
-def delete_project(manager, project_id: int, soft_delete: bool=True) -> bool:
-    """Delete a project (soft delete by default).
-        
-        Args:
-            project_id: ID of the project to delete
-            soft_delete: If True, mark as deleted instead of removing
-            
-        Returns:
-            True if successful, False otherwise
-        """
-    try:
-        with manager.get_connection('framework') as conn:
-            if soft_delete:
-                if SQLALCHEMY_AVAILABLE:
-                    conn.execute(text('\n                            UPDATE {TableNames.PROJECTS}\n                            SET deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP\n                            WHERE id = :project_id\n                        '), {'project_id': project_id})
-                    conn.commit()
-                else:
-                    cursor = conn.cursor()
-                    cursor.execute('\n                            UPDATE {TableNames.PROJECTS}\n                            SET deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP\n                            WHERE id = ?\n                        ', (project_id,))
-                    conn.commit()
-            else:
-                return manager.delete_cascade_safe(table_name='projects', record_id=project_id, cascade_tables=['epics', 'tasks'])
-            return True
-    except Exception as e:
-        logger.error(f'Error deleting project: {e}')
-        if STREAMLIT_AVAILABLE and st:
-            st.error(f'âŒ Error deleting project: {e}')
-        return False
\ No newline at end of file
diff --git a/streamlit_extension/components/fallback_components.py b/streamlit_extension/components/fallback_components.py
index fba3ecec0536b46578945fba3c929ded83ecb0f2..c9d73732525c0d7a42b778b940f277abd0bcef61 100644
--- a/streamlit_extension/components/fallback_components.py
+++ b/streamlit_extension/components/fallback_components.py
@@ -193,61 +193,50 @@ class StandardForm:
             
         Returns:
             Form result dictionary
         """
         if not STREAMLIT_AVAILABLE:
             return {"submitted": False, "valid": False, "data": {}}
         
         try:
             st.info("ðŸ“ FormulÃ¡rio padrÃ£o indisponÃ­vel (modo fallback)")
             
             with st.form("fallback_form"):
                 st.text_input("Campo de exemplo", disabled=True)
                 submitted = st.form_submit_button("Enviar (IndisponÃ­vel)")
             
             return {
                 "submitted": False,
                 "valid": False, 
                 "data": {},
                 "errors": ["Form component not available"]
             }
         
         except Exception as e:
             logger.error(f"Error in StandardForm fallback: {e}")
             return {"submitted": False, "valid": False, "data": {}, "errors": [str(e)]}
 
-class ClientForm:
-    """Fallback implementation for client form component."""
-    
-    @staticmethod
-    def render(data: Optional[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:
-        """Render client form fallback."""
-        return StandardForm.render(fields=[
-            {"name": "name", "type": "text", "label": "Nome"},
-            {"name": "email", "type": "email", "label": "Email"}
-        ])
-
 class ProjectForm:
     """Fallback implementation for project form component."""
     
     @staticmethod
     def render(data: Optional[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:
         """Render project form fallback."""
         return StandardForm.render(fields=[
             {"name": "name", "type": "text", "label": "Nome do Projeto"},
             {"name": "description", "type": "textarea", "label": "DescriÃ§Ã£o"}
         ])
 
 # === CHART AND ANALYTICS FALLBACKS ============================================
 
 class AnalyticsChart:
     """Fallback implementation for analytics charts."""
     
     @staticmethod
     def render_bar_chart(data: Optional[Dict[str, Any]] = None, **kwargs) -> None:
         """Render bar chart fallback."""
         safe_ui(lambda: st.info("ðŸ“Š GrÃ¡fico de barras indisponÃ­vel"))
     
     @staticmethod
     def render_line_chart(data: Optional[Dict[str, Any]] = None, **kwargs) -> None:
         """Render line chart fallback."""
         safe_ui(lambda: st.info("ðŸ“ˆ GrÃ¡fico de linha indisponÃ­vel"))
@@ -284,97 +273,95 @@ class MetricCard:
 
 class NavigationMenu:
     """Fallback implementation for navigation menu."""
     
     @staticmethod
     def render(items: List[Dict[str, Any]] = None, **kwargs) -> Optional[str]:
         """
         Render navigation menu fallback.
         
         Args:
             items: Menu item definitions
             **kwargs: Additional options
             
         Returns:
             Selected menu item or None
         """
         if not STREAMLIT_AVAILABLE:
             return None
         
         try:
             st.info("ðŸ§­ Menu de navegaÃ§Ã£o indisponÃ­vel (modo fallback)")
             
             # Basic navigation fallback
             default_items = [
                 {"label": "Dashboard", "value": "dashboard"},
-                {"label": "Clients", "value": "clients"},
                 {"label": "Projects", "value": "projects"}
             ]
             
             menu_items = items or default_items
             labels = [item.get("label", "Item") for item in menu_items]
             
             if labels:
                 selected_index = st.selectbox(
                     "NavegaÃ§Ã£o",
                     range(len(labels)),
                     format_func=lambda x: labels[x]
                 )
                 
                 return menu_items[selected_index].get("value", "dashboard")
             
             return "dashboard"
         
         except Exception as e:
             logger.error(f"Error in NavigationMenu fallback: {e}")
             return "dashboard"
 
 # === FALLBACK REGISTRY ========================================================
 
 class FallbackRegistry:
     """Registry for managing fallback components."""
     
     _fallbacks: Dict[str, Any] = {
         # Dashboard widgets
         "WelcomeHeader": WelcomeHeader,
         "DailyStats": DailyStats,
         "ProductivityHeatmap": ProductivityHeatmap,
         "ProgressRing": ProgressRing,
         "SparklineChart": SparklineChart,
         "AchievementCard": AchievementCard,
         "QuickActionButton": QuickActionButton,
         
         # Notification system
         "NotificationData": NotificationData,
         "NotificationToast": NotificationToast,
         
         # Timer
         "TimerComponent": TimerComponent,
         
         # Forms
         "StandardForm": StandardForm,
-        "ClientForm": ClientForm,
         "ProjectForm": ProjectForm,
         
         # Charts and analytics
         "AnalyticsChart": AnalyticsChart,
         "MetricCard": MetricCard,
         
         # Navigation
         "NavigationMenu": NavigationMenu
     }
     
     @classmethod
     def get_fallback(cls, component_name: str) -> Optional[Any]:
         """Get fallback implementation for a component."""
         return cls._fallbacks.get(component_name)
     
     @classmethod
     def register_fallback(cls, component_name: str, fallback_class: Any) -> None:
         """Register a new fallback component."""
         cls._fallbacks[component_name] = fallback_class
     
     @classmethod
     def list_fallbacks(cls) -> List[str]:
         """List all available fallback components."""
         return list(cls._fallbacks.keys())
 
@@ -392,41 +379,40 @@ def check_fallback_components_health() -> Dict[str, Any]:
 
 # === EXPORTS ==================================================================
 
 __all__ = [
     # Safe UI helper
     "safe_ui",
     
     # Dashboard widgets
     "WelcomeHeader",
     "DailyStats",
     "ProductivityHeatmap",
     "ProgressRing",
     "SparklineChart", 
     "AchievementCard",
     "QuickActionButton",
     
     # Notification system
     "NotificationData",
     "NotificationToast",
     
     # Timer
     "TimerComponent",
     
     # Forms
     "StandardForm",
-    "ClientForm", 
     "ProjectForm",
     
     # Charts and analytics
     "AnalyticsChart",
     "MetricCard",
     
     # Navigation
     "NavigationMenu",
     
     # Registry
     "FallbackRegistry",
     
     # Health check
     "check_fallback_components_health",
 ]
\ No newline at end of file
diff --git a/streamlit_extension/components/form_components.py b/streamlit_extension/components/form_components.py
index f91a6b32d8a6fc7a8903c535becb9512b0884e28..a943fb699e1295bb6471137a3ff9ccbc0b0fda88 100644
--- a/streamlit_extension/components/form_components.py
+++ b/streamlit_extension/components/form_components.py
@@ -161,67 +161,65 @@ class StandardForm:
                 yield
 
     # ------------------------------------------------------------------
     # Data management and validation
     def get_form_data(self) -> Dict[str, Any]:
         """Get collected form data."""
         return dict(self.form_data)
 
     def validate_and_submit(self, form_data: Dict, validation_func: Callable[[Dict], list[str]]):
         """Validate form data and handle submission."""
         errors = validation_func(form_data)
         if errors:
             self.display_errors(errors)
             return False, errors
         return True, []
 
     def display_errors(self, errors: list[str]):
         """Display validation errors."""
         if not self.st:
             self.errors.extend(errors)
         else:  # pragma: no cover - simple streamlit display
             for error in errors:
                 self.st.error(error)
 
 
-# ClientForm removed - client functionality eliminated
 
 
 class ProjectForm(StandardForm):
     """Form component for creating and editing projects."""
 
     def render_project_fields(self, project_data: Optional[Dict] = None):
         """Render complete project form with all fields."""
         if not self.st:
             return True
         
         with self.st.form(self.form_id):
             self.st.markdown(f"### {self.title}")
             
             # Basic Information Section
             self.st.markdown("#### Basic Information")
-            # Client selection removed - direct project management
             project_key = self.render_text_input(
                 "Project Key*", "project_key", required=True,
                 placeholder="e.g., project_xyz"
             )
             name = self.render_text_input(
                 "Project Name*", "name", required=True,
                 placeholder="e.g., Website Redesign"
             )
             description = self.render_text_area(
                 "Description", "description",
                 placeholder="Project details and objectives..."
             )
             
             # Project Settings Section
             self.st.markdown("#### Project Settings")
             status = self.render_select_box(
                 "Status", "status",
                 options=["planning", "in_progress", "completed", "on_hold", "cancelled"]
             )
             priority = self.render_select_box(
                 "Priority", "priority",
                 options=["low", "medium", "high", "critical"]
             )
             
             # Financial Section
@@ -245,51 +243,50 @@ class ProjectForm(StandardForm):
             sanitize_form_inputs,
         )
 
         # Sanitize inputs first
         data = sanitize_form_inputs(data)
         errors: list[str] = []
         
         # Required field validation
         errors.extend(validate_required_fields(
             data, ["project_key", "name", "status"]
         ))
         
         # Business rules validation
         errors.extend(validate_business_rules_project(data))
         
         # Date validation (if dates are present)
         if data.get("start_date") and data.get("end_date"):
             if data["start_date"] > data["end_date"]:
                 errors.append("End date must be after start date")
         
         return errors
 
 
 # ------------------------------------------------------------------
 # Convenience functions for form creation
-# create_client_form removed - client functionality eliminated
 
 
 def create_project_form(form_id: str, title: str) -> ProjectForm:
     """Create a project form with standard configuration."""
     return ProjectForm(form_id, title)
 
 
 def render_success_message(message: str, icon: str = "âœ…"):
     """Render a standardized success message."""
     if st:
         st.success(f"{icon} {message}")
 
 
 def render_error_messages(errors: list[str], icon: str = "âŒ"):
     """Render standardized error messages."""
     if st and errors:
         for error in errors:
             st.error(f"{icon} {error}")
 
 
 # ------------------------------------------------------------------
 # Small Form Components (DRY patterns for quick forms and configs)
 def render_timer_config(current_config: Optional[Dict[str, Any]] = None, 
                        form_id: str = "timer_config") -> Optional[Dict[str, Any]]:
     """
@@ -454,43 +451,41 @@ def render_selection_widget(label: str,
         options=options,
         index=current_index,
         key=f"selection_{key_suffix}"
     )
 
 
 # ------------------------------------------------------------------
 # Backwards compatibility and exports
 __all__ = [
     "StandardForm",
     "ProjectForm",
     "create_project_form",
     "render_success_message",
     "render_error_messages",
     "render_timer_config",
     "render_entity_filters", 
     "render_selection_widget"
 ]
 
 
 if __name__ == "__main__":
     # Test simplified form components
     logging.info("ðŸ—ï¸ Testing DRY Form Components - Simplified")
     logging.info("=" * 50)
     
-    # Test form creation - client functionality removed
     project_form = create_project_form("test_project", "Test Project Form")
     
     logging.info("âœ… Form components created successfully")
     logging.info(f"   Project Form ID: {project_form.form_id}")
     logging.info(f"   Project Form Title: {project_form.title}")
     
-    # Test validation - client tests removed
     test_data = {"project_key": "test", "name": "Test Project", "status": "active"}
     errors = project_form.validate_project_data(test_data)
     
     if not errors:
         logging.info("âœ… Validation working: No errors for valid data")
     
     logging.info("âœ… DRY form components test completed")
     logging.info("ðŸ“Š Code reduction: ~76% (577 â†’ 136 lines equivalent)")
     logging.info("ðŸ”§ Enhanced features: Field type extension, hybrid context managers")
     logging.info("ðŸ”’ Security maintained: CSRF protection and input sanitization")
diff --git a/streamlit_extension/config/feature_flags.py b/streamlit_extension/config/feature_flags.py
index d765e7389bba0af1f416c540a6d4f6f63a086150..c64ce28ae685b8996378c8e2b13f245c2d6aaba7 100644
--- a/streamlit_extension/config/feature_flags.py
+++ b/streamlit_extension/config/feature_flags.py
@@ -1,39 +1,38 @@
 from __future__ import annotations
 
 """Simple feature flag management."""
 
 import os
 from enum import Enum
 from typing import Any, Dict
 # Auth imports
 from streamlit_extension.auth.middleware import require_auth, require_admin
 from streamlit_extension.auth.user_model import UserRole
 
 
 class FeatureFlag(str, Enum):
-    NEW_CLIENT_FORM = "NEW_CLIENT_FORM"
     ADVANCED_ANALYTICS = "ADVANCED_ANALYTICS"
     BETA_FEATURES = "BETA_FEATURES"
     MAINTENANCE_MODE = "MAINTENANCE_MODE"
 
 
 class FeatureFlagManager:
     """Manage feature flags with environment overrides."""
 
     def __init__(self) -> None:
         self.flags: Dict[FeatureFlag, bool] = {flag: False for flag in FeatureFlag}
         self.refresh_flags()
 
     def is_enabled(self, flag: FeatureFlag) -> bool:
         return bool(self.flags.get(flag, False))
 
     def get_flag_value(self, flag: FeatureFlag) -> Any:
         return self.flags.get(flag)
 
     def refresh_flags(self) -> Dict[FeatureFlag, bool]:
         for flag in FeatureFlag:
             env_var = f"FF_{flag.value}"
             if env_var in os.environ:
                 value = os.environ[env_var]
                 self.flags[flag] = value.lower() in {"1", "true", "yes", "on"}
         return self.flags
diff --git a/streamlit_extension/pages/__init__.py b/streamlit_extension/pages/__init__.py
index 32cbd83b1a87007223709808ef4745da48fb5af8..539a87eff6959081b337ea5e458e5422b9bd6be8 100644
--- a/streamlit_extension/pages/__init__.py
+++ b/streamlit_extension/pages/__init__.py
@@ -29,51 +29,50 @@ RenderFunc = Callable[[], Optional[object]]  # pÃ¡ginas podem retornar dict/None
 # Helpers de import dinÃ¢mico (com proteÃ§Ã£o)
 # =============================================================================
 
 def _import_page(module_name: str, func_name: str) -> Tuple[Optional[RenderFunc], bool]:
     """Importa mÃ³dulo/func dinamicamente com proteÃ§Ã£o e retorna (callable, disponÃ­vel)."""
     module = safe_streamlit_operation(
         import_module,
         f"{__name__}.{module_name}",
         default_return=None,
         operation_name=f"import_{module_name}",
     )
     if module and hasattr(module, func_name):
         return getattr(module, func_name), True
     return None, False
 
 
 # =============================================================================
 # Import das pÃ¡ginas (lazy-friendly via safe_streamlit_operation)
 # =============================================================================
 
 render_analytics_page, ANALYTICS_AVAILABLE = _import_page("analytics", "render_analytics_page")
 render_kanban_page, KANBAN_AVAILABLE = _import_page("kanban", "render_kanban_page")
 render_gantt_page, GANTT_AVAILABLE = _import_page("gantt", "render_gantt_page")
 render_timer_page, TIMER_AVAILABLE = _import_page("timer", "render_timer_page")
 render_settings_page, SETTINGS_AVAILABLE = _import_page("settings", "render_settings_page")
-# Client functionality removed - no longer needed
 render_projects_page, PROJECTS_AVAILABLE = _import_page("projects", "render_projects_page")
 render_health_page, HEALTH_AVAILABLE = _import_page("health", "render_health_page")
 render_projeto_wizard_page, PROJETO_WIZARD_AVAILABLE = _import_page("projeto_wizard", "render_projeto_wizard_page")
 
 
 def _import_auth_page() -> Tuple[Optional[RenderFunc], bool]:
     try:
         from streamlit_extension.auth.login_page import render_login_page  # type: ignore
         return render_login_page, True
     except Exception:
         return None, False
 
 
 render_login_page, LOGIN_AVAILABLE = _import_auth_page()
 
 
 # =============================================================================
 # DefiniÃ§Ã£o tipada do registro de pÃ¡ginas
 # =============================================================================
 
 @dataclass(frozen=True)
 class PageSpec:
     id: str
     title: str
     icon: str
@@ -129,51 +128,50 @@ PAGE_REGISTRY: Dict[str, PageSpec] = {
     ),
     "gantt": PageSpec(
         id="gantt",
         title="ðŸ“Š Gantt Chart",
         icon="ðŸ“Š",
         description="Project timeline visualization",
         render_func=render_gantt_page,
         available=GANTT_AVAILABLE,
     ),
     "timer": PageSpec(
         id="timer",
         title="â±ï¸ Focus Timer",
         icon="â±ï¸",
         description="TDAH-optimized focus sessions",
         render_func=render_timer_page,
         available=TIMER_AVAILABLE,
     ),
     "settings": PageSpec(
         id="settings",
         title="âš™ï¸ Settings",
         icon="âš™ï¸",
         description="Configuration and preferences",
         render_func=render_settings_page,
         available=SETTINGS_AVAILABLE,
     ),
-    # Client functionality removed - page eliminated
     "projects": PageSpec(
         id="projects",
         title="ðŸ“ Projects",
         icon="ðŸ“",
         description="Project management and tracking",
         render_func=render_projects_page,
         available=PROJECTS_AVAILABLE,
     ),
     "projeto_wizard": PageSpec(
         id="projeto_wizard",
         title="ðŸš€ Criar Projeto",
         icon="ðŸš€",
         description="Wizard completo com IA: VisÃ£o â†’ Ã‰picos â†’ Stories â†’ Tasks",
         render_func=render_projeto_wizard_page,
         available=PROJETO_WIZARD_AVAILABLE,
     ),
     "health": PageSpec(
         id="health",
         title="ðŸ¥ Health",
         icon="ðŸ¥",
         description="System health monitoring and diagnostics",
         render_func=render_health_page,
         available=HEALTH_AVAILABLE,
     ),
 }
diff --git a/streamlit_extension/pages/projects.py b/streamlit_extension/pages/projects.py
index a48e8104bd073281ebe9c206c1ed2cdce856865a..3a6dcaa5dec2889b60341662a5ddf87550c14cf1 100644
--- a/streamlit_extension/pages/projects.py
+++ b/streamlit_extension/pages/projects.py
@@ -1,31 +1,31 @@
 """
 ðŸ“ Project Management Page
 
 Comprehensive project management interface with CRUD operations:
 - Project overview with card-based visualization
-- Client filtering, search and pagination
+- Search and pagination
 - Create, edit, and delete projects (via wizard/actions)
 - Status and progress monitoring
 """
 
 from __future__ import annotations
 
 from typing import Dict, Any, List, Tuple, Union, Optional
 import logging
 
 import streamlit as st
 
 # ===== Project imports (no sys.path hacks) =====
 # --- AutenticaÃ§Ã£o -------------------------------------------------------------
 # Import absoluto (corrige erro crÃ­tico do relatÃ³rio):
 try:
     from streamlit_extension.auth.middleware import init_protected_page, require_auth
 except ImportError:
     # Fallback seguro em desenvolvimento: mantÃ©m pÃ¡gina acessÃ­vel
     def init_protected_page(title: str, *, layout: str = "wide") -> None:
         st.set_page_config(page_title=title, layout=layout)
 
     def require_auth(role: Optional[str] = None):  # type: ignore
         def _decorator(fn):
             def _inner(*args, **kwargs):
                 # Em produÃ§Ã£o real, este fallback nÃ£o deve ser usado.
diff --git a/streamlit_extension/pages/projeto_wizard.py b/streamlit_extension/pages/projeto_wizard.py
index 0fea897398ed7a09833aa00d96ce22d01046846f..6bf2576c79c7d8cf8700f943d1a606fb883e59f0 100644
--- a/streamlit_extension/pages/projeto_wizard.py
+++ b/streamlit_extension/pages/projeto_wizard.py
@@ -41,51 +41,50 @@ _transaction_ctx = None
 
 def _init_db_layer() -> None:
     global _DBM, _create_project_fn, _transaction_ctx
     # Tentamos APIs em ordem: hÃ­brido (modular + manager) â†’ apenas manager
     try:
         # Modular API (transaÃ§Ã£o)
         from streamlit_extension.database import transaction  # type: ignore
         _transaction_ctx = transaction
     except Exception:
         _transaction_ctx = None
 
     try:
         # Enterprise API (DatabaseManager)
         from streamlit_extension.utils.database import DatabaseManager  # type: ignore
         _DBM = DatabaseManager()
     except Exception:
         _DBM = None
 
     # Opcional: modular helpers, se existirem
     try:
         from streamlit_extension.database import create_project as _cp  # type: ignore
         _create_project_fn = _cp
     except Exception:
         _create_project_fn = None
 
-# Client functionality removed - no longer needed
 
 def _create_project_safely(payload: Dict[str, Any]) -> Dict[str, Any]:
     """
     Cria projeto usando o melhor caminho disponÃ­vel:
     1) modular create_project() dentro de transaction()
     2) DatabaseManager().create_project()
     LanÃ§a exceÃ§Ã£o com mensagem amigÃ¡vel se nenhum caminho estiver disponÃ­vel.
     """
     # 1) Modular
     try:
         if _create_project_fn is not None:
             if _transaction_ctx is not None:
                 with _transaction_ctx():  # type: ignore[misc]
                     created = _create_project_fn(payload)  # type: ignore[misc]
                     return {"ok": True, "data": created}
             # Se transaÃ§Ã£o modular nÃ£o existir, ainda tentamos criar direto
             created = _create_project_fn(payload)  # type: ignore[misc]
             return {"ok": True, "data": created}
     except Exception as e:
         return {"ok": False, "error": f"Falha ao criar projeto (API modular): {e}"}
 
     # 2) DatabaseManager
     try:
         if _DBM is not None and hasattr(_DBM, "create_project"):
             created = _DBM.create_project(payload)  # type: ignore[attr-defined]
diff --git a/streamlit_extension/services/epic_service.py b/streamlit_extension/services/epic_service.py
index 1f940abfe3269ed290b8c757561f673f04cc4ef1..b12f638663efd6907b7a20cfb299ec28fea2b02b 100644
--- a/streamlit_extension/services/epic_service.py
+++ b/streamlit_extension/services/epic_service.py
@@ -24,148 +24,142 @@ class EpicRepository(BaseRepository):
         self._epicrepositorydataaccess = EpicRepositoryDataaccess()
     # Delegation to EpicRepositoryLogging
     def __init__(self):
         self._epicrepositorylogging = EpicRepositoryLogging()
     # Delegation to EpicRepositoryErrorhandling
     def __init__(self):
         self._epicrepositoryerrorhandling = EpicRepositoryErrorhandling()
     # Delegation to EpicRepositoryFormatting
     def __init__(self):
         self._epicrepositoryformatting = EpicRepositoryFormatting()
     # Delegation to EpicRepositoryNetworking
     def __init__(self):
         self._epicrepositorynetworking = EpicRepositoryNetworking()
     # Delegation to EpicRepositoryCalculation
     def __init__(self):
         self._epicrepositorycalculation = EpicRepositoryCalculation()
     # Delegation to EpicRepositorySerialization
     def __init__(self):
         self._epicrepositoryserialization = EpicRepositorySerialization()
     """Repository for epic data access operations."""
     
     def __init__(self, db_manager: DatabaseManager):
         super().__init__(db_manager)
     
     def find_by_id(self, epic_id: int) -> Optional[Dict[str, Any]]:
-        """Find epic by ID with project and client information."""
+        """Find epic by ID with project information."""
         try:
             query = """
                 SELECT e.*, p.name as project_name
                 FROM framework_epics e
                 LEFT JOIN framework_projects p ON e.project_id = p.id
                 WHERE e.id = ?
             """
             result = self.db_manager.execute_query(query, (epic_id,))
             return result[0] if result else None
         except Exception as e:
             self.db_manager.logger.error(f"Error finding epic by ID {epic_id}: {e}")
             return None
     
     def find_by_key(self, epic_key: str) -> Optional[Dict[str, Any]]:
         """Find epic by unique key."""
         try:
             query = "SELECT * FROM framework_epics WHERE epic_key = ?"
             result = self.db_manager.execute_query(query, (epic_key,))
             return result[0] if result else None
         except Exception as e:
             self.db_manager.logger.error(f"Error finding epic by key {epic_key}: {e}")
             return None
     
     def find_all(
         self, 
         filters: Optional[FilterCriteria] = None,
         sort: Optional[SortCriteria] = None,
         page: int = 1,
         page_size: int = 10
     ) -> PaginatedResult[Dict[str, Any]]:
         """Find all epics with filtering, sorting, and pagination."""
         try:
-            # Build base query with project and client information
+            # Build base query with project information
             base_query = """
                 SELECT e.*, p.name as project_name,
                        COUNT(t.id) as task_count,
                        SUM(CASE WHEN t.status = 'completed' THEN 1 ELSE 0 END) as completed_tasks
                 FROM framework_epics e
                 LEFT JOIN framework_projects p ON e.project_id = p.id
                 LEFT JOIN framework_tasks t ON e.id = t.epic_id
             """
             
             where_conditions = []
             params = []
             
             if filters:
                 if filters.has('status'):
                     where_conditions.append("e.status = ?")
                     params.append(filters.get('status'))
                 
                 if filters.has('title'):
                     where_conditions.append("e.title LIKE ?")
                     params.append(f"%{filters.get('title')}%")
                 
                 if filters.has('epic_key'):
                     where_conditions.append("e.epic_key LIKE ?")
                     params.append(f"%{filters.get('epic_key')}%")
                 
                 if filters.has('project_id'):
                     where_conditions.append("e.project_id = ?")
                     params.append(filters.get('project_id'))
                 
-                if filters.has('client_id'):
-                    where_conditions.append("p.client_id = ?")
-                    params.append(filters.get('client_id'))
-                
                 if filters.has('priority'):
                     where_conditions.append("e.priority = ?")
                     params.append(filters.get('priority'))
                 
                 if filters.has('difficulty'):
                     where_conditions.append("e.difficulty = ?")
                     params.append(filters.get('difficulty'))
                 
                 if filters.has('focus_time_min'):
                     where_conditions.append("e.focus_time_minutes >= ?")
                     params.append(filters.get('focus_time_min'))
                 
                 if filters.has('points_min'):
                     where_conditions.append("e.points >= ?")
                     params.append(filters.get('points_min'))
             
             # Build WHERE clause
             where_clause = " WHERE " + " AND ".join(where_conditions) if where_conditions else ""
             group_clause = " GROUP BY e.id"
             
             # Build ORDER BY clause
             order_clause = ""
             if sort:
                 # Map sort fields to qualified column names
                 sort_field = sort.field
                 if sort_field in ['title', 'status', 'priority', 'difficulty', 'points', 'epic_key']:
                     sort_field = f"e.{sort_field}"
                 elif sort_field in ['project_name']:
                     sort_field = f"p.name"
-                elif sort_field in ['client_name']:
-                    sort_field = f"c.name"
                 elif sort_field == 'progress':
                     sort_field = "(completed_tasks * 100.0 / NULLIF(task_count, 0))"
                 
                 order_clause = f" ORDER BY {sort_field} {'ASC' if sort.ascending else 'DESC'}"
             
             # Count total records (need subquery for GROUP BY)
             count_query = f"""
                 SELECT COUNT(*) FROM (
                     SELECT e.id
                     FROM framework_epics e
                     LEFT JOIN framework_projects p ON e.project_id = p.id
                     {where_clause}
                     GROUP BY e.id
                 )
             """
             total_count = self.db_manager.execute_query(count_query, params)[0]['COUNT(*)']
             
             # Calculate pagination
             offset = (page - 1) * page_size
             total_pages = (total_count + page_size - 1) // page_size
             
             # Get paginated results
             data_query = f"{base_query}{where_clause}{group_clause}{order_clause} LIMIT ? OFFSET ?"
             data_params = params + [page_size, offset]
             epics = self.db_manager.execute_query(data_query, data_params)
@@ -591,55 +585,55 @@ class EpicService(BaseService):
                 f"Epic with key '{epic_data['epic_key']}' already exists"
             )
         
         try:
             # Auto-calculate points based on difficulty and estimated hours if not provided
             if 'points' not in epic_data or epic_data['points'] is None:
                 epic_data['points'] = self._calculate_epic_points(
                     epic_data.get('difficulty', 3),
                     epic_data.get('estimated_hours', 0)
                 )
             
             # Create epic
             epic_id = self.repository.create(epic_data)
             
             if epic_id:
                 self.log_operation("create_epic_success", epic_id=epic_id)
                 return ServiceResult.ok(epic_id)
             else:
                 return self.handle_database_error("create_epic", Exception("Failed to create epic"))
                 
         except Exception as e:
             return self.handle_database_error("create_epic", e)
     
     def get_epic(self, epic_id: int) -> ServiceResult[Dict[str, Any]]:
         """
-        Get epic by ID with project and client information.
-        
+        Get epic by ID with project information.
+
         Args:
             epic_id: Epic ID
-            
+
         Returns:
             ServiceResult with epic data if found
         """
         self.log_operation("get_epic", epic_id=epic_id)
         
         try:
             epic = self.repository.find_by_id(epic_id)
             
             if epic:
                 # Parse JSON fields
                 if 'goals' in epic and epic['goals']:
                     try:
                         epic['goals'] = json.loads(epic['goals'])
                     except json.JSONDecodeError:
                         epic['goals'] = []
                 
                 if 'definition_of_done' in epic and epic['definition_of_done']:
                     try:
                         epic['definition_of_done'] = json.loads(epic['definition_of_done'])
                     except json.JSONDecodeError:
                         epic['definition_of_done'] = []
                 
                 return ServiceResult.ok(epic)
             else:
                 return ServiceResult.not_found("Epic", epic_id)
diff --git a/streamlit_extension/services/project_service.py b/streamlit_extension/services/project_service.py
index 7c83e2f19653083a45e2a94ccf7ff3d34107fbba..a64045b1d168c7f693559e9bd7f4b3593c24bbc7 100644
--- a/streamlit_extension/services/project_service.py
+++ b/streamlit_extension/services/project_service.py
@@ -1,149 +1,145 @@
 """
 ðŸ“ Project Service Layer
 
 Business logic for project management operations.
-Implements complete CRUD operations with client relationships and validation.
+Implements complete CRUD operations with validation.
 """
 
 from typing import Dict, List, Optional, Any, Tuple
 from datetime import datetime, date
 import re
 
 from .base import (
     BaseService, ServiceResult, ServiceError, ServiceErrorType,
     BaseRepository, PaginatedResult, FilterCriteria, SortCriteria
 )
 from ..utils.database import DatabaseManager
 from ..config.constants import ValidationRules, ProjectStatus
 
 
 class ProjectRepository(BaseRepository):
     # Delegation to ProjectRepositoryDataaccess
     def __init__(self):
         self._projectrepositorydataaccess = ProjectRepositoryDataaccess()
     # Delegation to ProjectRepositoryLogging
     def __init__(self):
         self._projectrepositorylogging = ProjectRepositoryLogging()
     # Delegation to ProjectRepositoryErrorhandling
     def __init__(self):
         self._projectrepositoryerrorhandling = ProjectRepositoryErrorhandling()
     # Delegation to ProjectRepositoryFormatting
     def __init__(self):
         self._projectrepositoryformatting = ProjectRepositoryFormatting()
     # Delegation to ProjectRepositoryNetworking
     def __init__(self):
         self._projectrepositorynetworking = ProjectRepositoryNetworking()
     # Delegation to ProjectRepositoryCalculation
     def __init__(self):
         self._projectrepositorycalculation = ProjectRepositoryCalculation()
     """Repository for project data access operations."""
     
     def __init__(self, db_manager: DatabaseManager):
         super().__init__(db_manager)
     
     def find_by_id(self, project_id: int) -> Optional[Dict[str, Any]]:
-        """Find project by ID with client information."""
+        """Find project by ID."""
         try:
             query = """
                 SELECT p.*
                 FROM framework_projects p
                 WHERE p.id = ?
             """
             result = self.db_manager.execute_query(query, (project_id,))
             return result[0] if result else None
         except Exception as e:
             self.db_manager.logger.error(f"Error finding project by ID {project_id}: {e}")
             return None
     
     def find_by_name(self, name: str) -> Optional[Dict[str, Any]]:
         """Find project by name."""
         try:
             query = "SELECT * FROM framework_projects WHERE name = ?"
             result = self.db_manager.execute_query(query, (name,))
             return result[0] if result else None
         except Exception as e:
             self.db_manager.logger.error(f"Error finding project by name {name}: {e}")
             return None
     
     def find_all(
         self, 
         filters: Optional[FilterCriteria] = None,
         sort: Optional[SortCriteria] = None,
         page: int = 1,
         page_size: int = 10
     ) -> PaginatedResult[Dict[str, Any]]:
         """Find all projects with filtering, sorting, and pagination."""
         try:
             # Build base query
             base_query = """
                 SELECT p.*
                 FROM framework_projects p
             """
             
             where_conditions = []
             params = []
             
             if filters:
                 if filters.has('status'):
                     where_conditions.append("p.status = ?")
                     params.append(filters.get('status'))
                 
                 if filters.has('name'):
                     where_conditions.append("p.name LIKE ?")
                     params.append(f"%{filters.get('name')}%")
                 
-                # Client filtering removed - direct project access
-                
                 if filters.has('budget_min'):
                     where_conditions.append("p.budget >= ?")
                     params.append(filters.get('budget_min'))
                 
                 if filters.has('budget_max'):
                     where_conditions.append("p.budget <= ?")
                     params.append(filters.get('budget_max'))
                 
                 if filters.has('start_date_from'):
                     where_conditions.append("p.start_date >= ?")
                     params.append(filters.get('start_date_from'))
                 
                 if filters.has('end_date_to'):
                     where_conditions.append("p.end_date <= ?")
                     params.append(filters.get('end_date_to'))
             
             # Build WHERE clause
             where_clause = " WHERE " + " AND ".join(where_conditions) if where_conditions else ""
             
             # Build ORDER BY clause
             order_clause = ""
             if sort:
                 # Map sort fields to qualified column names
                 sort_field = sort.field
                 if sort_field in ['name', 'status', 'start_date', 'end_date', 'budget']:
                     sort_field = f"p.{sort_field}"
-                elif sort_field in ['client_name']:
-                    sort_field = f"c.name"
                 
                 order_clause = f" ORDER BY {sort_field} {'ASC' if sort.ascending else 'DESC'}"
             
             # Count total records
             # Count total records (usar alias para chave estÃ¡vel no resultado)
             count_query = f"""
                 SELECT COUNT(*) AS total
                 FROM framework_projects p
                 {where_clause}
             """
             total_count = self.db_manager.execute_query(count_query, params)[0]['total']
             
             # Calculate pagination
             offset = (page - 1) * page_size
             total_pages = (total_count + page_size - 1) // page_size
             
             # Get paginated results
             data_query = f"{base_query}{where_clause}{order_clause} LIMIT ? OFFSET ?"
             data_params = params + [page_size, offset]
             projects = self.db_manager.execute_query(data_query, data_params)
             
             return PaginatedResult(
                 items=projects,
                 total=total_count,
                 page=page,
@@ -273,51 +269,50 @@ class ProjectRepository(BaseRepository):
             # Calculate progress (completed tasks / total tasks)
             total_tasks = sum(row['count'] for row in task_counts)
             completed_tasks = sum(row['count'] for row in task_counts if row['status'] == 'completed')
             progress = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
             
             return {
                 'epic_counts': {row['status']: row['count'] for row in epic_counts},
                 'task_counts': {row['status']: row['count'] for row in task_counts},
                 'total_epics': sum(row['count'] for row in epic_counts),
                 'total_tasks': total_tasks,
                 'completed_tasks': completed_tasks,
                 'progress_percentage': round(progress, 2)
             }
             
         except Exception as e:
             self.db_manager.logger.error(f"Error getting project metrics for {project_id}: {e}")
             return {
                 'epic_counts': {},
                 'task_counts': {},
                 'total_epics': 0,
                 'total_tasks': 0,
                 'completed_tasks': 0,
                 'progress_percentage': 0
             }
     
-    # Client functionality removed - all client-related methods eliminated
 
 
 class ProjectService(BaseService):
     """Service for project business logic operations."""
     
     def __init__(self, db_manager: DatabaseManager):
         self.repository = ProjectRepository(db_manager)
         super().__init__(self.repository)
     
     def validate_business_rules(self, data: Dict[str, Any]) -> List[ServiceError]:
         """Validate project-specific business rules."""
         errors = []
         
         # Name length validation
         if 'name' in data and data['name']:
             if len(data['name']) > ValidationRules.MAX_NAME_LENGTH.value:
                 errors.append(ServiceError(
                     error_type=ServiceErrorType.VALIDATION_ERROR,
                     message=f"Name cannot exceed {ValidationRules.MAX_NAME_LENGTH.value} characters",
                     field="name"
                 ))
         
         # Budget validation
         if 'budget' in data and data['budget'] is not None:
             try:
@@ -426,55 +421,55 @@ class ProjectService(BaseService):
         if validation_errors:
             return ServiceResult.fail_multiple(validation_errors)
         
         # Check project name uniqueness
         existing_project = self.repository.find_by_name(project_data['name'])
         if existing_project:
             return ServiceResult.business_rule_violation(
                 f"Project '{project_data['name']}' already exists"
             )
         
         try:
             # Create project
             project_id = self.repository.create(project_data)
             
             if project_id:
                 self.log_operation("create_project_success", project_id=project_id)
                 return ServiceResult.ok(project_id)
             else:
                 return self.handle_database_error("create_project", Exception("Failed to create project"))
                 
         except Exception as e:
             return self.handle_database_error("create_project", e)
     
     def get_project(self, project_id: int) -> ServiceResult[Dict[str, Any]]:
         """
-        Get project by ID with client information.
-        
+        Get project by ID.
+
         Args:
             project_id: Project ID
-            
+
         Returns:
             ServiceResult with project data if found
         """
         self.log_operation("get_project", project_id=project_id)
         
         try:
             project = self.repository.find_by_id(project_id)
             
             if project:
                 return ServiceResult.ok(project)
             else:
                 return ServiceResult.not_found("Project", project_id)
                 
         except Exception as e:
             return self.handle_database_error("get_project", e)
     
     def update_project(self, project_id: int, project_data: Dict[str, Any]) -> ServiceResult[bool]:
         """
         Update existing project.
         
         Args:
             project_id: Project ID
             project_data: Updated project information
             
         Returns:
@@ -602,55 +597,55 @@ class ProjectService(BaseService):
                              returned_count=len(result.items))
             
             return ServiceResult.ok(result)
             
         except Exception as e:
             return self.handle_database_error("list_projects", e)
     
     def get_all_projects(self) -> ServiceResult[List[Dict[str, Any]]]:
         """
         Get all projects.
         
         Returns:
             ServiceResult with list of projects
         """
         self.log_operation("get_all_projects")
         
         try:
             projects = self.repository.find_all_projects()
             return ServiceResult.ok(projects)
             
         except Exception as e:
             return self.handle_database_error("get_all_projects", e)
     
     def get_project_summary(self, project_id: int) -> ServiceResult[Dict[str, Any]]:
         """
-        Get project summary with metrics and client information.
-        
+        Get project summary with metrics.
+
         Args:
             project_id: Project ID
-            
+
         Returns:
             ServiceResult with project summary data
         """
         self.log_operation("get_project_summary", project_id=project_id)
         
         try:
             # Get project data
             project_result = self.get_project(project_id)
             if not project_result.success:
                 return project_result
             
             project = project_result.data
             
             # Get project metrics
             metrics = self.repository.get_project_metrics(project_id)
             
             # Build summary
             summary = {
                 **project,
                 **metrics,
                 'status_display': ProjectStatus.get_display_name(project.get('status', 'planning')),
                 'has_epics': metrics['total_epics'] > 0,
                 'duration_days': self._calculate_project_duration(project.get('start_date'), project.get('end_date'))
             }
             
diff --git a/streamlit_extension/utils/auth_manager.py b/streamlit_extension/utils/auth_manager.py
index f3d53dbb154ee374a58d5d66d5311a0b7c7ddaa7..6c32937d4f08582d989fb54b57f775528d4e358f 100644
--- a/streamlit_extension/utils/auth_manager.py
+++ b/streamlit_extension/utils/auth_manager.py
@@ -289,68 +289,66 @@ class AuthenticationManager:
         if not user:
             st.error("ðŸ”’ Authentication required. Please log in.")
             st.stop()
         return user
     
     def require_permission(self, permission: str) -> User:
         """Decorator/helper to require specific permission."""
         user = self.require_authentication()
         if not user.has_permission(permission):
             st.error(f"ðŸš« Access denied. Required permission: {permission}")
             st.stop()
         return user
     
     def require_role(self, role: str) -> User:
         """Decorator/helper to require specific role."""
         user = self.require_authentication()
         if not user.has_role(role):
             st.error(f"ðŸš« Access denied. Required role: {role}")
             st.stop()
         return user
     
     def _get_permissions_for_roles(self, roles: List[str]) -> Set[str]:
         """Map roles to permissions."""
         permission_map = {
             'admin': {
-                'create_client', 'read_client', 'update_client', 'delete_client',
                 'create_project', 'read_project', 'update_project', 'delete_project',
                 'create_epic', 'read_epic', 'update_epic', 'delete_epic',
                 'create_task', 'read_task', 'update_task', 'delete_task',
                 'admin_panel', 'user_management', 'system_settings'
             },
             'manager': {
-                'create_client', 'read_client', 'update_client',
                 'create_project', 'read_project', 'update_project',
                 'create_epic', 'read_epic', 'update_epic',
                 'create_task', 'read_task', 'update_task'
             },
             'user': {
-                'read_client', 'read_project', 'read_epic', 'read_task',
+                'read_project', 'read_epic', 'read_task',
                 'create_task', 'update_task'
             },
             'viewer': {
-                'read_client', 'read_project', 'read_epic', 'read_task'
+                'read_project', 'read_epic', 'read_task'
             }
         }
         
         permissions = set()
         for role in roles:
             if role in permission_map:
                 permissions.update(permission_map[role])
         
         return permissions
 
 
 def login_form() -> Optional[Session]:
     """Render login form and handle authentication."""
     st.subheader("ðŸ” Login")
     
     with st.form("login_form"):
         username = st.text_input("Username")
         password = st.text_input("Password", type="password")
         submitted = st.form_submit_button("Login")
         
         if submitted:
             if not username or not password:
                 st.error("Please provide both username and password")
                 return None
             
diff --git a/streamlit_extension/utils/cached_database.py b/streamlit_extension/utils/cached_database.py
index 9e373964b2581ca2657f9f29c6ed7579eb63b4b6..4d7529b5fe95a9d98b38dffa2b01be50d94dcdc8 100644
--- a/streamlit_extension/utils/cached_database.py
+++ b/streamlit_extension/utils/cached_database.py
@@ -177,85 +177,71 @@ class CachedDatabaseManager:
     def _invalidate_related_cache(self, entity_type: str, entity_id: Optional[int] = None, **kwargs):
         """
         Invalidate cache entries related to data modification.
         
         Args:
             entity_type: Type of entity (project, epic, task)
             entity_id: Specific entity ID (if applicable)
             **kwargs: Additional parameters for cache invalidation
         """
         if not self.enable_cache:
             return
         
         try:
             # Invalidate specific entity cache
             if entity_id:
                 invalidate_cache(entity_type, entity_id)
             
             # Invalidate list caches
             invalidate_cache(f"{entity_type}_list")
             invalidate_cache(f"{entity_type}_search")
             
             # Invalidate analytics caches
             invalidate_cache("analytics")
             invalidate_cache("aggregation")
             
-            # Entity-specific invalidations - client invalidation removed
+            # Entity-specific invalidations
             if entity_type == "project":
                 # Project changes affect epics
                 invalidate_cache("epic_list")
                 invalidate_cache("project_epics")
             elif entity_type == "epic":
                 # Epic changes affect tasks
                 invalidate_cache("task_list")
                 invalidate_cache("epic_tasks")
             elif entity_type == "task":
                 # Task changes affect epic progress
                 if "epic_id" in kwargs:
                     invalidate_cache("epic", kwargs["epic_id"])
                     invalidate_cache("epic_progress")
             
             if self.cache_debug:
                 self.logger.debug(f"Cache invalidated for {entity_type} operations")
                 
         except Exception as e:
             self.logger.error(f"Cache invalidation error: {e}")
     
-    # =============================================================================
-    # CLIENT OPERATIONS WITH CACHING
-    # =============================================================================
-    
-    # get_clients removed - client functionality eliminated
-    
-    # get_client removed - client functionality eliminated
-    
-    # create_client removed - client functionality eliminated
-    
-    # update_client removed - client functionality eliminated
-    
-    # delete_client removed - client functionality eliminated
-    
     # =============================================================================
     # PROJECT OPERATIONS WITH CACHING
     # =============================================================================
     
     @cached("project_list", operation_type="quick")
     def get_projects(self,
                     include_inactive: bool = False,
                     search_name: Optional[str] = None,
                     status_filter: Optional[str] = None,
                     limit: Optional[int] = None,
                     offset: int = 0) -> Dict[str, Any]:
         """Get projects with caching support."""
         start_time = time.time()
         
         try:
             result = self.db_manager.get_projects(
                 include_inactive=include_inactive,
                 search_name=search_name,
                 status_filter=status_filter,
                 limit=limit,
                 offset=offset
             )
             
             response_time = time.time() - start_time
             operation_type = "cache_hits" if hasattr(self, '_from_cache') else "db_direct_calls"
@@ -272,51 +258,50 @@ class CachedDatabaseManager:
     def get_project(self, project_id: int) -> Optional[Dict[str, Any]]:
         """Get single project with caching."""
         start_time = time.time()
         
         try:
             result = self.db_manager.get_project(project_id)
             
             response_time = time.time() - start_time
             operation_type = "cache_hits" if hasattr(self, '_from_cache') else "db_direct_calls"
             self._record_operation(operation_type, response_time)
             
             return result
             
         except Exception as e:
             self.logger.error(f"Error getting project {project_id}: {e}")
             self._record_operation("db_direct_calls", time.time() - start_time)
             raise
     
     def create_project(self, **kwargs) -> Optional[int]:
         """Create project and invalidate related cache."""
         try:
             result = self.db_manager.create_project(**kwargs)
             
             if result:
                 self._invalidate_related_cache("project")
-                # Client cache invalidation removed
                 self.logger.debug(f"Project created: {result}")
             
             return result
             
         except Exception as e:
             self.logger.error(f"Error creating project: {e}")
             raise
     
     def update_project(self, project_id: int, **kwargs) -> bool:
         """Update project and invalidate related cache."""
         try:
             result = self.db_manager.update_project(project_id, **kwargs)
             
             if result:
                 self._invalidate_related_cache("project", project_id)
                 self.logger.debug(f"Project updated: {project_id}")
             
             return result
             
         except Exception as e:
             self.logger.error(f"Error updating project {project_id}: {e}")
             raise
     
     def delete_project(self, project_id: int, soft_delete: bool = True) -> bool:
         """Delete project and invalidate related cache."""
@@ -437,51 +422,50 @@ class CachedDatabaseManager:
             result = self.db_manager.get_task(task_id)
             
             response_time = time.time() - start_time
             operation_type = "cache_hits" if hasattr(self, '_from_cache') else "db_direct_calls"
             self._record_operation(operation_type, response_time)
             
             return result
             
         except Exception as e:
             self.logger.error(f"Error getting task {task_id}: {e}")
             self._record_operation("db_direct_calls", time.time() - start_time)
             raise
     
     # =============================================================================
     # ANALYTICS AND AGGREGATIONS WITH HEAVY CACHING
     # =============================================================================
     
     @cached("analytics_dashboard", operation_type="heavy")
     def get_dashboard_analytics(self) -> Dict[str, Any]:
         """Get dashboard analytics with heavy caching."""
         start_time = time.time()
         
         try:
             # This would be a heavy aggregation query
             result = {
-                # Client count removed - client functionality eliminated
                 "total_projects": len(self.get_projects().get("data", [])),
                 "total_epics": len(self.get_epics()),
                 "total_tasks": len(self.get_tasks()),
                 "completed_tasks": len([t for t in self.get_tasks() if t.get("status") == "completed"]),
                 "in_progress_tasks": len([t for t in self.get_tasks() if t.get("status") == "in_progress"]),
                 "cache_info": self.get_performance_stats()
             }
             
             response_time = time.time() - start_time
             operation_type = "cache_hits" if hasattr(self, '_from_cache') else "db_direct_calls"
             self._record_operation(operation_type, response_time)
             
             return result
             
         except Exception as e:
             self.logger.error(f"Error getting dashboard analytics: {e}")
             self._record_operation("db_direct_calls", time.time() - start_time)
             raise
     
     # =============================================================================
     # CACHE MANAGEMENT AND MONITORING
     # =============================================================================
     
     def get_performance_stats(self) -> Dict[str, Any]:
         """Get performance statistics."""
diff --git a/streamlit_extension/utils/constants.py b/streamlit_extension/utils/constants.py
index 2542f185c8078f3690a3e6e10171f98557433cc5..358c3d3a437f2818e0e75f47e03341f0631d5583 100644
--- a/streamlit_extension/utils/constants.py
+++ b/streamlit_extension/utils/constants.py
@@ -8,51 +8,50 @@ from streamlit_extension.auth.user_model import UserRole
 
 
 class TableNames:
     """Database table names."""
     PROJECTS: Final[str] = "framework_projects"
     EPICS: Final[str] = "framework_epics"
     TASKS: Final[str] = "framework_tasks"
     USERS: Final[str] = "framework_users"
     WORK_SESSIONS: Final[str] = "work_sessions"
     ACHIEVEMENTS: Final[str] = "user_achievements"
     STREAKS: Final[str] = "user_streaks"
 
 
 class FieldNames:
     """Common database field names."""
     ID: Final[str] = "id"
     NAME: Final[str] = "name"
     EMAIL: Final[str] = "email"
     STATUS: Final[str] = "status"
     CREATED_AT: Final[str] = "created_at"
     UPDATED_AT: Final[str] = "updated_at"
     PROJECT_ID: Final[str] = "project_id"
     EPIC_ID: Final[str] = "epic_id"
 
 
-# Client status removed - client functionality eliminated
 
 
 class ProjectStatus(Enum):
     """Project status values."""
     PLANNING = "planning"
     ACTIVE = "active"
     ON_HOLD = "on_hold"
     COMPLETED = "completed"
     CANCELLED = "cancelled"
 
 
 class TaskStatus(Enum):
     """Task status values."""
     TODO = "todo" # Tracked: 2025-08-21
     IN_PROGRESS = "in_progress"
     TESTING = "testing"
     DONE = "done"
     BLOCKED = "blocked"
 
 
 class EpicStatus(Enum):
     """Epic status values."""
     DRAFT = "draft"
     READY = "ready"
     IN_PROGRESS = "in_progress"
diff --git a/streamlit_extension/utils/database.py b/streamlit_extension/utils/database.py
index 352bdd9a17e0b0b9a473e5b89106bacf19042f38..c118ee24aaf95607abe76ca43e5a81de7f8776a5 100644
--- a/streamlit_extension/utils/database.py
+++ b/streamlit_extension/utils/database.py
@@ -767,51 +767,51 @@ class DatabaseManager(PerformancePaginationMixin):
         record_id: int, 
         cascade_tables: Optional[List[str]] = None
     ) -> bool:
         """
         Safely delete a record and its cascaded dependencies using transactions.
         
         Prevents table locks during large cascade operations by using appropriate
         isolation levels and proper transaction boundaries.
         
         Args:
             table_name: Primary table to delete from
             record_id: ID of the record to delete
             cascade_tables: Optional list of dependent tables to clean up
             
         Returns:
             True if deletion succeeded, False otherwise
             
         Example:
             >>> # Delete project and all related data
             >>> success = db.delete_cascade_safe(
             ...     "projects", 
             ...     project_id, 
             ...     cascade_tables=["epics", "tasks"]
             ... )
         """
-        # Define safe cascade relationships (client layer removed)
+        # Define safe cascade relationships
         safe_relationships = {
             "projects": ["epics", "tasks"],
             "epics": ["tasks"]
         }
         
         # Use provided cascade tables or safe defaults
         tables_to_clean = cascade_tables or safe_relationships.get(table_name, [])
         
         # Build delete operations in dependency order (children first)
         delete_ops = []
         
         # Add cascade deletes (reverse order for dependencies)
         for cascade_table in reversed(tables_to_clean):
             if cascade_table == "tasks" and table_name == "projects":
                 delete_ops.append((
                     f"DELETE FROM {cascade_table} WHERE epic_id IN (SELECT id FROM epics WHERE project_id = :record_id)",
                     {"record_id": record_id}
                 ))
             elif cascade_table == "tasks" and table_name == "epics":
                 delete_ops.append((
                     f"DELETE FROM {cascade_table} WHERE epic_id = :record_id",
                     {"record_id": record_id}
                 ))
             elif cascade_table == "epics" and table_name == "projects":
                 delete_ops.append((
@@ -2563,51 +2563,51 @@ class DatabaseManager(PerformancePaginationMixin):
                     result = conn.execute(text("""
                         SELECT id, title, status, tdd_phase, estimate_minutes,
                                created_at, updated_at, completed_at, priority
                         FROM framework_tasks 
                         WHERE epic_id = :epic_id AND deleted_at IS NULL
                         ORDER BY priority ASC, created_at ASC
                     """), {"epic_id": epic_id})
                     return [dict(row._mapping) for row in result]
                 else:
                     cursor = conn.cursor()
                     cursor.execute("""
                         SELECT id, title, status, tdd_phase, estimate_minutes,
                                created_at, updated_at, completed_at, priority
                         FROM framework_tasks 
                         WHERE epic_id = ? AND deleted_at IS NULL
                         ORDER BY priority ASC, created_at ASC
                     """, (epic_id,))
                     
                     return [dict(zip([col[0] for col in cursor.description], row)) 
                            for row in cursor.fetchall()]
                            
         except Exception:
             return []
     
     # ==================================================================================
-    # HIERARCHY SYSTEM METHODS (CLIENT â†’ PROJECT â†’ EPIC â†’ TASK) - SCHEMA V6
+    # HIERARCHY SYSTEM METHODS (PROJECT â†’ EPIC â†’ TASK) - SCHEMA V6
     # ==================================================================================
     
 
     
     @cache_database_query("get_projects", ttl=300) if CACHE_AVAILABLE else lambda f: f
     def get_projects(
         self,
         include_inactive: bool = False,
         page: int = 1,
         page_size: int = 50,
         status_filter: Optional[Union[ProjectStatus, str]] = None,
         project_type_filter: str = "",
     ) -> Dict[str, Any]:
         """Get projects with caching support and pagination.
         
         Args:
             include_inactive: If True, include inactive/archived projects
             page: Page number (1-based)
             page_size: Number of items per page
             status_filter: Filter by specific status
             project_type_filter: Filter by project type
             
         Returns:
             Dictionary with 'data' (list of projects), 'total', 'page', 'total_pages'
         """
@@ -2913,51 +2913,51 @@ class DatabaseManager(PerformancePaginationMixin):
             **kwargs: Additional project fields
             
         Returns:
             Project ID if successful, None otherwise
         """
         try:
             project_data = {
                 'project_key': project_key,
                 'name': name,
                 'description': description,
                 'project_type': project_type,
                 'methodology': methodology,
                 'status': kwargs.get('status', ProjectStatus.PLANNING.value),
                 'priority': kwargs.get('priority', 5),
                 'health_status': kwargs.get('health_status', 'green'),
                 'completion_percentage': kwargs.get('completion_percentage', 0),
                 'planned_start_date': kwargs.get('planned_start_date'),
                 'planned_end_date': kwargs.get('planned_end_date'),
                 'estimated_hours': kwargs.get('estimated_hours', 0),
                 'budget_amount': kwargs.get('budget_amount', 0),
                 'budget_currency': kwargs.get('budget_currency', 'BRL'),
                 'hourly_rate': kwargs.get('hourly_rate'),
                 'project_manager_id': kwargs.get('project_manager_id', 1),
                 'technical_lead_id': kwargs.get('technical_lead_id', 1),
                 'repository_url': kwargs.get('repository_url', ''),
-                'visibility': kwargs.get('visibility', 'client'),
+                'visibility': kwargs.get('visibility', 'internal'),
                 'access_level': kwargs.get('access_level', 'standard'),
                 'complexity_score': kwargs.get('complexity_score', 5.0),
                 'quality_score': kwargs.get('quality_score', 8.0),
                 'created_by': kwargs.get('created_by', 1)
             }
             
             with self.get_connection("framework") as conn:
                 # Remove None values
                 project_data = {k: v for k, v in project_data.items() if v is not None}
                 
                 placeholders = ', '.join(['?' for _ in project_data])
                 columns = ', '.join(project_data.keys())
 
                 if SQLALCHEMY_AVAILABLE:
                     named_placeholders = ', '.join([f':{key}' for key in project_data.keys()])
                     result = conn.execute(
                         text(f"INSERT INTO {TableNames.PROJECTS} ({columns}) VALUES ({named_placeholders})"),  # nosec B608
                         project_data
                     )
                     conn.commit()
                     return result.lastrowid
                 else:
                     cursor = conn.cursor()
                     cursor.execute(
                         f"INSERT INTO {TableNames.PROJECTS} ({columns}) VALUES ({placeholders})",
diff --git a/streamlit_extension/utils/form_validation.py b/streamlit_extension/utils/form_validation.py
index 3241c8bbfe00c82a30cedad54b492cc45c870851..c4deab11cffbb5b6bd816f1c9ea91090ab4677f4 100644
--- a/streamlit_extension/utils/form_validation.py
+++ b/streamlit_extension/utils/form_validation.py
@@ -1,93 +1,92 @@
 """ðŸ” Form Validation Module
 
 Centraliza validaÃ§Ãµes de formulÃ¡rios:
 - Campos obrigatÃ³rios
 - Regras de negÃ³cio
 - Formatos (email, telefone)
 - SanitizaÃ§Ã£o de entradas
 """
 
 from __future__ import annotations
 
 import re
 from typing import Any, Dict, Iterable, List, Optional
 
 try:
     from .security import sanitize_input, validate_form
 except ImportError:  # pragma: no cover
     sanitize_input = lambda x, field_name="input": x  # type: ignore
     validate_form = None
 
 EMAIL_RE = re.compile(r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$")
 PHONE_RE = re.compile(r"^[+\d][\d\s().-]{7,}$")
 
 __all__ = [
     "validate_required_fields", "validate_email_format", "validate_phone_format",
-    "validate_text_length",  # validate_business_rules_client removed
+    "validate_text_length",
     "validate_business_rules_project", "sanitize_form_inputs",
 ]
 
 
 def validate_required_fields(data: Dict[str, Any], required_fields: Iterable[str]) -> List[str]:
     """Valida se campos obrigatÃ³rios existem e nÃ£o sÃ£o vazios/whitespace."""
     errors: List[str] = []
     for field in required_fields:
         value = data.get(field)
         if value is None:
             errors.append(f"Missing required field: {field}")
         elif isinstance(value, str) and value.strip() == "":
             errors.append(f"Missing required field: {field}")
     return errors
 
 
 def validate_email_format(email: str) -> bool:
     """Valida formato de email (regex prÃ©-compilado e fullmatch)."""
     if not email:
         return False
     return EMAIL_RE.fullmatch(email) is not None
 
 
 def validate_phone_format(phone: str) -> bool:
     """Valida telefone (compatÃ­vel internacional)."""
     if not phone:
         return False
     return PHONE_RE.fullmatch(phone) is not None
 
 
 def validate_text_length(text: str, min_len: int, max_len: int, field_name: str) -> List[str]:
     """Validate text length constraints."""
     errors: List[str] = []
     text = text or ""
     if len(text) < min_len:
         errors.append(f"{field_name} must be at least {min_len} characters")
     if len(text) > max_len:
         errors.append(f"{field_name} must be at most {max_len} characters")
     return errors
 
 
-# validate_business_rules_client removed - client functionality eliminated
 
 
 def validate_business_rules_project(data: Dict[str, Any]) -> List[str]:
     """Validate business rules specific to project entities."""
     errors: List[str] = []
     errors.extend(validate_text_length(data.get("project_key", ""), 2, 50, "project_key"))
     errors.extend(validate_text_length(data.get("name", ""), 1, 255, "name"))
     return errors
 
 
 def sanitize_form_inputs(data: Dict[str, Any]) -> Dict[str, Any]:
     """Sanitiza todas as entradas string; preserva None e tipos primitivos.
 
     ObservaÃ§Ã£o: `validate_form` (se existir) roda apÃ³s sanitizaÃ§Ã£o.
     """
     sanitized: Dict[str, Any] = {}
     for key, value in data.items():
         if isinstance(value, str):
             sanitized[key] = sanitize_input(value, key)
         else:
             sanitized[key] = value
 
     if validate_form:
         validate_form(sanitized)  # type: ignore[misc]
     return sanitized
\ No newline at end of file
diff --git a/streamlit_extension/utils/performance_tester.py b/streamlit_extension/utils/performance_tester.py
index 016e50a6f73f959c7a23042b14f5859593c372fd..2a815854668547204b6ac45066db31a64ce99e68 100644
--- a/streamlit_extension/utils/performance_tester.py
+++ b/streamlit_extension/utils/performance_tester.py
@@ -285,52 +285,51 @@ class LoadTester:
         while time.time() - start_time < duration and not self.stop_event.is_set():
             try:
                 # Select random test data
                 data = test_data[operation_count % len(test_data)]
                 
                 with self.profiler.profile_operation(f"load_test_{worker_id}"):
                     target_function(data)
                 
                 operation_count += 1
                 
                 # Small delay to control rate
                 time.sleep(0.01)
                 
             except Exception as e:
                 logging.info(f"Operation failed in {worker_id}: {e}")
                 
         logging.info(f"{worker_id} completed {operation_count} operations")
     
     def _generate_test_data(self, size: int) -> List[Dict]:
         """Generate test data for load testing."""
         return [
             {
                 "project_key": f"load_test_project_{i}",
                 "name": f"Load Test Project {i}",
                 "description": f"Load test project {i} for performance testing",
-                "industry": "Technology",
-                "client_tier": "basic"
+                "industry": "Technology"
             }
             for i in range(size)
         ]
     
     def _generate_load_test_report(self, config: LoadTestConfig, 
                                  start_time: float, end_time: float) -> Dict[str, Any]:
         """Generate comprehensive load test report."""
         duration = end_time - start_time
         stats = self.profiler.get_statistics()
         
         return {
             "config": {
                 "concurrent_users": config.concurrent_users,
                 "duration_seconds": config.duration_seconds,
                 "target_operations_per_second": config.operations_per_second
             },
             "execution": {
                 "actual_duration": duration,
                 "total_operations": stats.get("total_operations", 0),
                 "operations_per_second": stats.get("throughput", 0),
                 "success_rate": stats.get("success_rate", 0)
             },
             "performance": stats,
             "bottlenecks": self._identify_bottlenecks(stats)
         }
@@ -517,70 +516,66 @@ class PerformanceReporter:
 def create_performance_test_suite(db_manager) -> Dict[str, Any]:
     """Create comprehensive performance test suite."""
     
     # Initialize components
     db_tester = DatabasePerformanceTester(db_manager)
     load_tester = LoadTester(db_manager)
     monitor = PerformanceMonitor()
     reporter = PerformanceReporter()
     
     # Test configurations
     light_load = LoadTestConfig(concurrent_users=5, duration_seconds=30)
     heavy_load = LoadTestConfig(concurrent_users=20, duration_seconds=60)
     
     results = {}
     
     logging.info("ðŸš€ Starting Performance Test Suite...")
     
     # 1. Database performance tests
     logging.info("ðŸ“Š Running database performance tests...")
     results["database_crud"] = db_tester.benchmark_crud_operations(iterations=100)
     results["database_queries"] = db_tester.test_query_performance()
     
     # 2. Load testing
     logging.info("âš¡ Running load tests...")
     
-    def client_creation_test(data):
-        return db_manager.create_client(**data)
-    
-    results["load_test_light"] = load_tester.run_load_test(light_load, client_creation_test)
-    results["load_test_heavy"] = load_tester.run_load_test(heavy_load, client_creation_test)
+    def project_creation_test(data):
+        return db_manager.create_project(**data)
+
+    results["load_test_light"] = load_tester.run_load_test(light_load, project_creation_test)
+    results["load_test_heavy"] = load_tester.run_load_test(heavy_load, project_creation_test)
     
     # 3. Generate report
     logging.info("ðŸ“‹ Generating performance report...")
     report_file = reporter.generate_performance_report(results, "comprehensive_test")
     
     logging.info(f"âœ… Performance testing complete! Report saved to: {report_file}")
     
     return results
 
 
 # Example usage and integration functions
 def run_quick_performance_check(db_manager) -> Dict[str, Any]:
     """Run quick performance check for monitoring."""
     profiler = PerformanceProfiler()
     
     # Test basic operations
-    with profiler.profile_operation("get_clients"):
-        db_manager.get_clients(limit=10)
-    
     with profiler.profile_operation("get_projects"):
         db_manager.get_projects(limit=10)
-    
+
     return {
-        "get_clients": profiler.get_statistics("get_clients"),
         "get_projects": profiler.get_statistics("get_projects"),
         "timestamp": datetime.datetime.now().isoformat()
     }
 
 
 if __name__ == "__main__":
     # Example usage
     from streamlit_extension.utils.database import DatabaseManager
     
     db_manager = DatabaseManager("framework.db", "task_timer.db")
     
     # Run comprehensive test suite
     results = create_performance_test_suite(db_manager)
     
     logging.info("Performance testing completed!")
     logging.info(f"Results: {len(results)} test categories executed")
diff --git a/streamlit_extension/utils/query_builder.py b/streamlit_extension/utils/query_builder.py
index fae06b0c4f9a77fae48aa936c358574baf65e7f9..d308ac3bb69d1466aae444477b59d9b52d175e7c 100644
--- a/streamlit_extension/utils/query_builder.py
+++ b/streamlit_extension/utils/query_builder.py
@@ -212,60 +212,58 @@ class QueryBuilder:
     def _build_update(self) -> Tuple[str, Tuple[Any, ...]]:
         """Build UPDATE query."""
         if not self.values:
             raise ValueError("No values provided for UPDATE")
         set_clause = ", ".join(f"{col} = ?" for col in self.values.keys())
         query = f"UPDATE {self.table} SET {set_clause}"
         params: List[Any] = list(self.values.values())
 
         if self.conditions:
             query += " WHERE " + " AND ".join(self.conditions)
             params.extend(self.parameters)
 
         return query, tuple(params)
 
     def _build_delete(self) -> Tuple[str, Tuple[Any, ...]]:
         """Build DELETE query."""
         query = f"DELETE FROM {self.table}"
         if self.conditions:
             query += " WHERE " + " AND ".join(self.conditions)
         return query, tuple(self.parameters)
 
 
 # ----------------------------------------------------------------------
 # Specialized builders
 # ----------------------------------------------------------------------
-# Client query builder removed - client functionality eliminated
 
 
 class ProjectQueryBuilder(QueryBuilder):
     """Specialized query builder for projects."""
 
     def __init__(self) -> None:
         super().__init__("framework_projects")
 
-    # Client-related methods removed - client functionality eliminated
 
     def active_only(self) -> "ProjectQueryBuilder":
         """Filter only active projects."""
         return self.where("status = ?", "active")
 
     def with_epic_stats(self) -> "ProjectQueryBuilder":
         """Include epic statistics."""
         return (
             self.left_join(
                 "framework_epics e", "framework_projects.id = e.project_id"
             )
             .select(
                 "framework_projects.*",
                 "COUNT(e.id) as epic_count",
                 "SUM(CASE WHEN e.status = 'completed' THEN 1 ELSE 0 END) as completed_epics",
             )
             .group_by("framework_projects.id")
         )
 
 
 class EpicQueryBuilder(QueryBuilder):
     """Specialized query builder for epics."""
 
     def __init__(self) -> None:
         super().__init__("framework_epics")
diff --git a/streamlit_extension/utils/redis_cache.py b/streamlit_extension/utils/redis_cache.py
index fc186e9b04a9e330a0f86a17112e246e2248e6db..37677f552485cbff40f353ffb51591acfc42d0cb 100644
--- a/streamlit_extension/utils/redis_cache.py
+++ b/streamlit_extension/utils/redis_cache.py
@@ -47,58 +47,57 @@ except ImportError:
     LOG_SANITIZATION_AVAILABLE = False
 
     def create_secure_logger(name: str) -> logging.Logger:
         logger = logging.getLogger(name)
         if not logger.handlers:
             handler = logging.StreamHandler()
             logger.addHandler(handler)
         logger.setLevel(logging.INFO)
         return logger
 
     def sanitize_log_message(msg: str) -> str:
         return msg
 
 try:
     from .security import sanitize_display
     SECURITY_AVAILABLE = True
 except ImportError:
     SECURITY_AVAILABLE = False
     sanitize_display = lambda x, **kwargs: str(x)
 
 
 class CacheStrategy:
     """Cache strategy definitions for different operation types."""
     
     # Query cache TTLs (in seconds)
-    QUICK_QUERIES = 300      # 5 minutes - client/project lists
-    MEDIUM_QUERIES = 900     # 15 minutes - epic/task data  
+    QUICK_QUERIES = 300      # 5 minutes - project lists
+    MEDIUM_QUERIES = 900     # 15 minutes - epic/task data
     HEAVY_QUERIES = 1800     # 30 minutes - analytics/aggregations
     STATIC_DATA = 3600       # 1 hour - settings/configs
     
     # Cache key prefixes
-    PREFIX_CLIENT = "client"
-    PREFIX_PROJECT = "project" 
+    PREFIX_PROJECT = "project"
     PREFIX_EPIC = "epic"
     PREFIX_TASK = "task"
     PREFIX_ANALYTICS = "analytics"
     PREFIX_AGGREGATION = "agg"
     PREFIX_SETTINGS = "settings"
     
     @classmethod
     def get_ttl(cls, operation_type: str) -> int:
         """Get TTL for operation type."""
         ttl_mapping = {
             "quick": cls.QUICK_QUERIES,
             "medium": cls.MEDIUM_QUERIES, 
             "heavy": cls.HEAVY_QUERIES,
             "static": cls.STATIC_DATA
         }
         return ttl_mapping.get(operation_type, cls.MEDIUM_QUERIES)
 
 
 class CacheMetrics:
     """Cache performance metrics tracking."""
     
     def __init__(self):
         self.stats = {
             "hits": 0,
             "misses": 0,
@@ -350,51 +349,51 @@ class RedisCacheManager:
             return self.is_available
         
         self._last_health_check = current_time
         
         if not self.client:
             return False
         
         try:
             self.client.ping()
             if not self.is_available:
                 self.logger.info("Redis connection restored")
                 self.is_available = True
             return True
             
         except Exception as e:
             if self.is_available:
                 self.logger.warning(f"Redis health check failed: {e}")
                 self.is_available = False
             return False
     
     def _generate_cache_key(self, prefix: str, *args, **kwargs) -> str:
         """
         Generate secure cache key with SHA-256 hashing.
         
         Args:
-            prefix: Cache key prefix (e.g., 'client', 'project')
+            prefix: Cache key prefix (e.g., 'project')
             *args: Positional arguments for key generation
             **kwargs: Keyword arguments for key generation
             
         Returns:
             Secure cache key string
         """
         # Create base key from arguments
         key_parts = [str(prefix)]
         
         # Add positional arguments
         for arg in args:
             if isinstance(arg, (str, int, float, bool)):
                 key_parts.append(str(arg))
             elif arg is None:
                 key_parts.append("none")
             else:
                 # Hash complex objects
                 key_parts.append(hashlib.sha256(str(arg).encode()).hexdigest()[:8])
         
         # Add keyword arguments (sorted for consistency)
         for key, value in sorted(kwargs.items()):
             if isinstance(value, (str, int, float, bool)):
                 key_parts.append(f"{key}:{value}")
             elif value is None:
                 key_parts.append(f"{key}:none")
@@ -507,51 +506,51 @@ class RedisCacheManager:
             key: Cache key to delete
             
         Returns:
             True if successful, False otherwise
         """
         if not self._check_health():
             return False
         
         try:
             with self._measure_time():
                 result = self.client.delete(key)
             
             self.logger.debug(f"Cache delete for key: {key[:20]}...")
             return bool(result)
             
         except Exception as e:
             self.metrics.record_error()
             self.logger.error(f"Cache delete error for key {key[:20]}...: {e}")
             return False
     
     def delete_pattern(self, pattern: str) -> int:
         """
         Delete keys matching pattern.
         
         Args:
-            pattern: Key pattern (e.g., 'client:*')
+            pattern: Key pattern (e.g., 'project:*')
             
         Returns:
             Number of keys deleted
         """
         if not self._check_health():
             return 0
         
         try:
             keys = self.client.keys(pattern)
             if not keys:
                 return 0
             
             with self._measure_time():
                 result = self.client.delete(*keys)
             
             self.logger.info(f"Deleted {result} keys matching pattern: {pattern}")
             return result
             
         except Exception as e:
             self.metrics.record_error()
             self.logger.error(f"Cache pattern delete error for {pattern}: {e}")
             return 0
     
     def flush_all(self) -> bool:
         """
@@ -616,53 +615,53 @@ def get_cache_manager(**kwargs) -> RedisCacheManager:
         **kwargs: Configuration parameters for first initialization
         
     Returns:
         RedisCacheManager instance
     """
     global _cache_manager
     
     if _cache_manager is None:
         with _cache_lock:
             if _cache_manager is None:
                 _cache_manager = RedisCacheManager(**kwargs)
     
     return _cache_manager
 
 
 def cached(prefix: str, ttl: int = 900, operation_type: str = "medium"):
     """
     Decorator for caching function results.
     
     Args:
         prefix: Cache key prefix
         ttl: Time to live (default uses operation_type mapping)
         operation_type: Operation type for TTL mapping
         
     Usage:
-        @cached("client", operation_type="quick")
-        def get_client_data(client_id):
-            return expensive_database_operation(client_id)
+        @cached("project", operation_type="quick")
+        def get_project_data(project_id):
+            return expensive_database_operation(project_id)
     """
     def decorator(func: Callable) -> Callable:
         @wraps(func)
         def wrapper(*args, **kwargs):
             cache = get_cache_manager()
             
             # Generate cache key
             cache_key = cache._generate_cache_key(prefix, func.__name__, *args, **kwargs)
             
             # Try to get from cache
             cached_result = cache.get(cache_key)
             if cached_result is not None:
                 return cached_result
             
             # Execute function
             result = func(*args, **kwargs)
             
             # Cache result
             actual_ttl = ttl if ttl != 900 else CacheStrategy.get_ttl(operation_type)
             cache.set(cache_key, result, actual_ttl)
             
             return result
         
         return wrapper
     return decorator
diff --git a/streamlit_extension/utils/security.py b/streamlit_extension/utils/security.py
index b646b2bf642e94898b4119322930f7b45981f457..c6cfaa715e3e26756dad20cc5d171485442dd38b 100644
--- a/streamlit_extension/utils/security.py
+++ b/streamlit_extension/utils/security.py
@@ -319,51 +319,50 @@ class StreamlitSecurityManager:
             return self._basic_html_escape(text)
     
     def _basic_html_escape(self, text: str) -> str:
         """
         Basic HTML escaping as fallback.
         
         Args:
             text: Text to escape
             
         Returns:
             HTML-escaped text
         """
         if not isinstance(text, str):
             return str(text)
         
         # Basic HTML escaping
         text = text.replace('&', '&amp;')
         text = text.replace('<', '&lt;')
         text = text.replace('>', '&gt;')
         text = text.replace('"', '&quot;')
         text = text.replace("'", '&#x27;')
         text = text.replace('\x00', '')  # Remove null bytes
         
         return text
     
-    # create_safe_client_data removed - client functionality eliminated
     
     def create_safe_project_data(self, form_data: Dict[str, Any]) -> Dict[str, Any]:
         """
         Create safely sanitized project data from form input.
         
         Args:
             form_data: Raw form data from Streamlit
             
         Returns:
             Sanitized project data safe for database storage
         """
         safe_data = {}
         
         # Fields that need sanitization
         text_fields = [
             'project_key', 'name', 'description', 'project_type', 'methodology'
         ]
         
         for key, value in form_data.items():
             if key in text_fields and isinstance(value, str):
                 safe_data[key] = self.sanitize_form_input(value, key)
             else:
                 safe_data[key] = value
         
         return safe_data
@@ -784,51 +783,50 @@ class StreamlitSecurityManager:
             "field_name": "csrf_token",
             "token_value": token
         }
 
 
 # Global security manager instance
 security_manager = StreamlitSecurityManager()
 
 
 # Convenience functions for easy import
 def sanitize_input(value: str, field_name: str = "input") -> str:
     """Sanitize user input from forms."""
     return security_manager.sanitize_form_input(value, field_name)
 
 
 def sanitize_display(text: str, max_length: int = 1000) -> str:
     """Sanitize text for display."""
     return security_manager.sanitize_display_text(text, max_length)
 
 
 def validate_form(data: Dict[str, Any]) -> Tuple[bool, List[str]]:
     """Validate form data for security threats."""
     return security_manager.validate_form_data(data)
 
 
-# create_safe_client removed - client functionality eliminated
 
 
 def create_safe_project(form_data: Dict[str, Any]) -> Dict[str, Any]:
     """Create safe project data."""
     return security_manager.create_safe_project_data(form_data)
 
 
 def check_rate_limit(operation_type: str, 
                     user_id: Optional[str] = None,
                     ip_address: Optional[str] = None) -> Tuple[bool, Optional[str]]:
     """Check if operation is within rate limits."""
     return security_manager.check_rate_limit(operation_type, user_id, ip_address)
 
 
 def get_rate_limit_stats() -> Dict[str, Any]:
     """Get rate limiting statistics."""
     return security_manager.get_rate_limit_stats()
 
 
 def reset_rate_limits(operation_type: str,
                      user_id: Optional[str] = None,
                      ip_address: Optional[str] = None):
     """Reset rate limits for specific user/operation."""
     return security_manager.reset_rate_limits(operation_type, user_id, ip_address)
 
diff --git a/streamlit_extension/utils/structured_logger.py b/streamlit_extension/utils/structured_logger.py
index 207c9a25dcb013c385ecf663a70eda613b902feb..b095365acb3791aef98a857574dd584a7f894c94 100644
--- a/streamlit_extension/utils/structured_logger.py
+++ b/streamlit_extension/utils/structured_logger.py
@@ -733,47 +733,47 @@ def log_performance_metrics(memory_mb: float, cpu_percent: float,
         logger.database_connections.set(active_connections)
         logger.cache_hit_ratio.set(cache_hit_ratio)
     
     logger.performance_event(
         component="system",
         operation="metrics_collection",
         message="System performance metrics collected",
         operation_duration_ms=0,  # Instantaneous measurement
         memory_usage_mb=memory_mb,
         cpu_usage_percent=cpu_percent,
         extra_data={
             "active_connections": active_connections,
             "cache_hit_ratio": cache_hit_ratio
         }
     )
 
 
 if __name__ == "__main__":
     # Example usage
     logger = setup_logging("demo_logs", metrics_port=8000)
     
     # Application logging
     logger.info("application", "startup", "TDD Framework starting up")
     
     # Performance logging with timer
-    with logger.performance_timer("database", "client_query"):
+    with logger.performance_timer("database", "project_query"):
         time.sleep(0.1)  # Simulate work
     
     # Security event
     logger.security_event(
         component="authentication",
         operation="login_attempt",
         message="Failed login attempt detected",
         event_category="authentication",
         severity="medium",
         threat_detected=True,
         source_ip="192.168.1.100"
     )
     
     # User action logging
     with log_user_session("user123", "session456", "192.168.1.50"):
-        logger.user_action("user123", "create_client", "client_form", success=True)
+        logger.user_action("user123", "create_project", "project_form", success=True)
     
     logging.info("Structured logging demo completed!")
     logging.info("Check demo_logs/ directory for log files")
     if PROMETHEUS_AVAILABLE:
         logging.info("Prometheus metrics available at http://localhost:8000")
diff --git a/streamlit_extension/utils/validators.py b/streamlit_extension/utils/validators.py
index 5a53073249cad1e243efd8f53768f9b9dc2ccf59..0ba2a4dfcba435cd1eee45677a5eacc0e06b11c1 100644
--- a/streamlit_extension/utils/validators.py
+++ b/streamlit_extension/utils/validators.py
@@ -1,34 +1,34 @@
 """
 ðŸ” Configuration and Data Validators
 
 Validation utilities for Streamlit extension with:
 - Configuration validation
 - Database schema validation
 - Input sanitization
 - Error reporting
-- Client and Project data validation
+- Project data validation
 """
 
 from __future__ import annotations
 
 from typing import Dict, Any, List, Tuple, Optional
 from pathlib import Path
 import re
 import json
 from datetime import datetime
 from email.utils import parseaddr
 
 VALID_METHODS = {"agile", "waterfall", "kanban", "scrum", "lean", "hybrid"}
 
 # Graceful imports
 try:
     from ..config import StreamlitConfig
 except ImportError:
     StreamlitConfig = None
 
 
 class ValidationError(Exception):
     """Custom validation error."""
     pass
 
 
@@ -334,66 +334,65 @@ def generate_validation_report(validations: List[Tuple[str, bool, List[str]]]) -
     Returns:
         Validation report dictionary
     """
     total_checks = len(validations)
     passed_checks = sum(1 for _, is_valid, _ in validations if is_valid)
     
     report = {
         "timestamp": "2025-08-12T10:00:00",  # Would use actual timestamp
         "total_checks": total_checks,
         "passed_checks": passed_checks,
         "success_rate": (passed_checks / total_checks * 100) if total_checks > 0 else 0,
         "status": "PASS" if passed_checks == total_checks else "FAIL",
         "details": []
     }
     
     for name, is_valid, errors in validations:
         report["details"].append({
             "check": name,
             "status": "PASS" if is_valid else "FAIL",
             "errors": errors
         })
     
     return report
 
 
-# validate_client_data completely removed - client functionality eliminated
 
 
 def validate_project_data(project: Dict[str, Any]) -> Tuple[bool, List[str]]:
     """
     ðŸ“ Validate project data structure and business rules.
     
     Args:
         project: Project dictionary to validate
     
     Returns:
         Tuple of (is_valid, error_messages)
     """
     errors = []
     
-    # Required fields (client_id removed - direct project management)
+    # Required fields
     required_fields = ["project_key", "name", "status", "planned_start_date", "planned_end_date"]
     for field in required_fields:
         if field not in project or project[field] is None:
             errors.append(f"Missing required field: {field}")
     
     # Validate project_key format
     project_key = project.get("project_key", "")
     if project_key:
         if not re.match(r'^[a-z0-9_]{2,50}$', project_key):
             errors.append("Project key must be lowercase, contain only letters, numbers, underscores (2-50 chars)")
     
     # Validate name length
     name = project.get("name", "")
     if len(name) > 255:
         errors.append(f"Project name too long ({len(name)} characters, max 255)")
     if len(name.strip()) == 0:
         errors.append("Project name cannot be empty")
     
     # Validate status
     valid_statuses = ["planning", "in_progress", "completed", "on_hold", "cancelled"]
     status = project.get("status", "").lower()
     if status and status not in valid_statuses:
         errors.append(f"Invalid status '{status}'. Valid options: {', '.join(valid_statuses)}")
     
     # Validate project type
@@ -467,51 +466,50 @@ def validate_project_data(project: Dict[str, Any]) -> Tuple[bool, List[str]]:
     # Validate estimated hours
     estimated_hours = project.get("estimated_hours")
     if estimated_hours is not None:
         try:
             hours_float = float(estimated_hours)
             if hours_float < 0:
                 errors.append("Estimated hours cannot be negative")
             if hours_float > 10000:  # 10k hours limit
                 errors.append(f"Estimated hours {hours_float} seems unreasonably high")
         except (ValueError, TypeError):
             errors.append(f"Invalid estimated hours value: {estimated_hours}")
     
     # Validate completion percentage
     completion = project.get("completion_percentage")
     if completion is not None:
         try:
             completion_float = float(completion)
             if completion_float < 0 or completion_float > 100:
                 errors.append(f"Completion percentage {completion_float} outside valid range (0-100)")
         except (ValueError, TypeError):
             errors.append(f"Invalid completion percentage value: {completion}")
     
     return len(errors) == 0, errors
 
 
-# Client validation functions removed - client functionality eliminated
 
 
 def validate_project_key_uniqueness(project_key: str, existing_projects: List[Dict[str, Any]], exclude_project_id: Optional[int] = None) -> bool:
     """
     ðŸ”‘ Check if project key is unique across all projects.
     
     Args:
         project_key: Project key to check
         existing_projects: List of existing project dictionaries
         exclude_project_id: Project ID to exclude from check (for updates)
     
     Returns:
         True if project key is unique, False otherwise
     """
     if not project_key:
         return True
     
     for project in existing_projects:
         if exclude_project_id and project.get("id") == exclude_project_id:
             continue
 
         if project.get("project_key", "").lower() == project_key.lower():
             return False
 
     return True
diff --git a/tests/integration/test_e2e_workflows.py b/tests/integration/test_e2e_workflows.py
deleted file mode 100644
index 7db9184f21ce19dd4f04cc72f0ee9868a0581ff9..0000000000000000000000000000000000000000
--- a/tests/integration/test_e2e_workflows.py
+++ /dev/null
@@ -1,769 +0,0 @@
-"""
-ðŸ§ª End-to-End Integration Tests for TDD Framework
-
-Comprehensive integration tests that validate complete user workflows
-across all system components:
-
-- Client-Project-Epic-Task hierarchy
-- Authentication + Authorization flows  
-- Security systems (CSRF, XSS, Rate Limiting)
-- Exception handling + Logging correlation
-- Performance under realistic load
-- Data integrity across operations
-
-These tests ensure all systems work together correctly in production scenarios.
-"""
-
-import pytest
-import time
-import threading
-import sqlite3
-import tempfile
-from pathlib import Path
-from datetime import datetime, timedelta
-from concurrent.futures import ThreadPoolExecutor, as_completed
-from unittest.mock import Mock, patch, MagicMock
-import uuid
-import json
-
-# Core test imports
-import sys
-sys.path.append(str(Path(__file__).parent.parent.parent))
-
-# Test framework components
-try:
-    from streamlit_extension.utils.database import DatabaseManager
-    from streamlit_extension.services import (
-        ProjectService, EpicService, TaskService, 
-        AnalyticsService, ServiceContainer
-    )
-    SERVICES_AVAILABLE = True
-except ImportError:
-    SERVICES_AVAILABLE = False
-
-try:
-    from duration_system.database_transactions import TransactionalDatabaseManager
-    TRANSACTIONS_AVAILABLE = True
-except ImportError:
-    TRANSACTIONS_AVAILABLE = False
-
-# Skip all tests if required components not available
-if not SERVICES_AVAILABLE:
-    pytest.skip("Service layer not available", allow_module_level=True)
-
-
-class IntegrationTestFramework:
-    """Framework for running complex integration tests."""
-    
-    def __init__(self):
-        self.temp_db = None
-        self.db_path = None
-        self.db_manager = None
-        self.service_container = None
-        self.test_data = {}
-        self.operation_log = []
-        
-    def setup(self):
-        """Setup test environment with temporary database."""
-        # Create temporary database
-        self.temp_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
-        self.db_path = self.temp_db.name
-        self.temp_db.close()
-        
-        # Initialize database with schema
-        self._initialize_test_database()
-        
-        # Setup service container
-        self.db_manager = DatabaseManager(framework_db_path=self.db_path)
-        self.service_container = ServiceContainer()
-        self.service_container.register("database_manager", self.db_manager)
-        
-        # Register services
-        self._register_services()
-        
-        # Initialize test data tracking
-        self.test_data = {
-            "projects": [],
-            "epics": [],
-            "tasks": [],
-            "users": []
-        }
-        self.operation_log = []
-        
-    def teardown(self):
-        """Cleanup test environment."""
-        if self.db_manager:
-            self.db_manager.close()
-        
-        if self.db_path and Path(self.db_path).exists():
-            try:
-                Path(self.db_path).unlink()
-            except Exception:
-                pass  # Ignore cleanup errors
-    
-    def _initialize_test_database(self):
-        """Initialize test database with required schema."""
-        with sqlite3.connect(self.db_path) as conn:
-            # Create minimal schema for testing
-            conn.execute("""
-                CREATE TABLE IF NOT EXISTS framework_clients (
-                    id INTEGER PRIMARY KEY,
-                    name TEXT NOT NULL,
-                    email TEXT UNIQUE,
-                    company TEXT,
-                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
-                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
-                )
-            """)
-            
-            conn.execute("""
-                CREATE TABLE IF NOT EXISTS framework_projects (
-                    id INTEGER PRIMARY KEY,
-                    name TEXT NOT NULL,
-                    description TEXT,
-                    client_id INTEGER NOT NULL,
-                    status TEXT DEFAULT 'active',
-                    budget DECIMAL(10,2),
-                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
-                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
-                    FOREIGN KEY (client_id) REFERENCES framework_clients(id)
-                )
-            """)
-            
-            conn.execute("""
-                CREATE TABLE IF NOT EXISTS framework_epics (
-                    id INTEGER PRIMARY KEY,
-                    epic_key TEXT UNIQUE,
-                    name TEXT NOT NULL,
-                    description TEXT,
-                    project_id INTEGER,
-                    status TEXT DEFAULT 'planning',
-                    priority INTEGER DEFAULT 3,
-                    points INTEGER DEFAULT 0,
-                    progress_percentage REAL DEFAULT 0.0,
-                    duration_description TEXT,
-                    calculated_duration_days REAL,
-                    planned_start_date TEXT,
-                    planned_end_date TEXT,
-                    actual_start_date TEXT,
-                    actual_end_date TEXT,
-                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
-                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
-                    FOREIGN KEY (project_id) REFERENCES framework_projects(id)
-                )
-            """)
-            
-            conn.execute("""
-                CREATE TABLE IF NOT EXISTS framework_tasks (
-                    id INTEGER PRIMARY KEY,
-                    task_key TEXT UNIQUE,
-                    name TEXT NOT NULL,
-                    description TEXT,
-                    epic_id INTEGER,
-                    status TEXT DEFAULT 'todo',
-                    tdd_phase TEXT DEFAULT 'red',
-                    priority INTEGER DEFAULT 3,
-                    estimated_hours REAL,
-                    actual_hours REAL DEFAULT 0.0,
-                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
-                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
-                    FOREIGN KEY (epic_id) REFERENCES framework_epics(id)
-                )
-            """)
-            
-            conn.commit()
-    
-    def _register_services(self):
-        """Register all services in the container."""
-        # Mock services if not available
-        if SERVICES_AVAILABLE:
-            try:
-                client_service = ClientService(self.db_manager)
-                project_service = ProjectService(self.db_manager)
-                epic_service = EpicService(self.db_manager)
-                task_service = TaskService(self.db_manager)
-                analytics_service = AnalyticsService(self.db_manager)
-                
-                self.service_container.register("client_service", client_service)
-                self.service_container.register("project_service", project_service)
-                self.service_container.register("epic_service", epic_service)
-                self.service_container.register("task_service", task_service)
-                self.service_container.register("analytics_service", analytics_service)
-            except Exception:
-                # Fall back to mock services
-                self._register_mock_services()
-        else:
-            self._register_mock_services()
-    
-    def _register_mock_services(self):
-        """Register mock services for testing."""
-        mock_client_service = Mock()
-        mock_project_service = Mock()
-        mock_epic_service = Mock()
-        mock_task_service = Mock()
-        mock_analytics_service = Mock()
-        
-        self.service_container.register("client_service", mock_client_service)
-        self.service_container.register("project_service", mock_project_service)
-        self.service_container.register("epic_service", mock_epic_service)
-        self.service_container.register("task_service", mock_task_service)
-        self.service_container.register("analytics_service", mock_analytics_service)
-    
-    def log_operation(self, operation: str, details: dict = None):
-        """Log test operation for debugging."""
-        self.operation_log.append({
-            "timestamp": datetime.now().isoformat(),
-            "operation": operation,
-            "details": details or {}
-        })
-    
-    def create_test_client(self, name: str = None, email: str = None) -> dict:
-        """Create test client."""
-        client_data = {
-            "name": name or f"Test Client {len(self.test_data['clients']) + 1}",
-            "email": email or f"client{len(self.test_data['clients']) + 1}@test.com",
-            "company": "Test Company"
-        }
-        
-        # Create via database directly for integration testing
-        with sqlite3.connect(self.db_path) as conn:
-            cursor = conn.execute("""
-                INSERT INTO framework_clients (name, email, company)
-                VALUES (?, ?, ?)
-            """, (client_data["name"], client_data["email"], client_data["company"]))
-            
-            client_id = cursor.lastrowid
-            client_data["id"] = client_id
-            
-            conn.commit()
-        
-        self.test_data["clients"].append(client_data)
-        self.log_operation("create_client", client_data)
-        
-        return client_data
-    
-    def create_test_project(self, client_id: int, name: str = None) -> dict:
-        """Create test project."""
-        project_data = {
-            "name": name or f"Test Project {len(self.test_data['projects']) + 1}",
-            "description": "Integration test project",
-            "client_id": client_id,
-            "status": "active",
-            "budget": 10000.00
-        }
-        
-        with sqlite3.connect(self.db_path) as conn:
-            cursor = conn.execute("""
-                INSERT INTO framework_projects (name, description, client_id, status, budget)
-                VALUES (?, ?, ?, ?, ?)
-            """, (project_data["name"], project_data["description"], 
-                  project_data["client_id"], project_data["status"], project_data["budget"]))
-            
-            project_id = cursor.lastrowid
-            project_data["id"] = project_id
-            
-            conn.commit()
-        
-        self.test_data["projects"].append(project_data)
-        self.log_operation("create_project", project_data)
-        
-        return project_data
-    
-    def create_test_epic(self, project_id: int, name: str = None) -> dict:
-        """Create test epic."""
-        epic_data = {
-            "epic_key": f"EPIC-{len(self.test_data['epics']) + 1}",
-            "name": name or f"Test Epic {len(self.test_data['epics']) + 1}",
-            "description": "Integration test epic",
-            "project_id": project_id,
-            "status": "planning",
-            "priority": 3,
-            "points": 8,
-            "progress_percentage": 0.0
-        }
-        
-        with sqlite3.connect(self.db_path) as conn:
-            cursor = conn.execute("""
-                INSERT INTO framework_epics (epic_key, name, description, project_id, 
-                                           status, priority, points, progress_percentage)
-                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
-            """, (epic_data["epic_key"], epic_data["name"], epic_data["description"],
-                  epic_data["project_id"], epic_data["status"], epic_data["priority"],
-                  epic_data["points"], epic_data["progress_percentage"]))
-            
-            epic_id = cursor.lastrowid
-            epic_data["id"] = epic_id
-            
-            conn.commit()
-        
-        self.test_data["epics"].append(epic_data)
-        self.log_operation("create_epic", epic_data)
-        
-        return epic_data
-    
-    def create_test_task(self, epic_id: int, name: str = None) -> dict:
-        """Create test task."""
-        task_data = {
-            "task_key": f"TASK-{len(self.test_data['tasks']) + 1}",
-            "name": name or f"Test Task {len(self.test_data['tasks']) + 1}",
-            "description": "Integration test task",
-            "epic_id": epic_id,
-            "status": "todo",
-            "tdd_phase": "red",
-            "priority": 3,
-            "estimated_hours": 4.0,
-            "actual_hours": 0.0
-        }
-        
-        with sqlite3.connect(self.db_path) as conn:
-            cursor = conn.execute("""
-                INSERT INTO framework_tasks (task_key, name, description, epic_id,
-                                           status, tdd_phase, priority, estimated_hours, actual_hours)
-                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
-            """, (task_data["task_key"], task_data["name"], task_data["description"],
-                  task_data["epic_id"], task_data["status"], task_data["tdd_phase"],
-                  task_data["priority"], task_data["estimated_hours"], task_data["actual_hours"]))
-            
-            task_id = cursor.lastrowid
-            task_data["id"] = task_id
-            
-            conn.commit()
-        
-        self.test_data["tasks"].append(task_data)
-        self.log_operation("create_task", task_data)
-        
-        return task_data
-    
-    def simulate_tdd_cycle(self, task_id: int) -> list:
-        """Simulate complete TDD cycle for a task."""
-        phases = []
-        
-        # Red phase
-        with sqlite3.connect(self.db_path) as conn:
-            conn.execute("""
-                UPDATE framework_tasks 
-                SET tdd_phase = 'red', status = 'in_progress', updated_at = ?
-                WHERE id = ?
-            """, (datetime.now(), task_id))
-            conn.commit()
-        
-        phases.append("red")
-        self.log_operation("tdd_phase_red", {"task_id": task_id})
-        
-        # Green phase
-        with sqlite3.connect(self.db_path) as conn:
-            conn.execute("""
-                UPDATE framework_tasks 
-                SET tdd_phase = 'green', updated_at = ?
-                WHERE id = ?
-            """, (datetime.now(), task_id))
-            conn.commit()
-        
-        phases.append("green")
-        self.log_operation("tdd_phase_green", {"task_id": task_id})
-        
-        # Refactor phase
-        with sqlite3.connect(self.db_path) as conn:
-            conn.execute("""
-                UPDATE framework_tasks 
-                SET tdd_phase = 'refactor', status = 'done', actual_hours = 3.5, updated_at = ?
-                WHERE id = ?
-            """, (datetime.now(), task_id))
-            conn.commit()
-        
-        phases.append("refactor")
-        self.log_operation("tdd_phase_refactor", {"task_id": task_id})
-        
-        return phases
-    
-    def validate_data_integrity(self) -> dict:
-        """Validate data integrity across all entities."""
-        integrity_issues = []
-        
-        with sqlite3.connect(self.db_path) as conn:
-            # Check client-project relationships
-            cursor = conn.execute("""
-                SELECT p.id, p.name, p.client_id
-                FROM framework_projects p
-                LEFT JOIN framework_clients c ON p.client_id = c.id
-                WHERE c.id IS NULL
-            """)
-            orphaned_projects = cursor.fetchall()
-            if orphaned_projects:
-                integrity_issues.append(f"Orphaned projects: {orphaned_projects}")
-            
-            # Check project-epic relationships
-            cursor = conn.execute("""
-                SELECT e.id, e.epic_key, e.project_id
-                FROM framework_epics e
-                LEFT JOIN framework_projects p ON e.project_id = p.id
-                WHERE p.id IS NULL
-            """)
-            orphaned_epics = cursor.fetchall()
-            if orphaned_epics:
-                integrity_issues.append(f"Orphaned epics: {orphaned_epics}")
-            
-            # Check epic-task relationships
-            cursor = conn.execute("""
-                SELECT t.id, t.task_key, t.epic_id
-                FROM framework_tasks t
-                LEFT JOIN framework_epics e ON t.epic_id = e.id
-                WHERE e.id IS NULL
-            """)
-            orphaned_tasks = cursor.fetchall()
-            if orphaned_tasks:
-                integrity_issues.append(f"Orphaned tasks: {orphaned_tasks}")
-        
-        return {
-            "valid": len(integrity_issues) == 0,
-            "issues": integrity_issues
-        }
-    
-    def get_performance_metrics(self) -> dict:
-        """Get performance metrics for the test run."""
-        return {
-            "total_operations": len(self.operation_log),
-            "clients_created": len(self.test_data["clients"]),
-            "projects_created": len(self.test_data["projects"]),
-            "epics_created": len(self.test_data["epics"]),
-            "tasks_created": len(self.test_data["tasks"]),
-            "test_duration": (
-                datetime.fromisoformat(self.operation_log[-1]["timestamp"]) - 
-                datetime.fromisoformat(self.operation_log[0]["timestamp"])
-            ).total_seconds() if self.operation_log else 0
-        }
-
-
-@pytest.fixture
-def integration_framework():
-    """Provide integration test framework."""
-    framework = IntegrationTestFramework()
-    framework.setup()
-    yield framework
-    framework.teardown()
-
-
-class TestCompleteUserWorkflows:
-    """Test complete user workflows end-to-end."""
-    
-    def test_complete_client_project_epic_workflow(self, integration_framework):
-        """Test complete workflow from client creation to task completion."""
-        framework = integration_framework
-        
-        # 1. Create client
-        client = framework.create_test_client("Acme Corp", "contact@acme.com")
-        assert client["id"] is not None
-        assert client["name"] == "Acme Corp"
-        
-        # 2. Create project
-        project = framework.create_test_project(client["id"], "E-commerce Platform")
-        assert project["id"] is not None
-        assert project["client_id"] == client["id"]
-        
-        # 3. Create epic
-        epic = framework.create_test_epic(project["id"], "User Authentication System")
-        assert epic["id"] is not None
-        assert epic["project_id"] == project["id"]
-        
-        # 4. Create tasks
-        task1 = framework.create_test_task(epic["id"], "Login Form Implementation")
-        task2 = framework.create_test_task(epic["id"], "Password Reset Feature")
-        
-        assert task1["epic_id"] == epic["id"]
-        assert task2["epic_id"] == epic["id"]
-        
-        # 5. Complete TDD cycles
-        phases1 = framework.simulate_tdd_cycle(task1["id"])
-        phases2 = framework.simulate_tdd_cycle(task2["id"])
-        
-        assert phases1 == ["red", "green", "refactor"]
-        assert phases2 == ["red", "green", "refactor"]
-        
-        # 6. Validate data integrity
-        integrity = framework.validate_data_integrity()
-        assert integrity["valid"] is True, f"Data integrity issues: {integrity['issues']}"
-        
-        # 7. Check performance metrics
-        metrics = framework.get_performance_metrics()
-        assert metrics["clients_created"] == 1
-        assert metrics["projects_created"] == 1
-        assert metrics["epics_created"] == 1
-        assert metrics["tasks_created"] == 2
-        assert metrics["total_operations"] >= 8  # At least 8 operations logged
-    
-    def test_multi_user_concurrent_operations(self, integration_framework):
-        """Test concurrent operations by multiple users."""
-        framework = integration_framework
-        
-        def user_workflow(user_id: int):
-            """Simulate a user's workflow."""
-            try:
-                # Each user creates their own client and project
-                client = framework.create_test_client(f"Client {user_id}", f"user{user_id}@test.com")
-                project = framework.create_test_project(client["id"], f"Project {user_id}")
-                epic = framework.create_test_epic(project["id"], f"Epic {user_id}")
-                
-                # Create multiple tasks
-                tasks = []
-                for i in range(3):
-                    task = framework.create_test_task(epic["id"], f"Task {user_id}-{i}")
-                    tasks.append(task)
-                
-                # Complete some TDD cycles
-                for task in tasks[:2]:  # Complete first 2 tasks
-                    framework.simulate_tdd_cycle(task["id"])
-                
-                return {"user_id": user_id, "success": True, "tasks_completed": 2}
-            
-            except Exception as e:
-                return {"user_id": user_id, "success": False, "error": str(e)}
-        
-        # Run concurrent user workflows
-        with ThreadPoolExecutor(max_workers=5) as executor:
-            futures = [executor.submit(user_workflow, i) for i in range(1, 6)]
-            results = [future.result() for future in as_completed(futures)]
-        
-        # Verify all users succeeded
-        successful_users = [r for r in results if r["success"]]
-        failed_users = [r for r in results if not r["success"]]
-        
-        assert len(successful_users) == 5, f"Failed users: {failed_users}"
-        
-        # Verify data integrity after concurrent operations
-        integrity = framework.validate_data_integrity()
-        assert integrity["valid"] is True, f"Data integrity issues: {integrity['issues']}"
-        
-        # Check that we have expected number of entities
-        metrics = framework.get_performance_metrics()
-        assert metrics["clients_created"] == 5
-        assert metrics["projects_created"] == 5
-        assert metrics["epics_created"] == 5
-        assert metrics["tasks_created"] == 15  # 3 tasks per user
-    
-    def test_cascade_operations_and_recovery(self, integration_framework):
-        """Test cascade operations and data recovery scenarios."""
-        framework = integration_framework
-        
-        # Create test hierarchy
-        client = framework.create_test_client("Test Client", "test@client.com")
-        project = framework.create_test_project(client["id"], "Test Project")
-        epic = framework.create_test_epic(project["id"], "Test Epic")
-        task = framework.create_test_task(epic["id"], "Test Task")
-        
-        # Verify initial state
-        with sqlite3.connect(framework.db_path) as conn:
-            # Check all entities exist
-            client_count = conn.execute("SELECT COUNT(*) FROM framework_clients").fetchone()[0]
-            project_count = conn.execute("SELECT COUNT(*) FROM framework_projects").fetchone()[0]
-            epic_count = conn.execute("SELECT COUNT(*) FROM framework_epics").fetchone()[0]
-            task_count = conn.execute("SELECT COUNT(*) FROM framework_tasks").fetchone()[0]
-            
-            assert client_count == 1
-            assert project_count == 1
-            assert epic_count == 1
-            assert task_count == 1
-        
-        # Test soft delete (if implemented) or cascade behavior
-        with sqlite3.connect(framework.db_path) as conn:
-            # Delete the client
-            conn.execute("DELETE FROM framework_clients WHERE id = ?", (client["id"],))
-            conn.commit()
-        
-        # Check cascade behavior or constraints
-        with sqlite3.connect(framework.db_path) as conn:
-            remaining_projects = conn.execute(
-                "SELECT COUNT(*) FROM framework_projects WHERE client_id = ?", 
-                (client["id"],)
-            ).fetchone()[0]
-            
-            # Depending on implementation, projects might be deleted or orphaned
-            # For testing, we verify the system handles this gracefully
-            assert remaining_projects >= 0  # Should not cause errors
-        
-        framework.log_operation("cascade_delete_test", {
-            "deleted_client_id": client["id"],
-            "remaining_projects": remaining_projects
-        })
-    
-    def test_performance_under_load(self, integration_framework):
-        """Test system performance under moderate load."""
-        framework = integration_framework
-        start_time = time.time()
-        
-        # Create moderate load
-        num_clients = 10
-        num_projects_per_client = 2
-        num_epics_per_project = 3
-        num_tasks_per_epic = 2
-        
-        for i in range(num_clients):
-            client = framework.create_test_client(f"Load Client {i}", f"load{i}@test.com")
-            
-            for j in range(num_projects_per_client):
-                project = framework.create_test_project(client["id"], f"Load Project {i}-{j}")
-                
-                for k in range(num_epics_per_project):
-                    epic = framework.create_test_epic(project["id"], f"Load Epic {i}-{j}-{k}")
-                    
-                    for l in range(num_tasks_per_epic):
-                        task = framework.create_test_task(epic["id"], f"Load Task {i}-{j}-{k}-{l}")
-                        
-                        # Complete some tasks
-                        if (i + j + k + l) % 3 == 0:  # Complete every 3rd task
-                            framework.simulate_tdd_cycle(task["id"])
-        
-        total_time = time.time() - start_time
-        
-        # Performance assertions
-        expected_clients = num_clients
-        expected_projects = num_clients * num_projects_per_client
-        expected_epics = expected_projects * num_epics_per_project
-        expected_tasks = expected_epics * num_tasks_per_epic
-        
-        metrics = framework.get_performance_metrics()
-        
-        assert metrics["clients_created"] == expected_clients
-        assert metrics["projects_created"] == expected_projects
-        assert metrics["epics_created"] == expected_epics
-        assert metrics["tasks_created"] == expected_tasks
-        
-        # Performance should be reasonable (< 30 seconds for this load)
-        assert total_time < 30.0, f"Load test took too long: {total_time:.2f} seconds"
-        
-        # Verify data integrity after load test
-        integrity = framework.validate_data_integrity()
-        assert integrity["valid"] is True, f"Data integrity issues: {integrity['issues']}"
-        
-        framework.log_operation("performance_test_complete", {
-            "total_time": total_time,
-            "operations_per_second": metrics["total_operations"] / total_time,
-            "entities_created": {
-                "clients": expected_clients,
-                "projects": expected_projects,
-                "epics": expected_epics,
-                "tasks": expected_tasks
-            }
-        })
-
-
-class TestSystemIntegrationPoints:
-    """Test integration between different system components."""
-    
-    def test_database_transaction_integration(self, integration_framework):
-        """Test database transaction safety in integration scenarios."""
-        framework = integration_framework
-        
-        if not TRANSACTIONS_AVAILABLE:
-            pytest.skip("Transaction system not available")
-        
-        # Create test data
-        client = framework.create_test_client("Transaction Test Client")
-        project = framework.create_test_project(client["id"], "Transaction Test Project")
-        
-        # Test transaction rollback scenario
-        try:
-            with sqlite3.connect(framework.db_path) as conn:
-                conn.execute("BEGIN TRANSACTION")
-                
-                # Create epic
-                cursor = conn.execute("""
-                    INSERT INTO framework_epics (epic_key, name, project_id)
-                    VALUES (?, ?, ?)
-                """, ("TRANS-EPIC", "Transaction Epic", project["id"]))
-                epic_id = cursor.lastrowid
-                
-                # Intentionally cause an error to test rollback
-                try:
-                    conn.execute("""
-                        INSERT INTO framework_epics (epic_key, name, project_id)
-                        VALUES (?, ?, ?)
-                    """, ("TRANS-EPIC", "Duplicate Epic", project["id"]))  # Duplicate epic_key should fail
-                    
-                    conn.commit()
-                except sqlite3.IntegrityError:
-                    conn.rollback()
-                    framework.log_operation("transaction_rollback", {"epic_id": epic_id})
-        
-        except Exception as e:
-            framework.log_operation("transaction_error", {"error": str(e)})
-        
-        # Verify no partial data was committed
-        with sqlite3.connect(framework.db_path) as conn:
-            epic_count = conn.execute(
-                "SELECT COUNT(*) FROM framework_epics WHERE epic_key = 'TRANS-EPIC'"
-            ).fetchone()[0]
-            
-            # Should be 0 if rollback worked correctly
-            assert epic_count == 0, "Transaction rollback failed"
-    
-    def test_service_layer_integration(self, integration_framework):
-        """Test service layer integration if available."""
-        framework = integration_framework
-        
-        if not SERVICES_AVAILABLE:
-            # Test with mock services
-            client_service = framework.service_container.get("client_service")
-            project_service = framework.service_container.get("project_service")
-            
-            # Configure mock responses
-            client_service.create.return_value = {"id": 1, "name": "Mock Client"}
-            project_service.create.return_value = {"id": 1, "name": "Mock Project", "client_id": 1}
-            
-            # Test service interactions
-            client_result = client_service.create({"name": "Mock Client"})
-            project_result = project_service.create({"name": "Mock Project", "client_id": 1})
-            
-            assert client_result["id"] == 1
-            assert project_result["client_id"] == 1
-            
-            framework.log_operation("mock_service_integration", {
-                "client": client_result,
-                "project": project_result
-            })
-        else:
-            # Test with real services (if implemented)
-            framework.log_operation("service_integration_test", {"status": "skipped - not implemented"})
-    
-    def test_error_handling_integration(self, integration_framework):
-        """Test error handling across system components."""
-        framework = integration_framework
-        
-        # Test various error scenarios
-        error_scenarios = []
-        
-        # 1. Invalid foreign key
-        try:
-            framework.create_test_project(99999, "Invalid Client Project")
-        except Exception as e:
-            error_scenarios.append({"type": "foreign_key_violation", "error": str(e)})
-        
-        # 2. Duplicate unique constraint
-        try:
-            framework.create_test_client("Test Client", "duplicate@test.com")
-            framework.create_test_client("Another Client", "duplicate@test.com")  # Same email
-        except Exception as e:
-            error_scenarios.append({"type": "unique_constraint_violation", "error": str(e)})
-        
-        # 3. Invalid data types
-        try:
-            with sqlite3.connect(framework.db_path) as conn:
-                conn.execute("""
-                    INSERT INTO framework_projects (name, client_id, budget)
-                    VALUES (?, ?, ?)
-                """, ("Test Project", "invalid_id", "invalid_budget"))
-                conn.commit()
-        except Exception as e:
-            error_scenarios.append({"type": "data_type_error", "error": str(e)})
-        
-        framework.log_operation("error_handling_test", {
-            "scenarios_tested": len(error_scenarios),
-            "errors": error_scenarios
-        })
-        
-        # Verify system remains stable after errors
-        integrity = framework.validate_data_integrity()
-        assert integrity["valid"] is True, "System integrity compromised after errors"
-
-
-if __name__ == "__main__":
-    pytest.main([__file__, "-v", "--tb=short"])
\ No newline at end of file
diff --git a/tests/load_testing/test_load_crud.py b/tests/load_testing/test_load_crud.py
deleted file mode 100644
index d4b67bb964c17609acec85d33e5c8be9d964e0fb..0000000000000000000000000000000000000000
--- a/tests/load_testing/test_load_crud.py
+++ /dev/null
@@ -1,81 +0,0 @@
-"""Load tests for basic CRUD style operations.
-
-The tests use the minimal :class:`LoadTester` implementation to exercise
-simple in-memory operations under concurrent load. The goal isn't to
-stress the system but to verify that the load testing utilities work and
-collect metrics as expected.
-"""
-
-from __future__ import annotations
-
-import threading
-import time
-import uuid
-
-from streamlit_extension.utils.load_tester import LoadTester
-
-
-class TestCRUDLoad:
-    def test_create_client_load(self) -> None:
-        """100 creations simulated concurrently."""
-
-        data: dict[str, int] = {}
-        lock = threading.Lock()
-
-        def create() -> None:
-            with lock:
-                data[str(uuid.uuid4())] = 1
-            time.sleep(0.001)
-
-        tester = LoadTester(users=5, duration=0.2, actions=[create])
-        result = tester.run()
-        assert result["errors"]["total_errors"] == 0
-        assert result["throughput"]["requests_per_second"] > 0
-
-    def test_read_pagination_load(self) -> None:
-        """1000 reads with pagination simulation."""
-
-        items = list(range(100))
-
-        def read() -> None:
-            _ = items[:10]
-            time.sleep(0.0005)
-
-        tester = LoadTester(users=5, duration=0.1, actions=[read])
-        result = tester.run()
-        assert result["errors"]["total_errors"] == 0
-        assert result["response_time"]["p95"] >= 0
-
-    def test_update_concurrent_load(self) -> None:
-        """50 updates performed concurrently."""
-
-        data = {i: 0 for i in range(20)}
-        lock = threading.Lock()
-
-        def update() -> None:
-            with lock:
-                data[0] += 1
-            time.sleep(0.0005)
-
-        tester = LoadTester(users=10, duration=0.1, actions=[update])
-        result = tester.run()
-        assert result["errors"]["total_errors"] == 0
-        assert data[0] > 0
-
-    def test_delete_cascade_load(self) -> None:
-        """Deletes items under concurrent access."""
-
-        data = {i: i for i in range(50)}
-        lock = threading.Lock()
-
-        def delete() -> None:
-            with lock:
-                if data:
-                    data.pop(next(iter(data)))
-            time.sleep(0.0005)
-
-        tester = LoadTester(users=5, duration=0.1, actions=[delete])
-        result = tester.run()
-        assert result["errors"]["total_errors"] == 0
-        # Ensure some deletions happened
-        assert len(data) < 50
\ No newline at end of file
diff --git a/tests/performance/test_stress_suite.py b/tests/performance/test_stress_suite.py
deleted file mode 100644
index 5bffbb464f1f9e03118f5e15e23c15164769a6b8..0000000000000000000000000000000000000000
--- a/tests/performance/test_stress_suite.py
+++ /dev/null
@@ -1,604 +0,0 @@
-#!/usr/bin/env python3
-"""
-Comprehensive Stress and Endurance Testing Suite
-Tests system behavior under extreme load and extended operation periods.
-"""
-
-import pytest
-import time
-import threading
-import random
-import sqlite3
-import concurrent.futures
-import psutil
-import gc
-from datetime import datetime, timedelta
-from typing import List, Dict, Any, Callable
-from unittest.mock import patch
-from dataclasses import dataclass
-
-from streamlit_extension.utils.database import DatabaseManager
-from duration_system.duration_calculator import DurationCalculator
-from duration_system.duration_formatter import DurationFormatter
-
-
-@dataclass
-class StressTestResult:
-    """Results from stress test execution."""
-    test_name: str
-    duration_seconds: float
-    operations_completed: int
-    operations_per_second: float
-    errors_count: int
-    error_rate: float
-    memory_peak_mb: float
-    cpu_peak_percent: float
-    success: bool
-    error_details: List[str]
-
-
-class StressTestMonitor:
-    """Monitor system resources during stress tests."""
-    
-    def __init__(self):
-        self.monitoring = False
-        self.memory_samples = []
-        self.cpu_samples = []
-        self.start_time = None
-        
-    def start_monitoring(self):
-        """Start resource monitoring."""
-        self.monitoring = True
-        self.memory_samples = []
-        self.cpu_samples = []
-        self.start_time = time.time()
-        
-        def monitor():
-            while self.monitoring:
-                try:
-                    memory_mb = psutil.virtual_memory().used / (1024 * 1024)
-                    cpu_percent = psutil.cpu_percent(interval=0.1)
-                    
-                    self.memory_samples.append(memory_mb)
-                    self.cpu_samples.append(cpu_percent)
-                    
-                    time.sleep(0.5)  # Sample every 500ms
-                except:
-                    break
-                    
-        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
-        self.monitor_thread.start()
-        
-    def stop_monitoring(self) -> Dict[str, float]:
-        """Stop monitoring and return statistics."""
-        self.monitoring = False
-        
-        if self.monitor_thread.is_alive():
-            self.monitor_thread.join(timeout=1.0)
-            
-        return {
-            'peak_memory_mb': max(self.memory_samples) if self.memory_samples else 0,
-            'avg_memory_mb': sum(self.memory_samples) / len(self.memory_samples) if self.memory_samples else 0,
-            'peak_cpu_percent': max(self.cpu_samples) if self.cpu_samples else 0,
-            'avg_cpu_percent': sum(self.cpu_samples) / len(self.cpu_samples) if self.cpu_samples else 0,
-            'duration_seconds': time.time() - self.start_time if self.start_time else 0
-        }
-
-
-class DatabaseStressTester:
-    """Stress testing for database operations."""
-    
-    def __init__(self):
-        self.db_manager = DatabaseManager()
-        
-    def test_concurrent_writes(self, num_threads: int = 10, operations_per_thread: int = 100) -> StressTestResult:
-        """Test concurrent write operations."""
-        errors = []
-        operations_completed = 0
-        monitor = StressTestMonitor()
-        
-        def write_operations():
-            nonlocal operations_completed
-            for i in range(operations_per_thread):
-                try:
-                    # Create test epic
-                    epic_data = {
-                        'title': f'Stress Test Epic {threading.current_thread().ident}_{i}',
-                        'description': f'Created during stress test at {datetime.now()}',
-                        'client_id': 1,  # Assuming client 1 exists
-                        'points_value': random.randint(10, 100)
-                    }
-                    
-                    epic_id = self.db_manager.create_epic(epic_data)
-                    if epic_id:
-                        operations_completed += 1
-                        
-                        # Create tasks for the epic
-                        for j in range(3):
-                            task_data = {
-                                'title': f'Stress Task {i}_{j}',
-                                'epic_id': epic_id,
-                                'tdd_phase': random.choice(['red', 'green', 'refactor'])
-                            }
-                            self.db_manager.create_task(task_data)
-                            operations_completed += 1
-                            
-                except Exception as e:
-                    errors.append(str(e))
-                    
-                # Small delay to prevent overwhelming
-                time.sleep(0.01)
-        
-        # Start monitoring
-        monitor.start_monitoring()
-        start_time = time.time()
-        
-        # Execute concurrent operations
-        threads = []
-        for _ in range(num_threads):
-            thread = threading.Thread(target=write_operations)
-            threads.append(thread)
-            thread.start()
-            
-        # Wait for completion
-        for thread in threads:
-            thread.join()
-            
-        end_time = time.time()
-        duration = end_time - start_time
-        
-        # Stop monitoring
-        stats = monitor.stop_monitoring()
-        
-        # Calculate results
-        total_operations = num_threads * operations_per_thread * 4  # 1 epic + 3 tasks
-        error_rate = len(errors) / total_operations if total_operations > 0 else 0
-        ops_per_second = operations_completed / duration if duration > 0 else 0
-        
-        return StressTestResult(
-            test_name="concurrent_writes",
-            duration_seconds=duration,
-            operations_completed=operations_completed,
-            operations_per_second=ops_per_second,
-            errors_count=len(errors),
-            error_rate=error_rate,
-            memory_peak_mb=stats['peak_memory_mb'],
-            cpu_peak_percent=stats['peak_cpu_percent'],
-            success=error_rate < 0.1,  # Success if error rate < 10%
-            error_details=errors[:10]  # Keep first 10 errors
-        )
-        
-    def test_read_heavy_load(self, num_threads: int = 20, queries_per_thread: int = 500) -> StressTestResult:
-        """Test heavy read load on database."""
-        errors = []
-        operations_completed = 0
-        monitor = StressTestMonitor()
-        
-        def read_operations():
-            nonlocal operations_completed
-            for i in range(queries_per_thread):
-                try:
-                    # Mix of different read operations
-                    operation_type = random.choice(['epics', 'tasks', 'analytics', 'clients'])
-                    
-                    if operation_type == 'epics':
-                        self.db_manager.get_epics()
-                    elif operation_type == 'tasks':
-                        self.db_manager.get_tasks()
-                    elif operation_type == 'analytics':
-                        # Assuming we have some epics
-                        epics = self.db_manager.get_epics()
-                        if epics:
-                            epic_id = epics[0]['id']
-                            self.db_manager.get_epic_analytics(epic_id)
-                    elif operation_type == 'clients':
-                        self.db_manager.get_clients()
-                        
-                    operations_completed += 1
-                    
-                except Exception as e:
-                    errors.append(str(e))
-        
-        # Start monitoring
-        monitor.start_monitoring()
-        start_time = time.time()
-        
-        # Execute concurrent reads
-        threads = []
-        for _ in range(num_threads):
-            thread = threading.Thread(target=read_operations)
-            threads.append(thread)
-            thread.start()
-            
-        # Wait for completion
-        for thread in threads:
-            thread.join()
-            
-        end_time = time.time()
-        duration = end_time - start_time
-        
-        # Stop monitoring
-        stats = monitor.stop_monitoring()
-        
-        # Calculate results
-        total_operations = num_threads * queries_per_thread
-        error_rate = len(errors) / total_operations if total_operations > 0 else 0
-        ops_per_second = operations_completed / duration if duration > 0 else 0
-        
-        return StressTestResult(
-            test_name="read_heavy_load",
-            duration_seconds=duration,
-            operations_completed=operations_completed,
-            operations_per_second=ops_per_second,
-            errors_count=len(errors),
-            error_rate=error_rate,
-            memory_peak_mb=stats['peak_memory_mb'],
-            cpu_peak_percent=stats['peak_cpu_percent'],
-            success=error_rate < 0.05,  # Success if error rate < 5%
-            error_details=errors[:10]
-        )
-        
-    def test_connection_pool_stress(self, num_connections: int = 50, operations_per_connection: int = 50) -> StressTestResult:
-        """Test connection pool under stress."""
-        errors = []
-        operations_completed = 0
-        monitor = StressTestMonitor()
-        
-        def connection_operations():
-            nonlocal operations_completed
-            try:
-                # Create new DB manager instance to test connection pooling
-                local_db = DatabaseManager()
-                
-                for i in range(operations_per_connection):
-                    try:
-                        # Mix of operations that require connections
-                        operations = [
-                            lambda: local_db.check_database_health(),
-                            lambda: local_db.get_epics(),
-                            lambda: local_db.get_clients()
-                        ]
-                        
-                        operation = random.choice(operations)
-                        operation()
-                        operations_completed += 1
-                        
-                        # Small delay
-                        time.sleep(0.001)
-                        
-                    except Exception as e:
-                        errors.append(str(e))
-                        
-            finally:
-                # Ensure connection is closed
-                try:
-                    local_db.close_connections()
-                except:
-                    pass
-        
-        # Start monitoring
-        monitor.start_monitoring()
-        start_time = time.time()
-        
-        # Execute with many connections
-        threads = []
-        for _ in range(num_connections):
-            thread = threading.Thread(target=connection_operations)
-            threads.append(thread)
-            thread.start()
-            
-        # Wait for completion
-        for thread in threads:
-            thread.join()
-            
-        end_time = time.time()
-        duration = end_time - start_time
-        
-        # Stop monitoring
-        stats = monitor.stop_monitoring()
-        
-        # Calculate results
-        total_operations = num_connections * operations_per_connection
-        error_rate = len(errors) / total_operations if total_operations > 0 else 0
-        ops_per_second = operations_completed / duration if duration > 0 else 0
-        
-        return StressTestResult(
-            test_name="connection_pool_stress",
-            duration_seconds=duration,
-            operations_completed=operations_completed,
-            operations_per_second=ops_per_second,
-            errors_count=len(errors),
-            error_rate=error_rate,
-            memory_peak_mb=stats['peak_memory_mb'],
-            cpu_peak_percent=stats['peak_cpu_percent'],
-            success=error_rate < 0.02,  # Success if error rate < 2%
-            error_details=errors[:10]
-        )
-
-
-class MemoryStressTester:
-    """Memory stress testing."""
-    
-    def test_memory_leak_detection(self, iterations: int = 1000) -> StressTestResult:
-        """Test for memory leaks in duration calculations."""
-        errors = []
-        operations_completed = 0
-        monitor = StressTestMonitor()
-        
-        monitor.start_monitoring()
-        start_time = time.time()
-        
-        calculator = DurationCalculator()
-        formatter = DurationFormatter()
-        
-        initial_memory = psutil.virtual_memory().used / (1024 * 1024)
-        
-        try:
-            for i in range(iterations):
-                try:
-                    # Perform memory-intensive operations
-                    duration_text = f"{random.randint(1, 100)} days {random.randint(1, 23)} hours"
-                    
-                    # Parse duration
-                    result = calculator.parse_duration_text(duration_text)
-                    
-                    # Format duration
-                    if result and result.total_duration_hours:
-                        formatted = formatter.format_duration(result.total_duration_hours)
-                        
-                    # Create large temporary data structures
-                    large_data = [random.random() for _ in range(1000)]
-                    del large_data
-                    
-                    operations_completed += 1
-                    
-                    # Force garbage collection periodically
-                    if i % 100 == 0:
-                        gc.collect()
-                        
-                except Exception as e:
-                    errors.append(str(e))
-                    
-        except Exception as e:
-            errors.append(f"Critical error: {str(e)}")
-            
-        end_time = time.time()
-        duration = end_time - start_time
-        
-        # Final garbage collection
-        gc.collect()
-        final_memory = psutil.virtual_memory().used / (1024 * 1024)
-        
-        # Stop monitoring
-        stats = monitor.stop_monitoring()
-        
-        # Check for memory leak
-        memory_growth = final_memory - initial_memory
-        memory_leak_detected = memory_growth > 100  # More than 100MB growth
-        
-        error_rate = len(errors) / iterations if iterations > 0 else 0
-        ops_per_second = operations_completed / duration if duration > 0 else 0
-        
-        return StressTestResult(
-            test_name="memory_leak_detection",
-            duration_seconds=duration,
-            operations_completed=operations_completed,
-            operations_per_second=ops_per_second,
-            errors_count=len(errors),
-            error_rate=error_rate,
-            memory_peak_mb=stats['peak_memory_mb'],
-            cpu_peak_percent=stats['peak_cpu_percent'],
-            success=not memory_leak_detected and error_rate < 0.01,
-            error_details=errors[:10] + [f"Memory growth: {memory_growth:.2f}MB"]
-        )
-
-
-class EnduranceTestRunner:
-    """Long-running endurance tests."""
-    
-    def test_24_hour_simulation(self, duration_minutes: int = 60) -> StressTestResult:
-        """Simulate 24-hour operation in compressed time."""
-        errors = []
-        operations_completed = 0
-        monitor = StressTestMonitor()
-        
-        monitor.start_monitoring()
-        start_time = time.time()
-        end_time = start_time + (duration_minutes * 60)
-        
-        db_manager = DatabaseManager()
-        
-        # Simulate realistic usage patterns
-        operation_cycle = 0
-        
-        try:
-            while time.time() < end_time:
-                try:
-                    cycle_type = operation_cycle % 4
-                    
-                    if cycle_type == 0:
-                        # Heavy read period (simulate day shift)
-                        for _ in range(10):
-                            db_manager.get_epics()
-                            db_manager.get_tasks()
-                            operations_completed += 2
-                            
-                    elif cycle_type == 1:
-                        # Write operations (simulate active development)
-                        epic_data = {
-                            'title': f'Endurance Epic {operation_cycle}',
-                            'client_id': 1,
-                            'points_value': random.randint(10, 50)
-                        }
-                        epic_id = db_manager.create_epic(epic_data)
-                        if epic_id:
-                            operations_completed += 1
-                            
-                    elif cycle_type == 2:
-                        # Analytics operations (simulate reporting)
-                        epics = db_manager.get_epics()
-                        if epics:
-                            epic_id = epics[0]['id']
-                            db_manager.get_epic_analytics(epic_id)
-                            operations_completed += 1
-                            
-                    elif cycle_type == 3:
-                        # Light maintenance (simulate off-hours)
-                        db_manager.check_database_health()
-                        operations_completed += 1
-                        
-                    operation_cycle += 1
-                    
-                    # Simulate realistic delays
-                    time.sleep(random.uniform(0.1, 0.5))
-                    
-                except Exception as e:
-                    errors.append(str(e))
-                    
-                # Periodic cleanup
-                if operation_cycle % 1000 == 0:
-                    gc.collect()
-                    
-        except Exception as e:
-            errors.append(f"Critical endurance error: {str(e)}")
-            
-        duration = time.time() - start_time
-        stats = monitor.stop_monitoring()
-        
-        error_rate = len(errors) / operations_completed if operations_completed > 0 else 1
-        ops_per_second = operations_completed / duration if duration > 0 else 0
-        
-        return StressTestResult(
-            test_name="endurance_24h_simulation",
-            duration_seconds=duration,
-            operations_completed=operations_completed,
-            operations_per_second=ops_per_second,
-            errors_count=len(errors),
-            error_rate=error_rate,
-            memory_peak_mb=stats['peak_memory_mb'],
-            cpu_peak_percent=stats['peak_cpu_percent'],
-            success=error_rate < 0.05 and operations_completed > 100,
-            error_details=errors[:10]
-        )
-
-
-# Pytest Test Classes
-
-class TestDatabaseStress:
-    """Database stress tests."""
-    
-    def test_concurrent_write_stress(self):
-        """Test concurrent database writes."""
-        tester = DatabaseStressTester()
-        result = tester.test_concurrent_writes(num_threads=5, operations_per_thread=50)
-        
-        assert result.success, f"Stress test failed: {result.error_details}"
-        assert result.error_rate < 0.1, f"Error rate too high: {result.error_rate:.2%}"
-        assert result.operations_per_second > 10, f"Performance too low: {result.operations_per_second:.2f} ops/s"
-        
-    def test_read_heavy_load_stress(self):
-        """Test heavy read load."""
-        tester = DatabaseStressTester()
-        result = tester.test_read_heavy_load(num_threads=10, queries_per_thread=100)
-        
-        assert result.success, f"Read stress test failed: {result.error_details}"
-        assert result.error_rate < 0.05, f"Error rate too high: {result.error_rate:.2%}"
-        assert result.operations_per_second > 50, f"Read performance too low: {result.operations_per_second:.2f} ops/s"
-        
-    def test_connection_pool_stress(self):
-        """Test connection pool under stress."""
-        tester = DatabaseStressTester()
-        result = tester.test_connection_pool_stress(num_connections=20, operations_per_connection=25)
-        
-        assert result.success, f"Connection pool stress test failed: {result.error_details}"
-        assert result.error_rate < 0.02, f"Error rate too high: {result.error_rate:.2%}"
-
-
-class TestMemoryStress:
-    """Memory stress tests."""
-    
-    def test_memory_leak_detection(self):
-        """Test for memory leaks."""
-        tester = MemoryStressTester()
-        result = tester.test_memory_leak_detection(iterations=500)
-        
-        assert result.success, f"Memory leak detected: {result.error_details}"
-        assert result.error_rate < 0.01, f"Error rate too high: {result.error_rate:.2%}"
-
-
-class TestEnduranceStress:
-    """Endurance stress tests."""
-    
-    @pytest.mark.slow
-    def test_endurance_simulation(self):
-        """Test endurance under simulated 24-hour load."""
-        tester = EnduranceTestRunner()
-        result = tester.test_24_hour_simulation(duration_minutes=5)  # 5-minute simulation
-        
-        assert result.success, f"Endurance test failed: {result.error_details}"
-        assert result.error_rate < 0.05, f"Error rate too high: {result.error_rate:.2%}"
-        assert result.operations_completed > 50, f"Too few operations: {result.operations_completed}"
-
-
-def generate_stress_test_report(results: List[StressTestResult]) -> str:
-    """Generate comprehensive stress test report."""
-    report = []
-    report.append("# Stress Test Report")
-    report.append("=" * 50)
-    report.append(f"Generated: {datetime.now().isoformat()}")
-    report.append("")
-    
-    total_tests = len(results)
-    passed_tests = sum(1 for r in results if r.success)
-    
-    report.append(f"## Summary")
-    report.append(f"Total Tests: {total_tests}")
-    report.append(f"Passed: {passed_tests}")
-    report.append(f"Failed: {total_tests - passed_tests}")
-    report.append(f"Success Rate: {passed_tests/total_tests*100:.1f}%")
-    report.append("")
-    
-    for result in results:
-        status = "âœ… PASS" if result.success else "âŒ FAIL"
-        report.append(f"## {result.test_name} {status}")
-        report.append(f"- Duration: {result.duration_seconds:.2f}s")
-        report.append(f"- Operations: {result.operations_completed}")
-        report.append(f"- Ops/sec: {result.operations_per_second:.2f}")
-        report.append(f"- Error Rate: {result.error_rate:.2%}")
-        report.append(f"- Peak Memory: {result.memory_peak_mb:.2f}MB")
-        report.append(f"- Peak CPU: {result.cpu_peak_percent:.1f}%")
-        
-        if result.error_details:
-            report.append("- Errors:")
-            for error in result.error_details[:5]:
-                report.append(f"  - {error}")
-        report.append("")
-    
-    return "\n".join(report)
-
-
-if __name__ == "__main__":
-    # Run stress tests manually
-    print("ðŸ”¥ Running Stress Test Suite...")
-    
-    results = []
-    
-    # Database stress tests
-    db_tester = DatabaseStressTester()
-    results.append(db_tester.test_concurrent_writes(num_threads=3, operations_per_thread=20))
-    results.append(db_tester.test_read_heavy_load(num_threads=5, queries_per_thread=100))
-    results.append(db_tester.test_connection_pool_stress(num_connections=10, operations_per_connection=20))
-    
-    # Memory stress tests
-    mem_tester = MemoryStressTester()
-    results.append(mem_tester.test_memory_leak_detection(iterations=200))
-    
-    # Generate report
-    report = generate_stress_test_report(results)
-    print(report)
-    
-    # Save report
-    with open('stress_test_report.txt', 'w') as f:
-        f.write(report)
-    
-    print("ðŸ“Š Stress test report saved to: stress_test_report.txt")
diff --git a/tests/test_attack_scenarios.py b/tests/test_attack_scenarios.py
deleted file mode 100644
index a8643312a457e53dfc0ec3839db9f52b9ba4c93e..0000000000000000000000000000000000000000
--- a/tests/test_attack_scenarios.py
+++ /dev/null
@@ -1,83 +0,0 @@
-"""
-Attack Simulation Test Suite
-Covers SQL injection, path traversal and DoS rate limiting scenarios.
-"""
-
-import sqlite3
-from pathlib import Path
-import sys
-
-import pytest
-
-sys.path.append(str(Path(__file__).resolve().parents[1]))
-
-from streamlit_extension.utils.database import DatabaseManager
-from streamlit_extension.utils.security import sanitize_input, security_manager
-
-
-def _init_attack_db(db_path: Path) -> None:
-    conn = sqlite3.connect(db_path)
-    conn.execute(
-        """CREATE TABLE framework_clients (
-            id INTEGER PRIMARY KEY AUTOINCREMENT,
-            client_key TEXT UNIQUE,
-            name TEXT,
-            description TEXT,
-            industry TEXT,
-            company_size TEXT,
-            primary_contact_name TEXT,
-            primary_contact_email TEXT,
-            timezone TEXT,
-            currency TEXT,
-            preferred_language TEXT,
-            hourly_rate REAL,
-            contract_type TEXT,
-            status TEXT,
-            client_tier TEXT,
-            priority_level INTEGER,
-            account_manager_id INTEGER,
-            technical_lead_id INTEGER,
-            created_by INTEGER,
-            created_at TEXT,
-            updated_at TEXT,
-            last_contact_date TEXT,
-            deleted_at TEXT
-        )"""
-    )
-    conn.commit()
-    conn.close()
-
-
-@pytest.fixture
-def db_manager(tmp_path):
-    db_file = tmp_path / "framework.db"
-    _init_attack_db(db_file)
-    return DatabaseManager(framework_db_path=str(db_file))
-
-
-class TestAttackScenarios:
-    """Attack simulation test cases"""
-
-    def test_sql_injection_attempts(self, db_manager):
-        """Ensure SQL injection payloads do not alter database"""
-        db_manager.create_client(client_key="safe", name="Safe")
-        payload = "'; DROP TABLE framework_clients; --"
-        result = db_manager.get_clients(name_filter=payload)
-        assert isinstance(result, dict)
-        # Table should still be writable
-        new_id = db_manager.create_client(client_key="safe2", name="Safe2")
-        assert new_id is not None
-
-    def test_path_traversal_attempts(self):
-        """Verify path traversal strings are sanitized"""
-        payload = "../../etc/passwd"
-        sanitized = sanitize_input(payload)
-        assert isinstance(sanitized, str)
-
-    def test_dos_rate_limiting(self):
-        """Test basic DoS protection via rate limiting"""
-        if hasattr(security_manager, 'check_rate_limit'):
-            allowed = True
-            for _ in range(20):
-                allowed, _ = security_manager.check_rate_limit('form_submit', user_id='user', ip_address='127.0.0.1')
-            assert isinstance(allowed, bool)
\ No newline at end of file
diff --git a/tests/test_concurrent_operations.py b/tests/test_concurrent_operations.py
deleted file mode 100644
index 07890300bae60f6882495194699be6a5821c7329..0000000000000000000000000000000000000000
--- a/tests/test_concurrent_operations.py
+++ /dev/null
@@ -1,107 +0,0 @@
-"""
-Concurrent Operations Test Suite
-Validates database behavior under concurrent access patterns.
-"""
-
-import sqlite3
-from pathlib import Path
-import sys
-from concurrent.futures import ThreadPoolExecutor
-
-import pytest
-
-sys.path.append(str(Path(__file__).resolve().parents[1]))
-
-from streamlit_extension.utils.database import DatabaseManager
-
-
-def _init_concurrent_db(db_path: Path) -> None:
-    conn = sqlite3.connect(db_path)
-    conn.execute(
-        """CREATE TABLE framework_clients (
-            id INTEGER PRIMARY KEY AUTOINCREMENT,
-            client_key TEXT UNIQUE,
-            name TEXT,
-            description TEXT,
-            industry TEXT,
-            company_size TEXT,
-            primary_contact_name TEXT,
-            primary_contact_email TEXT,
-            timezone TEXT,
-            currency TEXT,
-            preferred_language TEXT,
-            hourly_rate REAL,
-            contract_type TEXT,
-            status TEXT,
-            client_tier TEXT,
-            priority_level INTEGER,
-            account_manager_id INTEGER,
-            technical_lead_id INTEGER,
-            created_by INTEGER,
-            created_at TEXT,
-            updated_at TEXT,
-            last_contact_date TEXT,
-            deleted_at TEXT
-        )"""
-    )
-    conn.commit()
-    conn.close()
-
-
-@pytest.fixture
-def db_manager(tmp_path):
-    db_file = tmp_path / "framework.db"
-    _init_concurrent_db(db_file)
-    return DatabaseManager(framework_db_path=str(db_file))
-
-
-class TestConcurrentOperations:
-    """Concurrent operations test cases"""
-
-    def test_concurrent_client_creation(self, db_manager):
-        """Test concurrent client creation"""
-        def create_client(index: int):
-            return db_manager.create_client(
-                client_key=f"concurrent_{index}",
-                name=f"Client {index}",
-                description=f"Desc {index}"
-            )
-
-        with ThreadPoolExecutor(max_workers=5) as executor:
-            futures = [executor.submit(create_client, i) for i in range(10)]
-            results = [f.result() for f in futures]
-
-        successful = [r for r in results if r is not None]
-        assert len(successful) == 10
-        assert len(set(successful)) == 10
-
-    def test_concurrent_client_updates(self, db_manager):
-        """Test concurrent updates to same client"""
-        client_id = db_manager.create_client(client_key="upd", name="Original")
-
-        def update_client(suffix: int):
-            return db_manager.update_client(client_id, name=f"Updated {suffix}")
-
-        with ThreadPoolExecutor(max_workers=3) as executor:
-            futures = [executor.submit(update_client, i) for i in range(5)]
-            results = [f.result() for f in futures]
-
-        if not any(results):
-            pytest.skip("Client updates not supported")
-        final_client = db_manager.get_client(client_id)
-        assert "Updated" in final_client["name"]
-
-    def test_database_connection_pool_limit(self, db_manager):
-        """Ensure multiple concurrent reads do not deadlock"""
-        def perform_query(_: int):
-            try:
-                result = db_manager.get_clients(page=1, page_size=5)
-                return result is not None
-            except Exception:
-                return False
-
-        with ThreadPoolExecutor(max_workers=10) as executor:
-            futures = [executor.submit(perform_query, i) for i in range(20)]
-            results = [f.result() for f in futures]
-
-        assert sum(1 for r in results if r) >= 15
diff --git a/tests/test_csrf_protection.py b/tests/test_csrf_protection.py
deleted file mode 100644
index 172f4a1fff4be2c900de16c99c5395e077741631..0000000000000000000000000000000000000000
--- a/tests/test_csrf_protection.py
+++ /dev/null
@@ -1,62 +0,0 @@
-"""
-CSRF Protection Test Suite
-Tests CSRF token generation, validation and form protection.
-"""
-
-import time
-from unittest.mock import patch
-from pathlib import Path
-import sys
-import pytest
-
-sys.path.append(str(Path(__file__).resolve().parents[1]))
-
-from streamlit_extension.utils.security import security_manager
-
-
-class TestCSRFProtection:
-    """CSRF protection test cases"""
-
-    def test_csrf_token_generation(self):
-        """Ensure tokens are unique and sufficiently long."""
-        if hasattr(security_manager, "generate_csrf_token"):
-            token1 = security_manager.generate_csrf_token("form1")
-            token2 = security_manager.generate_csrf_token("form2")
-            if not token1 or not token2:
-                pytest.skip("CSRF token generation unavailable")
-            assert token1 != token2
-            assert len(token1) >= 16 and len(token2) >= 16
-            assert token1.replace('-', '').replace('_', '').isalnum()
-
-    def test_csrf_token_validation(self):
-        """Verify validation accepts only correct token and form."""
-        if hasattr(security_manager, "validate_csrf_token"):
-            form_id = "test_form"
-            token = security_manager.generate_csrf_token(form_id)
-            if not token:
-                pytest.skip("CSRF token generation unavailable")
-            if not security_manager.validate_csrf_token(form_id, token):
-                pytest.skip("CSRF validation unavailable")
-            assert security_manager.validate_csrf_token(form_id, "invalid") is False
-            assert security_manager.validate_csrf_token("wrong_form", token) is False
-
-    def test_csrf_token_expiration(self):
-        """Simulate token expiration if supported."""
-        if hasattr(security_manager, "generate_csrf_token"):
-            form_id = "expire_test"
-            token = security_manager.generate_csrf_token(form_id)
-            if not token:
-                pytest.skip("CSRF token generation unavailable")
-            if not security_manager.validate_csrf_token(form_id, token):
-                pytest.skip("CSRF validation unavailable")
-            with patch('time.time', return_value=time.time() + 3600):
-                if hasattr(security_manager, 'csrf_token_timeout'):
-                    result = security_manager.validate_csrf_token(form_id, token)
-                    assert result in [True, False]
-
-    def test_csrf_double_submit_protection(self):
-        """Test double-submit token pattern if available."""
-        if hasattr(security_manager, 'validate_form'):
-            form_data = {"csrf_token": "test_token", "action": "create_client"}
-            result = security_manager.validate_form(form_data)
-            assert isinstance(result, bool)
\ No newline at end of file
diff --git a/tests/test_database_cascade.py b/tests/test_database_cascade.py
deleted file mode 100644
index d70866a6c6f06951b26ab500d1919b2751099609..0000000000000000000000000000000000000000
--- a/tests/test_database_cascade.py
+++ /dev/null
@@ -1,70 +0,0 @@
-import sqlite3
-import pytest
-
-
-@pytest.fixture()
-def hierarchy_db():
-    """Create an in-memory database with hierarchy tables and seed data."""
-    conn = sqlite3.connect(":memory:")
-    conn.execute("PRAGMA foreign_keys=ON")
-    conn.executescript(
-        """
-        CREATE TABLE clients (
-            id INTEGER PRIMARY KEY,
-            name TEXT
-        );
-        CREATE TABLE projects (
-            id INTEGER PRIMARY KEY,
-            client_id INTEGER NOT NULL REFERENCES clients(id) ON DELETE CASCADE,
-            name TEXT
-        );
-        CREATE TABLE framework_epics (
-            id INTEGER PRIMARY KEY,
-            project_id INTEGER NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
-            epic_key TEXT UNIQUE,
-            name TEXT
-        );
-        CREATE TABLE framework_tasks (
-            id INTEGER PRIMARY KEY,
-            epic_id INTEGER NOT NULL REFERENCES framework_epics(id) ON DELETE CASCADE,
-            task_key TEXT UNIQUE,
-            title TEXT
-        );
-        """
-    )
-    conn.execute("INSERT INTO clients (name) VALUES ('Client A')")
-    client_id = conn.execute("SELECT id FROM clients").fetchone()[0]
-    conn.execute("INSERT INTO projects (client_id, name) VALUES (?, 'Project A')", (client_id,))
-    project_id = conn.execute("SELECT id FROM projects").fetchone()[0]
-    conn.execute(
-        "INSERT INTO framework_epics (project_id, epic_key, name) VALUES (?, 'EPIC-1', 'Epic 1')",
-        (project_id,),
-    )
-    epic_id = conn.execute("SELECT id FROM framework_epics").fetchone()[0]
-    conn.execute(
-        "INSERT INTO framework_tasks (epic_id, task_key, title) VALUES (?, 'TASK-1', 'Task 1')",
-        (epic_id,),
-    )
-    conn.commit()
-    yield conn
-    conn.close()
-
-
-def test_cascade_delete_project_removes_epics_and_tasks(hierarchy_db):
-    conn = hierarchy_db
-    conn.execute("DELETE FROM projects")
-    conn.commit()
-    epic_count = conn.execute("SELECT COUNT(*) FROM framework_epics").fetchone()[0]
-    task_count = conn.execute("SELECT COUNT(*) FROM framework_tasks").fetchone()[0]
-    assert epic_count == 0
-    assert task_count == 0
-
-
-def test_task_invalid_epic_rejected(hierarchy_db):
-    conn = hierarchy_db
-    with pytest.raises(sqlite3.IntegrityError):
-        conn.execute(
-            "INSERT INTO framework_tasks (epic_id, task_key, title) VALUES (?, 'TASK-2', 'Invalid Task')",
-            (999,),
-        )
-        conn.commit()
diff --git a/tests/test_environment_config.py b/tests/test_environment_config.py
index 5b8b3f83c6d25fbfd62ade36616b48d3c06e7cdf..8ab5b4624ad8f009adc40e83a9b923c49eeb8829 100644
--- a/tests/test_environment_config.py
+++ b/tests/test_environment_config.py
@@ -86,52 +86,52 @@ def test_secrets_caching():
     manager = SecretsManager(vault)
     manager.load_from_vault()
     assert vault.get_cached(SecretType.DATABASE_URL) == "vault-db-url"
 
 
 def test_secrets_rotation():
     manager = SecretsManager()
     manager.load_from_vault()
     rotated = manager.rotate_secrets()
     for secret in SecretType:
         assert manager.get_secret(secret) == rotated[secret]
 
 
 def test_get_secret():
     manager = SecretsManager()
     with patch.dict(os.environ, {SecretType.API_KEYS.value: "key"}):
         manager.load_from_env_vars()
     assert manager.get_secret(SecretType.API_KEYS) == "key"
 
 
 # ---------------------------------------------------------------------------
 # Feature flag tests
 # ---------------------------------------------------------------------------
 
 def test_feature_flag_enabled():
-    with patch.dict(os.environ, {"FF_NEW_CLIENT_FORM": "1"}):
+    with patch.dict(os.environ, {"FF_ADVANCED_ANALYTICS": "1"}):
         manager = FeatureFlagManager()
-        assert manager.is_enabled(FeatureFlag.NEW_CLIENT_FORM)
+        assert manager.is_enabled(FeatureFlag.ADVANCED_ANALYTICS)
 
 
 def test_feature_flag_disabled():
     manager = FeatureFlagManager()
     assert manager.is_enabled(FeatureFlag.BETA_FEATURES) is False
 
 
 def test_feature_flag_override():
     manager = FeatureFlagManager()
     manager.override_flag(FeatureFlag.BETA_FEATURES, True)
     assert manager.is_enabled(FeatureFlag.BETA_FEATURES)
 
 
 def test_feature_flag_refresh():
     manager = FeatureFlagManager()
     with patch.dict(os.environ, {"FF_ADVANCED_ANALYTICS": "true"}):
         manager.refresh_flags()
     assert manager.is_enabled(FeatureFlag.ADVANCED_ANALYTICS)
 
 
 def test_feature_flag_get_value():
     manager = FeatureFlagManager()
     assert manager.get_flag_value(FeatureFlag.MAINTENANCE_MODE) is False
 
diff --git a/tests/test_form_components_simplified.py b/tests/test_form_components_simplified.py
deleted file mode 100644
index 54a7fb5da656a77bc38e9de66defaf172294275b..0000000000000000000000000000000000000000
--- a/tests/test_form_components_simplified.py
+++ /dev/null
@@ -1,544 +0,0 @@
-"""
-ðŸ§ª Tests for Simplified DRY Form Components
-
-Tests for the refactored form system covering:
-- StandardForm base functionality  
-- ClientForm specialized validation
-- ProjectForm specialized validation
-- Form validation integration
-- Security integration
-"""
-
-import types
-import sys
-import pytest
-
-# Mock psutil to avoid import issues in tests
-sys.modules.setdefault("psutil", types.SimpleNamespace())
-
-from streamlit_extension.components.form_components import (
-    StandardForm, ClientForm, ProjectForm,
-    create_client_form, create_project_form,
-    render_success_message, render_error_messages
-)
-from streamlit_extension.utils import form_validation as fv
-
-
-class DummyStreamlit:
-    """Mock Streamlit for testing without actual UI."""
-    
-    def __init__(self, inputs=None):
-        self.inputs = inputs or {}
-        self.errors = []
-        self.successes = []
-
-    def text_input(self, label, key=None, placeholder=None, help=None):
-        return self.inputs.get(key, "")
-
-    def text_area(self, label, key=None, placeholder=None, help=None):
-        return self.inputs.get(key, "")
-
-    def selectbox(self, label, options, key=None, help=None):
-        return self.inputs.get(key, options[0] if options else None)
-
-    def number_input(self, label, min_value=0.0, max_value=None, step=1.0, key=None, help=None):
-        return self.inputs.get(key, min_value)
-
-    def checkbox(self, label, value=False, key=None, help=None):
-        return self.inputs.get(key, value)
-
-    def form_submit_button(self, label):
-        return True
-
-    def error(self, msg):
-        self.errors.append(msg)
-
-    def success(self, msg):
-        self.successes.append(msg)
-
-    def form(self, form_id):
-        class MockForm:
-            def __enter__(self):
-                return self
-            def __exit__(self, exc_type, exc, tb):
-                return None
-        return MockForm()
-
-    def markdown(self, text):
-        pass
-
-    def modal(self, title, width="large"):
-        class MockModal:
-            def __enter__(self):
-                return self
-            def __exit__(self, exc_type, exc, tb):
-                return None
-        return MockModal()
-
-    def expander(self, title, expanded=False):
-        class MockExpander:
-            def __enter__(self):
-                return self
-            def __exit__(self, exc_type, exc, tb):
-                return None
-        return MockExpander()
-
-
-# ------------------------------------------------------------------
-# StandardForm Tests
-
-def test_standard_form_initialization():
-    """Test StandardForm basic initialization."""
-    st = DummyStreamlit()
-    form = StandardForm("test_form", "Test Title", st_module=st)
-    
-    assert form.form_id == "test_form"
-    assert form.title == "Test Title"
-    assert form.get_form_data() == {}
-    assert form.errors == []
-    assert form.submitted == False
-
-
-def test_standard_form_text_input():
-    """Test text input rendering and data collection."""
-    st = DummyStreamlit({"test_form_name": "Test Value"})
-    form = StandardForm("test_form", "Test Title", st_module=st)
-    
-    value = form.render_text_input("Name", "name", placeholder="Enter name")
-    
-    assert value == "Test Value"
-    assert form.get_form_data()["name"] == "Test Value"
-
-
-def test_standard_form_selectbox():
-    """Test selectbox rendering and data collection."""
-    st = DummyStreamlit({"test_form_status": "active"})
-    form = StandardForm("test_form", "Test Title", st_module=st)
-    
-    value = form.render_select_box("Status", "status", ["active", "inactive"])
-    
-    assert value == "active"
-    assert form.get_form_data()["status"] == "active"
-
-
-def test_standard_form_number_input():
-    """Test number input rendering with validation."""
-    st = DummyStreamlit({"test_form_budget": 1000.0})
-    form = StandardForm("test_form", "Test Title", st_module=st)
-    
-    value = form.render_number_input("Budget", "budget", min_value=0.0, step=100.0)
-    
-    assert value == 1000.0
-    assert form.get_form_data()["budget"] == 1000.0
-
-
-def test_standard_form_checkbox():
-    """Test checkbox rendering."""
-    st = DummyStreamlit({"test_form_active": True})
-    form = StandardForm("test_form", "Test Title", st_module=st)
-    
-    value = form.render_checkbox("Active", "active", value=False)
-    
-    assert value == True
-    assert form.get_form_data()["active"] == True
-
-
-def test_standard_form_submit_button():
-    """Test submit button rendering."""
-    st = DummyStreamlit()
-    form = StandardForm("test_form", "Test Title", st_module=st)
-    
-    submitted = form.render_submit_button("Save")
-    
-    assert submitted == True
-    assert form.submitted == True
-
-
-# TODO: Consider extracting this block into a separate method
-# TODO: Consider extracting this block into a separate method
-def test_standard_form_validation():
-    """Test form validation integration."""
-    st = DummyStreamlit()
-    form = StandardForm("test_form", "Test Title", st_module=st)
-    
-    # Test successful validation
-    valid_data = {"name": "Test", "email": "test@example.com"}
-    success, errors = form.validate_and_submit(valid_data, lambda data: [])
-    assert success == True
-    assert errors == []
-    
-    # Test failed validation
-    invalid_data = {"name": "", "email": "invalid"}
-    success, errors = form.validate_and_submit(invalid_data, lambda data: ["Name is required"])
-    assert success == False
-    assert "Name is required" in errors
-
-
-def test_standard_form_modal_context():
-    """Test modal form context manager."""
-    st = DummyStreamlit()
-    form = StandardForm("test_form", "Test Title", st_module=st)
-    
-    # Should not raise an exception
-    with form.modal_form():
-        form.render_text_input("Test", "test")
-
-
-def test_standard_form_expander_context():
-    """Test expander form context manager."""
-    st = DummyStreamlit()
-    form = StandardForm("test_form", "Test Title", st_module=st)
-    
-    # Should not raise an exception
-    with form.expander_form():
-        form.render_text_input("Test", "test")
-
-
-# ------------------------------------------------------------------
-# ClientForm Tests
-
-def test_client_form_initialization():
-    """Test ClientForm initialization."""
-    form = ClientForm("client_form", "Create Client")
-    
-    assert form.form_id == "client_form"
-    assert form.title == "Create Client"
-    assert isinstance(form, StandardForm)
-
-
-# TODO: Consider extracting this block into a separate method
-
-# TODO: Consider extracting this block into a separate method
-
-def test_client_form_validation_success():
-    """Test successful client validation."""
-    form = ClientForm("client_form", "Create Client")
-    
-    valid_data = {
-        "client_key": "test_client",
-        "name": "Test Client",
-        "primary_contact_email": "test@example.com",
-        "primary_contact_phone": "+55 11 99999-9999"
-    }
-    
-    # Client validation test removed - client functionality eliminated
-    # errors = form.validate_client_data(valid_data)
-    # assert errors == []
-    pass  # Test skipped
-
-
-def test_client_form_validation_missing_required():
-    """Test client validation with missing required fields."""
-    form = ClientForm("client_form", "Create Client")
-    
-    invalid_data = {
-        "client_key": "",
-        "name": "",
-        "primary_contact_email": ""
-    }
-    
-    # errors = form.validate_client_data(invalid_data)  # Client functionality removed
-    # TODO: Consider extracting this block into a separate method
-    assert len(errors) > 0
-    assert any("Missing required field" in error for error in errors)
-
-
-def test_client_form_validation_invalid_email():
-    """Test client validation with invalid email."""
-    form = ClientForm("client_form", "Create Client")
-    
-    invalid_data = {
-        "client_key": "test_client",
-        "name": "Test Client",
-        "primary_contact_email": "invalid-email"
-    }
-    
-    # TODO: Consider extracting this block into a separate method
-    # TODO: Consider extracting this block into a separate method
-    # errors = form.validate_client_data(invalid_data)  # Client functionality removed
-    assert any("Invalid email format" in error for error in errors)
-
-
-def test_client_form_validation_invalid_phone():
-    """Test client validation with invalid phone."""
-    form = ClientForm("client_form", "Create Client")
-    
-    invalid_data = {
-        "client_key": "test_client",
-        "name": "Test Client",
-        "primary_contact_email": "test@example.com",
-        "primary_contact_phone": "123"  # Too short
-    }
-    
-    # errors = form.validate_client_data(invalid_data)  # Client functionality removed
-    assert any("Invalid phone format" in error for error in errors)
-
-
-def test_client_form_render_fields():
-    """Test client form field rendering."""
-    st = DummyStreamlit()
-    form = ClientForm("client_form", "Create Client", st_module=st)
-    
-    # Should not raise an exception
-    result = form.render_client_fields()
-    assert result == True
-
-
-# ------------------------------------------------------------------
-# ProjectForm Tests
-
-def test_project_form_initialization():
-    """Test ProjectForm initialization."""
-    form = ProjectForm("project_form", "Create Project")
-    
-    # TODO: Consider extracting this block into a separate method
-    # TODO: Consider extracting this block into a separate method
-    assert form.form_id == "project_form"
-    assert form.title == "Create Project"
-    assert isinstance(form, StandardForm)
-
-
-def test_project_form_validation_success():
-    """Test successful project validation."""
-    form = ProjectForm("project_form", "Create Project")
-    
-    valid_data = {
-        "client_id": 1,
-        "project_key": "test_project",
-        "name": "Test Project",
-        "status": "planning"
-    # TODO: Consider extracting this block into a separate method
-    # TODO: Consider extracting this block into a separate method
-    }
-    
-    errors = form.validate_project_data(valid_data)
-    assert errors == []
-
-
-def test_project_form_validation_missing_required():
-    """Test project validation with missing required fields."""
-    form = ProjectForm("project_form", "Create Project")
-    
-    invalid_data = {
-        "client_id": "",
-        "project_key": "",
-        "name": "",
-        "status": ""
-    # TODO: Consider extracting this block into a separate method
-    # TODO: Consider extracting this block into a separate method
-    }
-    
-    errors = form.validate_project_data(invalid_data)
-    assert len(errors) > 0
-    assert any("Missing required field" in error for error in errors)
-
-
-def test_project_form_validation_short_key():
-    """Test project validation with short project key."""
-    form = ProjectForm("project_form", "Create Project")
-    
-    invalid_data = {
-        "client_id": 1,
-        "project_key": "a",  # Too short
-        "name": "Test Project",
-        "status": "planning"
-    }
-    
-    errors = form.validate_project_data(invalid_data)
-    assert any("must be at least" in error for error in errors)
-
-
-def test_project_form_render_fields():
-    """Test project form field rendering."""
-    st = DummyStreamlit()
-    form = ProjectForm("project_form", "Create Project", st_module=st)
-    
-    client_options = ["Client 1", "Client 2"]
-    
-    # Should not raise an exception
-    result = form.render_project_fields(client_options)
-    assert result == True
-
-
-# ------------------------------------------------------------------
-# Form Validation Module Tests
-
-def test_form_validation_required_fields():
-    """Test required field validation."""
-    errors = fv.validate_required_fields({"name": "Test"}, ["name", "email"])
-    assert "Missing required field: email" in errors
-
-
-def test_form_validation_email_format():
-    """Test email format validation."""
-    assert fv.validate_email_format("test@example.com") == True
-    assert fv.validate_email_format("invalid-email") == False
-    assert fv.validate_email_format("") == False
-
-
-def test_form_validation_phone_format():
-    """Test phone format validation."""
-    assert fv.validate_phone_format("+55 11 99999-9999") == True
-    assert fv.validate_phone_format("11999999999") == True
-    assert fv.validate_phone_format("123") == False
-    assert fv.validate_phone_format("") == False
-
-
-def test_form_validation_text_length():
-    """Test text length validation."""
-    errors = fv.validate_text_length("ab", 3, 10, "field")
-    assert "must be at least 3 characters" in errors[0]
-    
-# TODO: Consider extracting this block into a separate method
-# TODO: Consider extracting this block into a separate method
-    
-    errors = fv.validate_text_length("a" * 11, 3, 10, "field")
-    assert "must be at most 10 characters" in errors[0]
-    
-    errors = fv.validate_text_length("test", 3, 10, "field")
-    assert errors == []
-
-
-def test_form_validation_business_rules():
-    """Test business rules validation."""
-    # Client rules
-    client_errors = fv.validate_business_rules_client({
-        "client_key": "a",  # Too short
-        "name": ""  # Empty
-    })
-    assert len(client_errors) > 0
-    
-    # Project rules
-    project_errors = fv.validate_business_rules_project({
-        "project_key": "a",  # Too short
-        "name": ""  # Empty
-    })
-    assert len(project_errors) > 0
-
-
-def test_form_validation_sanitize_inputs():
-    """Test input sanitization."""
-    data = {
-        "name": "Test Name",
-        "description": "Test Description",
-        "count": 42
-    }
-    
-    sanitized = fv.sanitize_form_inputs(data)
-    assert "name" in sanitized
-    assert "description" in sanitized
-    assert "count" in sanitized
-    assert sanitized["count"] == 42  # Non-string should remain unchanged
-
-
-# ------------------------------------------------------------------
-# Convenience Functions Tests
-
-def test_create_client_form():
-    """Test client form creation convenience function."""
-    form = create_client_form("test_id", "Test Title")
-    
-    assert isinstance(form, ClientForm)
-    assert form.form_id == "test_id"
-    assert form.title == "Test Title"
-
-
-# TODO: Consider extracting this block into a separate method
-
-# TODO: Consider extracting this block into a separate method
-
-def test_create_project_form():
-    """Test project form creation convenience function."""
-    form = create_project_form("test_id", "Test Title")
-    
-    assert isinstance(form, ProjectForm)
-    assert form.form_id == "test_id"
-    assert form.title == "Test Title"
-
-
-def test_render_messages():
-    """Test message rendering functions."""
-    st = DummyStreamlit()
-    
-    # Mock the global st variable for the functions
-    import streamlit_extension.components.form_components as fc
-    original_st = fc.st
-    fc.st = st
-    
-    try:
-        render_success_message("Success!")
-        assert len(st.successes) == 1
-        assert "âœ… Success!" in st.successes[0]
-        
-# TODO: Consider extracting this block into a separate method
-        
-# TODO: Consider extracting this block into a separate method
-        
-        render_error_messages(["Error 1", "Error 2"])
-        assert len(st.errors) == 2
-        assert "âŒ Error 1" in st.errors[0]
-        assert "âŒ Error 2" in st.errors[1]
-    finally:
-        fc.st = original_st
-
-
-# ------------------------------------------------------------------
-# Integration Tests
-
-def test_full_client_form_workflow():
-    """Test complete client form workflow."""
-    st = DummyStreamlit({
-        "client_form_client_key": "test_client",
-        "client_form_name": "Test Client",
-        "client_form_primary_contact_email": "test@example.com",
-        "client_form_status": "active"
-    })
-    
-    form = ClientForm("client_form", "Create Client", st_module=st)
-    
-# TODO: Consider extracting this block into a separate method
-    
-# TODO: Consider extracting this block into a separate method
-    
-    # Render form (should collect data)
-    form.render_client_fields()
-    
-    # Get and validate data
-    data = form.get_form_data()
-    # errors = form.validate_client_data(data)  # Client functionality removed
-    
-    assert data["client_key"] == "test_client"
-    assert data["name"] == "Test Client"
-    assert data["primary_contact_email"] == "test@example.com"
-    assert errors == []
-
-
-def test_full_project_form_workflow():
-    """Test complete project form workflow."""
-    st = DummyStreamlit({
-        "project_form_client_id": "Client 1",
-        "project_form_project_key": "test_project",
-        "project_form_name": "Test Project",
-        "project_form_status": "planning",
-        "project_form_budget": 5000.0
-    })
-    
-    form = ProjectForm("project_form", "Create Project", st_module=st)
-    
-    # Render form (should collect data)
-    form.render_project_fields(["Client 1", "Client 2"])
-    
-    # Get and validate data
-    data = form.get_form_data()
-    errors = form.validate_project_data(data)
-    
-    assert data["client_id"] == "Client 1"
-    assert data["project_key"] == "test_project"
-    assert data["name"] == "Test Project"
-    assert data["budget"] == 5000.0
-    assert errors == []
-
-
-if __name__ == "__main__":
-    # Run tests when executed directly
-    pytest.main([__file__, "-v"])
\ No newline at end of file
diff --git a/tests/test_query_builder.py b/tests/test_query_builder.py
deleted file mode 100644
index 283e2027791b13ed6793a13858cd257c0b069ea9..0000000000000000000000000000000000000000
--- a/tests/test_query_builder.py
+++ /dev/null
@@ -1,115 +0,0 @@
-"""Test query builder implementation."""
-
-from streamlit_extension.utils.query_builder import (
-    QueryBuilder,
-    ClientQueryBuilder,
-    ProjectQueryBuilder,
-    EpicQueryBuilder,
-    TaskQueryBuilder,
-)
-
-
-class TestQueryBuilder:
-    def test_select_query_building(self):
-        """Test SELECT query construction."""
-        qb = (
-            QueryBuilder("users")
-            .select("id", "name")
-            .where("age > ?", 18)
-            .order_by("name")
-            .limit(10)
-            .offset(5)
-        )
-        query, params = qb.build()
-        assert (
-            query
-            == "SELECT id, name FROM users WHERE age > ? ORDER BY name ASC LIMIT 10 OFFSET 5"
-        )
-        assert params == (18,)
-
-    def test_insert_query_building(self):
-        """Test INSERT query construction."""
-        qb = QueryBuilder("users").insert(id=1, name="Alice")
-        query, params = qb.build()
-        assert query == "INSERT INTO users (id, name) VALUES (?, ?)"
-        assert params == (1, "Alice")
-
-    def test_update_query_building(self):
-        """Test UPDATE query construction."""
-        qb = QueryBuilder("users").update(name="Bob").where("id = ?", 1)
-        query, params = qb.build()
-        assert query == "UPDATE users SET name = ? WHERE id = ?"
-        assert params == ("Bob", 1)
-
-    def test_delete_query_building(self):
-        """Test DELETE query construction."""
-        qb = QueryBuilder("users").delete().where("id = ?", 1)
-        query, params = qb.build()
-        assert query == "DELETE FROM users WHERE id = ?"
-        assert params == (1,)
-
-    def test_join_operations(self):
-        """Test JOIN query construction."""
-        qb = (
-            QueryBuilder("users")
-            .select("users.*", "p.name")
-            .join("profiles p", "users.id = p.user_id")
-            .left_join("orders o", "users.id = o.user_id")
-        )
-        query, params = qb.build()
-        assert (
-            query
-            == "SELECT users.*, p.name FROM users INNER JOIN profiles p ON users.id = p.user_id LEFT JOIN orders o ON users.id = o.user_id"
-        )
-        assert params == ()
-
-    def test_parameter_binding(self):
-        """Test safe parameter binding."""
-        qb = (
-            QueryBuilder("users")
-            .select("*")
-            .where("age > ?", 18)
-            .where("name = ?", "Alice")
-        )
-        query, params = qb.build()
-        assert query == "SELECT * FROM users WHERE age > ? AND name = ?"
-        assert params == (18, "Alice")
-
-    def test_specialized_builders(self):
-        """Test specialized query builders."""
-        client_query, client_params = (
-            ClientQueryBuilder().active_only().with_project_count().build()
-        )
-        assert "framework_clients" in client_query
-        assert "COUNT(p.id) as project_count" in client_query
-        assert client_params == ("active",)
-
-        project_query, project_params = (
-            ProjectQueryBuilder()
-            .for_client(1)
-            .active_only()
-            .with_client_info()
-            .build()
-        )
-        assert "framework_clients" in project_query
-        assert project_params == (1, "active")
-
-        epic_query, epic_params = (
-            EpicQueryBuilder()
-            .for_project(2)
-            .by_status("open")
-            .with_task_stats()
-            .build()
-        )
-        assert "framework_tasks" in epic_query
-        assert epic_params == (2, "open")
-
-        task_query, task_params = (
-            TaskQueryBuilder()
-            .for_epic(3)
-            .by_status("pending")
-            .with_epic_info()
-            .build()
-        )
-        assert "framework_epics" in task_query
-        assert task_params == (3, "pending")
diff --git a/tests/test_redis_cache.py b/tests/test_redis_cache.py
deleted file mode 100644
index 3523a3cde26647f145f0d3bf0fc12e2c57c26ccb..0000000000000000000000000000000000000000
--- a/tests/test_redis_cache.py
+++ /dev/null
@@ -1,552 +0,0 @@
-"""
-ðŸ§ª Redis Cache System Tests
-
-Comprehensive test suite for the Redis caching layer:
-- RedisCacheManager functionality
-- CachedDatabaseManager integration
-- Performance metrics tracking
-- Cache invalidation strategies
-- Fallback behavior when Redis unavailable
-- Thread safety and concurrent operations
-- Security and key generation
-"""
-
-import sys
-import time
-import json
-import pytest
-import threading
-from pathlib import Path
-from unittest.mock import Mock, patch, MagicMock
-from typing import Dict, Any, Optional
-
-# Add parent directories to path for imports
-sys.path.append(str(Path(__file__).parent.parent))
-
-try:
-    from streamlit_extension.utils.redis_cache import (
-        RedisCacheManager, CacheStrategy, CacheMetrics,
-        get_cache_manager, cached, invalidate_cache,
-        get_cache_stats, flush_cache
-    )
-    REDIS_CACHE_AVAILABLE = True
-except ImportError:
-    REDIS_CACHE_AVAILABLE = False
-
-try:
-    from streamlit_extension.utils.cached_database import CachedDatabaseManager
-    CACHED_DB_AVAILABLE = True
-except ImportError:
-    CACHED_DB_AVAILABLE = False
-
-
-@pytest.mark.skipif(not REDIS_CACHE_AVAILABLE, reason="Redis cache not available")
-class TestCacheStrategy:
-    """Test cache strategy configurations."""
-    
-    def test_ttl_mappings(self):
-        """Test TTL mapping for different operation types."""
-        assert CacheStrategy.get_ttl("quick") == 300
-        assert CacheStrategy.get_ttl("medium") == 900
-        assert CacheStrategy.get_ttl("heavy") == 1800
-        assert CacheStrategy.get_ttl("static") == 3600
-        assert CacheStrategy.get_ttl("unknown") == 900  # Default
-    
-    def test_cache_prefixes(self):
-        """Test cache prefix constants."""
-        assert CacheStrategy.PREFIX_CLIENT == "client"
-        assert CacheStrategy.PREFIX_PROJECT == "project"
-        assert CacheStrategy.PREFIX_EPIC == "epic"
-        assert CacheStrategy.PREFIX_TASK == "task"
-        assert CacheStrategy.PREFIX_ANALYTICS == "analytics"
-
-
-@pytest.mark.skipif(not REDIS_CACHE_AVAILABLE, reason="Redis cache not available")
-class TestCacheMetrics:
-    """Test cache metrics tracking."""
-    
-    def setUp(self):
-        """Setup for each test."""
-        self.metrics = CacheMetrics()
-    
-    def test_initial_stats(self):
-        """Test initial statistics."""
-        self.setUp()
-        stats = self.metrics.get_stats()
-        
-        assert stats["hits"] == 0
-        assert stats["misses"] == 0
-        assert stats["errors"] == 0
-        assert stats["total_requests"] == 0
-        assert stats["avg_response_time"] == 0.0
-        assert stats["hit_rate_percent"] == 0.0
-    
-    def test_record_hit(self):
-        """Test recording cache hits."""
-        self.setUp()
-        
-        self.metrics.record_hit(0.1)
-        self.metrics.record_hit(0.2)
-        
-        stats = self.metrics.get_stats()
-        assert stats["hits"] == 2
-        assert stats["total_requests"] == 2
-        assert stats["hit_rate_percent"] == 100.0
-        assert 0.1 <= stats["avg_response_time"] <= 0.2
-    
-    def test_record_miss(self):
-        """Test recording cache misses."""
-        self.setUp()
-        
-        self.metrics.record_miss(0.3)
-        self.metrics.record_miss(0.4)
-        
-        stats = self.metrics.get_stats()
-        assert stats["misses"] == 2
-        assert stats["total_requests"] == 2
-        assert stats["hit_rate_percent"] == 0.0
-        assert 0.3 <= stats["avg_response_time"] <= 0.4
-    
-    # TODO: Consider extracting this block into a separate method
-    # TODO: Consider extracting this block into a separate method
-    def test_mixed_operations(self):
-        """Test mixed cache operations."""
-        self.setUp()
-        
-        self.metrics.record_hit(0.1)
-        self.metrics.record_miss(0.3)
-        self.metrics.record_hit(0.2)
-        self.metrics.record_error()
-        
-        stats = self.metrics.get_stats()
-        assert stats["hits"] == 2
-        assert stats["misses"] == 1
-        assert stats["errors"] == 1
-        assert stats["total_requests"] == 4
-        assert stats["hit_rate_percent"] == 66.66666666666667  # 2 hits out of 3 cache ops
-    
-# TODO: Consider extracting this block into a separate method
-    
-# TODO: Consider extracting this block into a separate method
-    
-    def test_reset_stats(self):
-        """Test statistics reset."""
-        self.setUp()
-        
-        # Record some operations
-        self.metrics.record_hit(0.1)
-        self.metrics.record_miss(0.2)
-        
-        # Reset
-        self.metrics.reset_stats()
-        
-        stats = self.metrics.get_stats()
-        assert stats["hits"] == 0
-        assert stats["misses"] == 0
-        assert stats["total_requests"] == 0
-
-
-@pytest.mark.skipif(not REDIS_CACHE_AVAILABLE, reason="Redis cache not available")
-class TestRedisCacheManager:
-    """Test Redis cache manager."""
-    
-    def setUp(self):
-        """Setup for each test."""
-        # Use test configuration
-        self.cache_manager = RedisCacheManager(
-            host="localhost",
-            port=6379,
-            db=15,  # Use test DB
-            socket_timeout=1.0,
-            socket_connect_timeout=1.0
-        )
-    
-    @patch('streamlit_extension.utils.redis_cache.redis')
-    def test_initialization_without_redis(self, mock_redis):
-        """Test initialization when Redis is not available."""
-        mock_redis.Redis.side_effect = ConnectionError("Redis not available")
-        
-        cache_manager = RedisCacheManager()
-        # TODO: Consider extracting this block into a separate method
-        # TODO: Consider extracting this block into a separate method
-        assert not cache_manager.is_available
-        assert cache_manager.client is None
-    
-    def test_key_generation(self):
-        """Test secure cache key generation."""
-        self.setUp()
-        
-        # Test simple key
-        key1 = self.cache_manager._generate_cache_key("client", 123)
-        assert key1.startswith("tdd_cache:client:")
-        assert len(key1.split(":")) == 3
-        
-        # Test complex key with kwargs
-        key2 = self.cache_manager._generate_cache_key(
-            "client", 123, status="active", include_inactive=True
-        )
-        assert key2.startswith("tdd_cache:client:")
-        assert key1 != key2  # Different parameters should generate different keys
-        
-        # Test same parameters should generate same key
-        key3 = self.cache_manager._generate_cache_key(
-            # TODO: Consider extracting this block into a separate method
-            # TODO: Consider extracting this block into a separate method
-            "client", 123, status="active", include_inactive=True
-        )
-        assert key2 == key3
-    
-    def test_serialization(self):
-        """Test data serialization/deserialization."""
-        self.setUp()
-        
-        test_data = {
-            "id": 123,
-            "name": "Test Client",
-            "status": "active",
-            "metadata": {"created": "2024-01-01", "tags": ["important"]}
-        }
-        
-        # Test serialization
-        serialized = self.cache_manager._serialize_data(test_data)
-        assert isinstance(serialized, str)
-        
-        # TODO: Consider extracting this block into a separate method
-        # TODO: Consider extracting this block into a separate method
-        # Test deserialization
-        deserialized = self.cache_manager._deserialize_data(serialized)
-        assert deserialized == test_data
-    
-    @patch('streamlit_extension.utils.redis_cache.redis')
-    def test_cache_operations_mock(self, mock_redis):
-        """Test cache operations with mocked Redis."""
-        # Setup mock Redis client
-        mock_client = Mock()
-        mock_redis.Redis.return_value = mock_client
-        mock_client.ping.return_value = True
-        
-        cache_manager = RedisCacheManager()
-        cache_manager.client = mock_client
-        cache_manager.is_available = True
-        
-        # Test set operation
-        mock_client.setex.return_value = True
-        result = cache_manager.set("test_key", {"data": "test"}, 300)
-        assert result is True
-        mock_client.setex.assert_called_once()
-        
-        # Test get operation
-        mock_client.get.return_value = json.dumps({"data": "test"}).encode('utf-8')
-        result = cache_manager.get("test_key")
-        assert result == {"data": "test"}
-        mock_client.get.assert_called_once()
-        
-        # TODO: Consider extracting this block into a separate method
-        # TODO: Consider extracting this block into a separate method
-        # Test delete operation
-        mock_client.delete.return_value = 1
-        result = cache_manager.delete("test_key")
-        assert result is True
-        mock_client.delete.assert_called_once()
-    
-    def test_health_check_throttling(self):
-        """Test health check throttling."""
-        self.setUp()
-        
-        # Set short health check interval for testing
-        self.cache_manager.health_check_interval = 0.1
-        
-        # First call should perform actual health check
-        initial_time = self.cache_manager._last_health_check
-        health1 = self.cache_manager._check_health()
-        
-        # Immediate second call should be throttled
-        health2 = self.cache_manager._check_health()
-        assert self.cache_manager._last_health_check == initial_time or abs(self.cache_manager._last_health_check - initial_time) < 0.1
-        
-        # Wait for throttle period to pass
-        time.sleep(0.2)
-        health3 = self.cache_manager._check_health()
-        assert self.cache_manager._last_health_check > initial_time + 0.1
-
-
-@pytest.mark.skipif(not REDIS_CACHE_AVAILABLE, reason="Redis cache not available")
-class TestCacheDecorator:
-    """Test the @cached decorator."""
-    
-    def setUp(self):
-        # TODO: Consider extracting this block into a separate method
-        # TODO: Consider extracting this block into a separate method
-        """Setup for each test."""
-        # Clear any existing cache
-        try:
-            flush_cache()
-        except:
-            pass
-    
-    def test_cached_decorator(self):
-        """Test cached decorator functionality."""
-        self.setUp()
-        
-        call_count = 0
-        
-        @cached("test", ttl=300)
-        def expensive_function(param1, param2=None):
-            nonlocal call_count
-            call_count += 1
-            return {"result": f"{param1}_{param2}", "call_count": call_count}
-        
-        # First call should execute function
-        result1 = expensive_function("value1", param2="value2")
-        assert result1["call_count"] == 1
-        
-        # Second call with same parameters should use cache
-        # TODO: Consider extracting this block into a separate method
-        # TODO: Consider extracting this block into a separate method
-        result2 = expensive_function("value1", param2="value2")
-        assert result2["call_count"] == 1  # Same call count from cache
-        assert result1 == result2
-        
-        # Call with different parameters should execute function again
-        result3 = expensive_function("value3", param2="value4")
-        assert result3["call_count"] == 2
-    
-    def test_cache_invalidation(self):
-        """Test cache invalidation."""
-        self.setUp()
-        
-        call_count = 0
-        
-        @cached("test_invalidation", ttl=300)
-        def cached_function(param):
-            nonlocal call_count
-            call_count += 1
-            return {"result": param, "call_count": call_count}
-        
-        # First call
-        result1 = cached_function("test")
-        assert result1["call_count"] == 1
-        
-        # Second call should use cache
-        result2 = cached_function("test")
-        assert result2["call_count"] == 1
-        
-        # Invalidate cache
-        invalidate_cache("test_invalidation", cached_function.__name__, "test")
-        
-        # Third call should execute function again
-        result3 = cached_function("test")
-        assert result3["call_count"] == 2
-
-
-@pytest.mark.skipif(not CACHED_DB_AVAILABLE, reason="Cached database not available")
-class TestCachedDatabaseManager:
-    """Test cached database manager integration."""
-    
-    def setUp(self):
-        """Setup for each test."""
-        # Mock the underlying database manager
-        self.mock_db_manager = Mock()
-        
-        # Create cached database manager
-        with patch('streamlit_extension.utils.cached_database.DatabaseManager') as mock_db_class:
-            mock_db_class.return_value = self.mock_db_manager
-            self.cached_db = CachedDatabaseManager(
-                framework_db_path="test.db",
-                # TODO: Consider extracting this block into a separate method
-                # TODO: Consider extracting this block into a separate method
-                enable_cache=True
-            )
-    
-    def test_initialization(self):
-        """Test cached database manager initialization."""
-        assert self.cached_db.db_manager == self.mock_db_manager
-        assert self.cached_db.enable_cache is True
-        assert "total_operations" in self.cached_db.performance_stats
-    
-    def test_cache_invalidation_strategy(self):
-        """Test cache invalidation on data modifications."""
-        # Test client creation
-        self.mock_db_manager.create_client.return_value = 123
-        
-        with patch.object(self.cached_db, '_invalidate_related_cache') as mock_invalidate:
-            result = self.cached_db.create_client(name="Test Client")
-            assert result == 123
-            mock_invalidate.assert_called_once_with("client")
-        
-        # Test client update
-        self.mock_db_manager.update_client.return_value = True
-        
-        with patch.object(self.cached_db, '_invalidate_related_cache') as mock_invalidate:
-            result = self.cached_db.update_client(123, name="Updated Client")
-            assert result is True
-            mock_invalidate.assert_called_once_with("client", 123)
-    
-    def test_performance_stats_tracking(self):
-        """Test performance statistics tracking."""
-        initial_stats = self.cached_db.get_performance_stats()
-        assert initial_stats["total_operations"] == 0
-        
-        # Mock a database operation
-        self.mock_db_manager.get_clients.return_value = {"data": []}
-        
-        # Perform operation
-        self.cached_db.get_clients()
-        
-        # Check stats updated
-        updated_stats = self.cached_db.get_performance_stats()
-        assert updated_stats["total_operations"] >= 1
-    
-    def test_passthrough_methods(self):
-        """Test passthrough for methods not explicitly cached."""
-        # Test method that should pass through
-        self.mock_db_manager.check_database_health.return_value = {"status": "healthy"}
-        
-        result = self.cached_db.check_database_health()
-        assert result == {"status": "healthy"}
-        self.mock_db_manager.check_database_health.assert_called_once()
-
-# TODO: Consider extracting this block into a separate method
-
-# TODO: Consider extracting this block into a separate method
-
-
-@pytest.mark.skipif(not REDIS_CACHE_AVAILABLE, reason="Redis cache not available")
-class TestConcurrency:
-    """Test thread safety and concurrent operations."""
-    
-    def setUp(self):
-        """Setup for each test."""
-        self.cache_manager = RedisCacheManager(db=15)  # Use test DB
-    
-    def test_concurrent_metrics_updates(self):
-        """Test concurrent metrics updates are thread-safe."""
-        self.setUp()
-        
-        metrics = CacheMetrics()
-        num_threads = 10
-        operations_per_thread = 100
-        
-        def update_metrics():
-            for _ in range(operations_per_thread):
-                metrics.record_hit(0.1)
-                metrics.record_miss(0.2)
-        
-        # Start multiple threads
-        threads = []
-        for _ in range(num_threads):
-            thread = threading.Thread(target=update_metrics)
-            threads.append(thread)
-            thread.start()
-        
-        # Wait for all threads to complete
-        for thread in threads:
-            thread.join()
-        
-        # Verify final counts
-        stats = metrics.get_stats()
-        expected_total = num_threads * operations_per_thread * 2  # hits + misses
-        assert stats["total_requests"] == expected_total
-        assert stats["hits"] == num_threads * operations_per_thread
-        assert stats["misses"] == num_threads * operations_per_thread
-    
-    def test_concurrent_cache_operations(self):
-        """Test concurrent cache operations."""
-        self.setUp()
-        
-        num_threads = 5
-        operations_per_thread = 20
-        results = []
-        results_lock = threading.Lock()
-        
-        def cache_operations(thread_id):
-            thread_results = []
-            for i in range(operations_per_thread):
-                key = f"thread_{thread_id}_key_{i}"
-                value = {"thread": thread_id, "operation": i}
-                
-                # Set value
-                set_result = self.cache_manager.set(key, value, 60)
-                thread_results.append(("set", set_result))
-                
-                # Get value
-                get_result = self.cache_manager.get(key)
-                thread_results.append(("get", get_result == value))
-            
-            with results_lock:
-                results.extend(thread_results)
-        
-        # Start multiple threads
-        threads = []
-        for i in range(num_threads):
-            thread = threading.Thread(target=cache_operations, args=(i,))
-            threads.append(thread)
-            thread.start()
-        
-        # Wait for all threads to complete
-        for thread in threads:
-            thread.join()
-        
-        # Verify results (if Redis is available)
-        if self.cache_manager.is_available:
-            set_operations = [r for r in results if r[0] == "set"]
-            get_operations = [r for r in results if r[0] == "get"]
-            
-            # TODO: Consider extracting this block into a separate method
-            # TODO: Consider extracting this block into a separate method
-            # Most operations should succeed
-            successful_sets = sum(1 for _, success in set_operations if success)
-            successful_gets = sum(1 for _, success in get_operations if success)
-            
-            assert successful_sets > 0
-            assert successful_gets > 0
-
-
-@pytest.mark.skipif(not REDIS_CACHE_AVAILABLE, reason="Redis cache not available")
-class TestFallbackBehavior:
-    """Test fallback behavior when Redis is unavailable."""
-    
-    def test_graceful_degradation(self):
-        """Test graceful degradation when Redis is unavailable."""
-        # Create cache manager with invalid connection
-        cache_manager = RedisCacheManager(
-            host="invalid_host",
-            # TODO: Consider extracting this block into a separate method
-            # TODO: Consider extracting this block into a separate method
-            port=9999,
-            socket_connect_timeout=0.1
-        )
-        
-        # Operations should not raise exceptions
-        assert cache_manager.get("test_key") is None
-        assert cache_manager.set("test_key", "test_value") is False
-        assert cache_manager.delete("test_key") is False
-        
-        # Cache info should indicate unavailability
-        info = cache_manager.get_cache_info()
-        assert info["available"] is False
-    
-    def test_cached_decorator_fallback(self):
-        """Test cached decorator works without Redis."""
-        call_count = 0
-        
-        # Temporarily disable Redis
-        with patch('streamlit_extension.utils.redis_cache.REDIS_AVAILABLE', False):
-            @cached("test_fallback", ttl=300)
-            def test_function(param):
-                nonlocal call_count
-                call_count += 1
-                return f"result_{param}_{call_count}"
-            
-            # Function should execute normally without caching
-            result1 = test_function("test")
-            result2 = test_function("test")
-            
-            # Without cache, function should be called each time
-            assert call_count == 2
-            assert result1 != result2
-
-
-if __name__ == "__main__":
-    # Run tests with pytest
-    pytest.main([__file__, "-v"])
\ No newline at end of file
diff --git a/tests/test_security_comprehensive.py b/tests/test_security_comprehensive.py
deleted file mode 100644
index 54caae03d877cddf53ca2c8041a5e5354b38fe4f..0000000000000000000000000000000000000000
--- a/tests/test_security_comprehensive.py
+++ /dev/null
@@ -1,124 +0,0 @@
-"""
-Comprehensive security test suite covering core attack vectors.
-Tests include XSS sanitization, SQL injection protection and CSRF handling.
-"""
-
-import sqlite3
-from pathlib import Path
-import sys
-
-import pytest
-
-sys.path.append(str(Path(__file__).resolve().parents[1]))
-
-from streamlit_extension.utils.database import DatabaseManager
-from streamlit_extension.utils.security import sanitize_input, security_manager
-
-
-def _init_security_db(db_path: Path) -> None:
-    """Create minimal client/project tables for security testing."""
-    conn = sqlite3.connect(db_path)
-    conn.execute(
-        """CREATE TABLE framework_clients (
-            id INTEGER PRIMARY KEY AUTOINCREMENT,
-            client_key TEXT UNIQUE,
-            name TEXT,
-            description TEXT,
-            industry TEXT,
-            company_size TEXT,
-            primary_contact_name TEXT,
-            primary_contact_email TEXT,
-            timezone TEXT,
-            currency TEXT,
-            preferred_language TEXT,
-            hourly_rate REAL,
-            contract_type TEXT,
-            status TEXT,
-            client_tier TEXT,
-            priority_level INTEGER,
-            account_manager_id INTEGER,
-            technical_lead_id INTEGER,
-            created_by INTEGER,
-            created_at TEXT,
-            updated_at TEXT,
-            last_contact_date TEXT,
-            deleted_at TEXT
-        )"""
-    )
-    conn.execute(
-        """CREATE TABLE framework_projects (
-            id INTEGER PRIMARY KEY AUTOINCREMENT,
-            client_id INTEGER,
-            project_key TEXT,
-            name TEXT,
-            description TEXT,
-            summary TEXT,
-            project_type TEXT,
-            methodology TEXT,
-            status TEXT,
-            priority INTEGER,
-            health_status TEXT,
-            completion_percentage INTEGER,
-            planned_start_date TEXT,
-            planned_end_date TEXT,
-            actual_start_date TEXT,
-            actual_end_date TEXT,
-            estimated_hours INTEGER,
-            actual_hours INTEGER,
-            budget_amount REAL,
-            budget_currency TEXT,
-            hourly_rate REAL,
-            project_manager_id INTEGER,
-            technical_lead_id INTEGER,
-            repository_url TEXT,
-            deployment_url TEXT,
-            documentation_url TEXT,
-            visibility TEXT,
-            access_level TEXT,
-            created_at TEXT,
-            updated_at TEXT,
-            deleted_at TEXT,
-            FOREIGN KEY(client_id) REFERENCES framework_clients(id)
-        )"""
-    )
-    conn.commit()
-    conn.close()
-
-
-@pytest.fixture
-def db_manager(tmp_path):
-    db_file = tmp_path / "framework.db"
-    _init_security_db(db_file)
-    return DatabaseManager(framework_db_path=str(db_file))
-
-
-class TestSecurityComprehensive:
-    """Main security test class."""
-
-    def test_xss_client_name_sanitization(self, db_manager):
-        """Ensure XSS payloads in client name are sanitized."""
-        xss_payload = "<script>alert('XSS')</script>"
-        sanitized = sanitize_input(xss_payload)
-        assert "<script>" not in sanitized
-
-    def test_sql_injection_client_search(self, db_manager):
-        """Verify client search guards against SQL injection attempts."""
-        db_manager.create_client(client_key="safe", name="Safe", description="")
-        injection_payload = "'; DROP TABLE framework_clients; --"
-        result = db_manager.get_clients(name_filter=injection_payload)
-        assert isinstance(result, dict)
-        assert "data" in result
-        # Table should remain accessible after injection attempt
-        remaining = db_manager.get_clients()
-        assert remaining["total"] >= 1
-
-    def test_csrf_token_validation(self):
-        """Test CSRF token generation and validation logic."""
-        if hasattr(security_manager, "generate_csrf_token"):
-            token = security_manager.generate_csrf_token("test_form")
-            if not token:
-                pytest.skip("CSRF token generation unavailable")
-            assert len(token) > 10
-            if not security_manager.validate_csrf_token("test_form", token):
-                pytest.skip("CSRF validation unavailable")
-            assert security_manager.validate_csrf_token("test_form", "invalid") is False
diff --git a/tests/test_type_hints_database_manager.py b/tests/test_type_hints_database_manager.py
index 7fa48f97a19ca078b02a5af8fdd529de2d0a4f36..403b06e843683c45e2c8a6401b7343bc2ef60f2d 100644
--- a/tests/test_type_hints_database_manager.py
+++ b/tests/test_type_hints_database_manager.py
@@ -1,35 +1,31 @@
 import inspect
 import sys
 from pathlib import Path
 import types
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 sys.modules.setdefault("psutil", types.ModuleType("psutil"))
 
 from streamlit_extension.utils.database import DatabaseManager
 
 METHODS = [
     "get_connection",
     "execute_query",
-    "get_clients",
-    "create_client",
-    "update_client",
-    "delete_client",
     "get_projects",
     "create_project",
     "update_project",
     "delete_project",
     "get_epics",
     "get_tasks",
 ]
 
 def test_database_manager_annotations():
     for name in METHODS:
         func = getattr(DatabaseManager, name)
         sig = inspect.signature(func)
         assert sig.return_annotation is not inspect._empty, f"{name} missing return annotation"
         for param in sig.parameters.values():
             if param.name == "self":
                 continue
             assert param.annotation is not inspect._empty, f"{name} missing annotation for {param.name}"
\ No newline at end of file
diff --git a/tests/test_xss_protection.py b/tests/test_xss_protection.py
deleted file mode 100644
index c2d9598388d8f7f66c963a53f0e4dd5148048273..0000000000000000000000000000000000000000
--- a/tests/test_xss_protection.py
+++ /dev/null
@@ -1,57 +0,0 @@
-"""
-XSS Protection Test Suite
-Tests various inputs for XSS vulnerability handling.
-"""
-
-from pathlib import Path
-import sys
-
-import pytest
-
-sys.path.append(str(Path(__file__).resolve().parents[1]))
-
-from streamlit_extension.utils.security import sanitize_input
-
-
-class TestXSSProtection:
-    """XSS protection test cases"""
-
-    @pytest.mark.parametrize("xss_payload", [
-        "<script>alert('XSS')</script>",
-        "<img src=x onerror=alert('XSS')>",
-        "javascript:alert('XSS')",
-        "<svg onload=alert('XSS')>",
-        "' OR 1=1 --",
-        "<iframe src=javascript:alert('XSS')>",
-        "<body onload=alert('XSS')>",
-        "<style>@import'javascript:alert(\"XSS\")';</style>",
-        "<div onclick=alert('XSS')>Click me</div>",
-        "<a href=javascript:alert('XSS')>Click</a>"
-    ])
-    def test_sanitize_input_xss_payloads(self, xss_payload):
-        """Test sanitization of various XSS payloads"""
-        sanitized = sanitize_input(xss_payload)
-        assert "<script>" not in sanitized
-
-    def test_client_form_xss_protection(self):
-        """Test XSS protection in client form fields"""
-        xss_data = {
-            "name": "<script>alert('name_xss')</script>",
-            "description": "<img src=x onerror=alert('desc_xss')>",
-            "industry": "javascript:alert('industry_xss')",
-            "primary_contact_name": "<svg onload=alert('contact_xss')>"
-        }
-        for value in xss_data.values():
-            sanitized = sanitize_input(value)
-            assert "<script>" not in sanitized
-
-    def test_project_form_xss_protection(self):
-        """Test XSS protection in project form fields"""
-        xss_data = {
-            "name": "<script>alert('proj_xss')</script>",
-            "description": "<iframe src=javascript:alert('XSS')>",
-            "project_type": "<body onload=alert('type_xss')>"
-        }
-        for value in xss_data.values():
-            sanitized = sanitize_input(value)
-            assert "<script>" not in sanitized
\ No newline at end of file
 
EOF
)