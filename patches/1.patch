*** a/audit_system/agents/tdd_intelligent_workflow_agent.py
--- b/audit_system/agents/tdd_intelligent_workflow_agent.py
@@
 from __future__ import annotations
 
 import logging
 import time
 import sys
+import os
+import subprocess
 from pathlib import Path
 from typing import Dict, List, Any, Optional, Tuple
 from dataclasses import dataclass, field
 from enum import Enum
 from datetime import datetime, timedelta
 import argparse
@@
 CONTEXT_BASE_PATH = Path(__file__).parent.parent / "context"
 GUIDES_PATH = CONTEXT_BASE_PATH / "guides"
 WORKFLOWS_PATH = CONTEXT_BASE_PATH / "workflows" 
 NAVIGATION_PATH = CONTEXT_BASE_PATH / "navigation"
 
@@
     def _load_tdd_analysis_context(self) -> Dict[str, Any]:
         """
         üìö Load TDD and TDAH context for enhanced workflow optimization.
         """
         context = {
             "tdd_patterns": {},
             "tdah_guidelines": {},
             "workflow_optimizations": {},
             "architecture_patterns": {}
         }
 
         try:
             # Load TDD workflow patterns for real understanding
             tdd_patterns_path = WORKFLOWS_PATH / "TDD_WORKFLOW_PATTERNS.md"
             if tdd_patterns_path.exists():
                 with open(tdd_patterns_path, 'r', encoding='utf-8') as f:
                     context["tdd_patterns"]["content"] = f.read()
                     context["tdd_patterns"]["cycle_insights"] = [
                         "Red phase focuses on clear test definitions and failure analysis",
                         "Green phase emphasizes minimal implementation and quick success",
                         "Refactor phase balances code quality with test preservation",
                         "Cycle efficiency depends on proper phase separation and focus"
                     ]
                 self.logger.info("‚úÖ Loaded TDD workflow patterns for real cycle analysis")
 
             # Load TDAH optimization guidelines for workflow management
             tdah_guide_path = WORKFLOWS_PATH / "TDAH_OPTIMIZATION_GUIDE.md"
             if tdah_guide_path.exists():
                 with open(tdah_guide_path, 'r', encoding='utf-8') as f:
                     context["tdah_guidelines"]["content"] = f.read()
                     context["tdah_guidelines"]["workflow_principles"] = [
                         "Break complex TDD cycles into 15-25 minute focused sessions",
                         "Provide immediate feedback on each cycle completion",
                         "Use visual progress indicators for sustained motivation",
                         "Allow flexible session timing based on energy levels",
                         "Implement gentle interruption recovery for hyperfocus protection"
                     ]
                 self.logger.info("‚úÖ Loaded TDAH optimization guidelines for workflow management")
 
-            # Load system architecture for TDD strategy adaptation
-            status_path = NAVIGATION_PATH / "STATUS.md"
-            if status_path.exists():
-                with open(status_path, 'r', encoding='utf-8') as f:
-                    context["architecture_patterns"]["system_status"] = f.read()
-                self.logger.info("‚úÖ Loaded system architecture for TDD strategy context")
+            # Load system architecture for TDD strategy adaptation
+            # Prefer NAVIGATION_PATH/STATUS.md; fallback para raiz do projeto
+            status_candidates = [
+                NAVIGATION_PATH / "STATUS.md",
+                self.project_root / "STATUS.md"
+            ]
+            for status_path in status_candidates:
+                if status_path.exists():
+                    with open(status_path, 'r', encoding='utf-8') as f:
+                        context["architecture_patterns"]["system_status"] = f.read()
+                    self.logger.info("‚úÖ Loaded system architecture for TDD strategy context: %s", status_path)
+                    break
 
             self.logger.info("üìö TDD analysis context loaded successfully with enhanced patterns")
 
         except Exception as e:
             self.logger.warning(f"‚ö†Ô∏è Error loading TDD analysis context: {e}")
 
         return context
@@
     def start_tdd_workflow_session(
         self, 
         target_file: str, 
         initial_phase: TDDPhase,
         user_energy_level: TDAHEnergyLevel,
         focus_session_minutes: Optional[int] = None
     ) -> TDDWorkflowSession:
         """Start a new TDD workflow session with TDAH optimization."""
 
         session_id = f"tdd_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
         focus_minutes = focus_session_minutes or self.default_focus_minutes
 
         # Analyze the target file
         self.logger.info("Analyzing %s for TDD workflow optimization", target_file)
+        # rate limiting guard para opera√ß√£o pesada
+        self._rl_guard(self.real_llm_config.get("tdd_phase_detection_tokens", 1500), "tdd_phase_analysis")
         semantic_analysis = self.code_agent.analyze_file_intelligently(target_file)
         tdd_analysis = self._analyze_tdd_specific_aspects(semantic_analysis, target_file)
 
         # Create TDAH-optimized tasks
         tasks = self._create_tdah_optimized_tasks(
             semantic_analysis, tdd_analysis, initial_phase, user_energy_level
         )
@@
         try:
             while session.tasks_remaining:
                 # Get next task based on energy level and complexity
                 next_task = self._select_next_task(session)
 
                 if not next_task:
                     self.logger.info("No suitable tasks for current energy level")
                     break
 
                 # Execute task with TDAH support
                 task_result = self._execute_tdah_optimized_task(session, next_task)
 
                 if task_result["success"]:
                     session.tasks_completed.append(next_task)
                     session.tasks_remaining.remove(next_task)
                     results["tasks_completed"] += 1
 
                     # TDAH encouragement
                     if self.tdah_mode:
                         self._provide_tdah_encouragement(next_task, task_result)
 
                 # Check for focus session boundaries
                 if self._should_suggest_break(session):
                     break_result = self._handle_focus_break(session)
                     results["tdah_support_used"].append(break_result)
 
                 # Check for phase transitions
                 phase_change = self._check_tdd_phase_transition(session)
                 if phase_change:
                     results["phase_transitions"].append(phase_change)
 
             # Final analysis
+            self._rl_guard(self.real_llm_config.get("progress_assessment_tokens", 1000), "progress_assessment")
             final_analysis = self.code_agent.analyze_file_intelligently(session.target_file)
             results["final_quality_score"] = final_analysis.semantic_quality_score
             results["quality_improvements"] = session.quality_improvements_applied
@@
     def _determine_tdd_phase(self, analysis: FileSemanticAnalysis, file_path: str) -> TDDPhase:
         """Determine current TDD phase based on analysis."""
-        # Simplified logic - in practice would analyze test files and implementation
-        if "test" in file_path:
+        # Melhoria: considera padr√µes de teste comuns
+        lower = file_path.lower()
+        if any(pat in lower for pat in ("/tests/", "\\tests\\", "test_", "_test.py")):
             return TDDPhase.RED
         elif analysis.semantic_quality_score < 70:
             return TDDPhase.REFACTOR
         else:
             return TDDPhase.GREEN
@@
     def _check_for_failing_tests(self, file_path: str) -> bool:
         """Check for failing tests using pytest execution."""
         try:
             # Find related test files
-            test_files = self._find_test_files_for_module(file_path)
+            test_files = self._find_test_files_for_module(file_path)
             if not test_files:
                 return False  # No tests = no failing tests
 
-            # Run pytest on test files and check for failures
-            for test_file in test_files:
-                if not os.path.exists(test_file):
-                    continue
-                    
-                result = subprocess.run([
-                    'python', '-m', 'pytest', test_file, '--tb=no', '-q', '--disable-warnings'
-                ], capture_output=True, text=True, timeout=30, cwd=self.project_root)
-                
-                # pytest returns >0 if there are failures
-                if result.returncode > 0:
-                    self.logger.info(f"Found failing tests in {test_file}")
-                    return True
-
-            return False  # All tests passed
+            # Executa em lote para reduzir overhead
+            self._rl_guard(600, "pytest_check_failing")
+            cmd = ['python', '-m', 'pytest', *test_files, '--tb=no', '-q', '--disable-warnings']
+            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60, cwd=self.project_root)
+            if result.returncode > 0:
+                self.logger.info("Found failing tests in related files: %s", ", ".join(test_files))
+                return True
+            return False
 
         except Exception as e:
             self.logger.warning(f"Failed to check for failing tests in {file_path}: {e}")
             return False
 
     def _check_for_passing_tests(self, file_path: str) -> bool:
         """Check for passing tests using pytest execution."""
         try:
             # Find related test files
-            test_files = self._find_test_files_for_module(file_path)
+            test_files = self._find_test_files_for_module(file_path)
             if not test_files:
                 return False  # No tests = no passing tests
 
-            # Run pytest on test files and check if all pass
-            all_passing = True
-            tests_found = False
-            
-            for test_file in test_files:
-                if not os.path.exists(test_file):
-                    continue
-                    
-                tests_found = True
-                result = subprocess.run([
-                    'python', '-m', 'pytest', test_file, '--tb=no', '-q', '--disable-warnings'
-                ], capture_output=True, text=True, timeout=30, cwd=self.project_root)
-                
-                # pytest returns 0 if all tests pass
-                if result.returncode != 0:
-                    self.logger.info(f"Tests failing in {test_file}")
-                    all_passing = False
-                    break
-            
-            return tests_found and all_passing
+            # Executa em lote e valida retorno 0
+            self._rl_guard(600, "pytest_check_passing")
+            cmd = ['python', '-m', 'pytest', *test_files, '--tb=no', '-q', '--disable-warnings']
+            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60, cwd=self.project_root)
+            return result.returncode == 0
 
         except Exception as e:
             self.logger.warning(f"Failed to check for passing tests in {file_path}: {e}")
             return False
@@
     def _generate_tdd_next_steps(
         self, 
         phase: TDDPhase, 
         coverage: float, 
         analysis: FileSemanticAnalysis
     ) -> List[str]:
         """Generate TDD-specific next steps."""
         steps = []
 
         if phase == TDDPhase.RED:
             steps.append("Write failing tests that specify desired behavior")
             steps.append("Ensure tests fail for the right reasons")
         elif phase == TDDPhase.GREEN:
             steps.append("Implement minimal code to make tests pass")
             steps.append("Avoid over-engineering in this phase")
         else:  # REFACTOR
             steps.extend([ref.description for ref in analysis.recommended_refactorings[:3]])
 
         return steps
@@
     def _provide_post_task_support(
         self, 
         task: TDAHOptimizedTask, 
         result: Dict[str, Any], 
         duration: timedelta
     ) -> None:
         """Provide post-task support and celebration."""
         print(f"\nüéâ Task completed: {task.description}")
         print(f"‚è±Ô∏è Actual time: {duration.total_seconds():.1f} seconds")
         print(f"üéØ {task.encouragement_message}")
 
         # Dopamine reward for TDAH
         if task.immediate_feedback_available:
             print("‚ú® Immediate feedback: Changes applied successfully!")
 
         print("\nüí™ Great job! Ready for the next challenge?\n")
+
+    def _find_test_files_for_module(self, file_path: str) -> List[str]:
+        """
+        Procura arquivos de teste relacionados ao m√≥dulo alvo.
+        Heur√≠stica:
+          - tests/test_<module>.py
+          - tests/**/test_<module>.py
+          - tests/**/<module>_test.py
+          - Se <module> estiver em subpastas, tenta os nomes-base.
+        """
+        project = Path(self.project_root)
+        tests_dir = project / "tests"
+        if not tests_dir.exists():
+            return []
+
+        target = Path(file_path)
+        module_stem = target.stem  # nome do arquivo sem extens√£o
+        candidates: List[Path] = []
+
+        # Padr√µes comuns
+        patterns = [
+            f"test_{module_stem}.py",
+            f"{module_stem}_test.py",
+        ]
+        # Busca direta em tests/
+        for pat in patterns:
+            for p in tests_dir.rglob(pat):
+                candidates.append(p)
+
+        # Se nada encontrado, fallback: todos testes que mencionem o stem no nome
+        if not candidates:
+            for p in tests_dir.rglob("test_*.py"):
+                if module_stem in p.name:
+                    candidates.append(p)
+
+        # Dedup e somente existentes
+        uniq = []
+        seen = set()
+        for p in candidates:
+            if p.exists():
+                s = str(p.resolve())
+                if s not in seen:
+                    uniq.append(s)
+                    seen.add(s)
+        return uniq
@@
     def _run_test_task(self, file_path: str, task: TDAHOptimizedTask) -> Dict[str, Any]:
-        """Run tests for a specific task."""
-        return {"type": "test", "passed": True}
+        """Run tests for a specific task."""
+        test_files = self._find_test_files_for_module(file_path)
+        if not test_files:
+            self.logger.info("No related tests found for %s", file_path)
+            return {"type": "test", "passed": False, "reason": "no_tests_found"}
+        self._rl_guard(800, "pytest_run_task")
+        cmd = ['python', '-m', 'pytest', *test_files, '--tb=short', '-q', '--disable-warnings']
+        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120, cwd=self.project_root)
+        passed = (result.returncode == 0)
+        if not passed:
+            self.logger.info("Tests failed for %s\nstdout:\n%s\nstderr:\n%s", file_path, result.stdout, result.stderr)
+        return {"type": "test", "passed": passed, "files": test_files}
