 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/audit_system/__init__.py b/audit_system/__init__.py
index a663c7935ade5751f52b67942b6263a2512411e3..27af3c04215dac1722fc10757e9bdbd742a61dec 100644
--- a/audit_system/__init__.py
+++ b/audit_system/__init__.py
@@ -4,42 +4,63 @@
 üõ°Ô∏è Audit System - Enterprise Code Analysis and Intelligent Refactoring
 
 Sistema completo de auditoria de c√≥digo com agentes de IA especializados para:
 - An√°lise sem√¢ntica avan√ßada de c√≥digo
 - Detec√ß√£o e refatora√ß√£o de god codes
 - Otimiza√ß√µes inteligentes de performance
 - Coordena√ß√£o segura de modifica√ß√µes em arquivos
 - Workflow TDD com recursos TDAH
 
 Arquitetura:
 - agents/: Agentes especializados de an√°lise e refatora√ß√£o
 - coordination/: Meta-agente e coordena√ß√£o de arquivos
 - core/: Sistema base de auditoria
 - cli/: Interface de linha de comando
 - utils/: Utilit√°rios compartilhados
 
 Uso:
     from audit_system.coordination.meta_agent import MetaAgent
     from audit_system.agents.god_code_refactoring_agent import GodCodeRefactoringAgent
 """
 
 __version__ = "1.0.0"
 __author__ = "Claude + David"
 
 # Main exports for easy access
-from audit_system.coordination.meta_agent import MetaAgent, TaskType, run_meta_agent_analysis
-from audit_system.agents.intelligent_code_agent import IntelligentCodeAgent, AnalysisDepth, SemanticMode
-from audit_system.agents.god_code_refactoring_agent import GodCodeRefactoringAgent, run_god_code_analysis
-from audit_system.coordination.file_coordination_manager import FileCoordinationManager, get_coordination_manager
+try:
+    from audit_system.coordination.meta_agent import (
+        MetaAgent,
+        TaskType,
+        run_meta_agent_analysis,
+    )
+    from audit_system.agents.intelligent_code_agent import (
+        IntelligentCodeAgent,
+        AnalysisDepth,
+        SemanticMode,
+    )
+    from audit_system.agents.god_code_refactoring_agent import (
+        GodCodeRefactoringAgent,
+        run_god_code_analysis,
+    )
+    from audit_system.coordination.file_coordination_manager import (
+        FileCoordinationManager,
+        get_coordination_manager,
+    )
+except Exception as e:  # pragma: no cover - optional deps may be missing
+    # Keep module importable even if optional dependencies (e.g., streamlit) are absent
+    MetaAgent = TaskType = run_meta_agent_analysis = None
+    IntelligentCodeAgent = AnalysisDepth = SemanticMode = None
+    GodCodeRefactoringAgent = run_god_code_analysis = None
+    FileCoordinationManager = get_coordination_manager = None
 
 __all__ = [
     "MetaAgent",
     "TaskType", 
     "run_meta_agent_analysis",
     "IntelligentCodeAgent",
     "AnalysisDepth",
     "SemanticMode", 
     "GodCodeRefactoringAgent",
     "run_god_code_analysis",
     "FileCoordinationManager",
     "get_coordination_manager"
 ]
\ No newline at end of file
diff --git a/audit_system/agents/__init__.py b/audit_system/agents/__init__.py
index 90e4cb95c390a8b07540988c941cbb9dbea5b8e8..5df8abc509e7a9175fe1e13130062f562ca2ff0d 100644
--- a/audit_system/agents/__init__.py
+++ b/audit_system/agents/__init__.py
@@ -1,61 +1,81 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 ü§ñ Audit System - Intelligent Agents
 
 Agentes especializados de an√°lise e refatora√ß√£o de c√≥digo:
 - IntelligentCodeAgent: An√°lise sem√¢ntica linha-por-linha com IA
 - GodCodeRefactoringAgent: Detec√ß√£o e refatora√ß√£o de god codes
 - IntelligentRefactoringEngine: Engine de refatora√ß√£o com 7 estrat√©gias
 - TDDIntelligentWorkflowAgent: Otimiza√ß√£o de workflow TDD com recursos TDAH
 """
 
-from audit_system.agents.intelligent_code_agent import (
-    IntelligentCodeAgent,
-    AnalysisDepth,
-    SemanticMode,
-    FileSemanticAnalysis,
-    IntelligentRefactoring
-)
+try:
+    from audit_system.agents.intelligent_code_agent import (
+        IntelligentCodeAgent,
+        AnalysisDepth,
+        SemanticMode,
+        FileSemanticAnalysis,
+        IntelligentRefactoring,
+    )
+except Exception:
+    (IntelligentCodeAgent,
+     AnalysisDepth,
+     SemanticMode,
+     FileSemanticAnalysis,
+     IntelligentRefactoring) = (None, None, None, None, None)
 
-from audit_system.agents.god_code_refactoring_agent import (
-    GodCodeRefactoringAgent,
-    run_god_code_analysis,
-    GodCodeDetection,
-    RefactoringStrategy,
-    RefactoringResult
-)
+try:
+    from audit_system.agents.god_code_refactoring_agent import (
+        GodCodeRefactoringAgent,
+        run_god_code_analysis,
+        GodCodeDetection,
+        RefactoringStrategy,
+        RefactoringResult,
+    )
+except Exception:
+    (GodCodeRefactoringAgent,
+     run_god_code_analysis,
+     GodCodeDetection,
+     RefactoringStrategy,
+     RefactoringResult) = (None, None, None, None, None)
 
-from audit_system.agents.intelligent_refactoring_engine import (
-    IntelligentRefactoringEngine
-)
+try:
+    from audit_system.agents.intelligent_refactoring_engine import (
+        IntelligentRefactoringEngine,
+    )
+except Exception:
+    IntelligentRefactoringEngine = None
 
-from audit_system.agents.tdd_intelligent_workflow_agent import (
-    TDDIntelligentWorkflowAgent,
-    TDDPhase,
-    TDAHEnergyLevel
-)
+try:
+    from audit_system.agents.tdd_intelligent_workflow_agent import (
+        TDDIntelligentWorkflowAgent,
+        TDDPhase,
+        TDAHEnergyLevel,
+    )
+except Exception:
+    TDDIntelligentWorkflowAgent = TDDPhase = TDAHEnergyLevel = None
 
 __all__ = [
     # IntelligentCodeAgent
     "IntelligentCodeAgent",
     "AnalysisDepth", 
     "SemanticMode",
     "FileSemanticAnalysis",
     "IntelligentRefactoring",
     
     # GodCodeRefactoringAgent
     "GodCodeRefactoringAgent",
     "run_god_code_analysis",
     "GodCodeDetection",
     "RefactoringStrategy", 
     "RefactoringResult",
     
     # IntelligentRefactoringEngine
     "IntelligentRefactoringEngine",
     
     # TDDIntelligentWorkflowAgent
     "TDDIntelligentWorkflowAgent",
     "TDDPhase",
     "TDAHEnergyLevel"
 ]
\ No newline at end of file
diff --git a/audit_system/agents/static_rules_agent.py b/audit_system/agents/static_rules_agent.py
new file mode 100644
index 0000000000000000000000000000000000000000..7e23d6b8a518b8a156d16d81d5dda519efec710f
--- /dev/null
+++ b/audit_system/agents/static_rules_agent.py
@@ -0,0 +1,18 @@
+from __future__ import annotations
+
+from pathlib import Path
+from typing import List
+
+from audit_system.core.contracts import Agent, FileFinding
+
+
+class StaticRulesAgent(Agent):
+    name = "static_rules"
+
+    def analyze(self, path: Path, content: str) -> List[FileFinding]:
+        findings: List[FileFinding] = []
+        if "TODO" in content:
+            findings.append(FileFinding(path, "TODO_PRESENT", "LOW", "Encontrado TODO no arquivo"))
+        if len(content) > 200_000:
+            findings.append(FileFinding(path, "FILE_TOO_LARGE", "MEDIUM", "Arquivo muito grande para an√°lise eficiente"))
+        return findings
diff --git a/audit_system/coordination/orchestrator.py b/audit_system/coordination/orchestrator.py
new file mode 100644
index 0000000000000000000000000000000000000000..48bc955ca82d95ad88013dd59d3d062a639e9aa2
--- /dev/null
+++ b/audit_system/coordination/orchestrator.py
@@ -0,0 +1,59 @@
+from __future__ import annotations
+
+from pathlib import Path
+from typing import Sequence, List
+
+from audit_system.core.contracts import (
+    AuditPlanner,
+    Auditor,
+    FileFinding,
+    FileRepository,
+    SessionRepository,
+    Agent,
+)
+from audit_system.utils.resilience import retry
+
+
+class SystematicFileAuditOrchestrator(Auditor):
+    """
+    Classe fina que apenas coordena servi√ßos especializados (SRP).
+    """
+    def __init__(
+        self,
+        project_root: Path,
+        file_repo: FileRepository,
+        session_repo: SessionRepository,
+        planner: AuditPlanner,
+        agents: Sequence[Agent],
+    ) -> None:
+        self.root = project_root
+        self.file_repo = file_repo
+        self.session_repo = session_repo
+        self.planner = planner
+        self.agents = list(agents)
+
+    def _scan_candidates(self) -> List[Path]:
+        return self.file_repo.scan(self.root, patterns=["**/*.py"])
+
+    @retry((Exception,), attempts=3, base_delay=0.2, max_delay=1.0)
+    def _analyze_with_agent(self, agent: Agent, path: Path, content: str) -> List[FileFinding]:
+        return agent.analyze(path, content)
+
+    def run(self, targets: Sequence[Path] | None = None) -> List[FileFinding]:
+        run_id = self.session_repo.start_run(self.root)
+        try:
+            candidates = list(targets) if targets else self._scan_candidates()
+            selected = self.planner.select_targets(candidates)
+            all_findings: List[FileFinding] = []
+            for p in selected:
+                content = self.file_repo.read_text(p)
+                for agent in self.agents:
+                    findings = self._analyze_with_agent(agent, p, content)
+                    if findings:
+                        self.session_repo.save_findings(run_id, findings)
+                        all_findings.extend(findings)
+            self.session_repo.end_run(run_id, "OK")
+            return all_findings
+        except Exception:
+            self.session_repo.end_run(run_id, "FAILED")
+            raise
diff --git a/audit_system/core/__init__.py b/audit_system/core/__init__.py
index e9c102045e0f21563825a2272026b1b1f9b6ee6c..428e724db99ae8397bb7af7837bdd2709c3392a9 100644
--- a/audit_system/core/__init__.py
+++ b/audit_system/core/__init__.py
@@ -1,22 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 üèóÔ∏è Audit System - Core Components
 
 Componentes centrais do sistema de auditoria:
 - EnhancedSystematicFileAuditor: Sistema base de auditoria com sess√µes empresariais
 """
 
-from audit_system.core.systematic_file_auditor import (
-    EnhancedSystematicFileAuditor,
-    SetimaDataLoader,
-    EnterpriseSessionManager,
-    SmartTokenBudgetManager
-)
+try:
+    from audit_system.core.systematic_file_auditor import (
+        EnhancedSystematicFileAuditor,
+        SetimaDataLoader,
+        EnterpriseSessionManager,
+        SmartTokenBudgetManager,
+    )
+except Exception:
+    EnhancedSystematicFileAuditor = (
+        SetimaDataLoader
+    ) = EnterpriseSessionManager = SmartTokenBudgetManager = None
 
 __all__ = [
     "EnhancedSystematicFileAuditor",
-    "SetimaDataLoader", 
+    "SetimaDataLoader",
     "EnterpriseSessionManager",
-    "SmartTokenBudgetManager"
+    "SmartTokenBudgetManager",
 ]
\ No newline at end of file
diff --git a/audit_system/core/container.py b/audit_system/core/container.py
new file mode 100644
index 0000000000000000000000000000000000000000..341543c7428e97b369ca34111315d9b6914de427
--- /dev/null
+++ b/audit_system/core/container.py
@@ -0,0 +1,27 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+from audit_system.agents.static_rules_agent import StaticRulesAgent
+from audit_system.coordination.orchestrator import SystematicFileAuditOrchestrator
+from audit_system.core.contracts import Auditor
+from audit_system.services.file_repository import LocalFileRepository
+from audit_system.services.planner_simple import SimpleAuditPlanner
+from audit_system.services.session_sqlite import SQLiteSessionRepository
+
+
+def build_default_auditor(project_root: Path, db_path: Path | None = None) -> Auditor:
+    """
+    Constr√≥i um Auditor com implementa√ß√µes padr√£o (DI).
+    """
+    file_repo = LocalFileRepository(project_root)
+    session_repo = SQLiteSessionRepository(db_path or (project_root / ".audit" / "audit_sessions.db"))
+    planner = SimpleAuditPlanner()
+    agents = [StaticRulesAgent()]
+    return SystematicFileAuditOrchestrator(
+        project_root=project_root,
+        file_repo=file_repo,
+        session_repo=session_repo,
+        planner=planner,
+        agents=agents,
+    )
diff --git a/audit_system/core/contracts.py b/audit_system/core/contracts.py
new file mode 100644
index 0000000000000000000000000000000000000000..0f8d1b28c9bdf111852822ec6ba5d9cfd3a5b6f8
--- /dev/null
+++ b/audit_system/core/contracts.py
@@ -0,0 +1,44 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from pathlib import Path
+from typing import List, Protocol, Sequence, runtime_checkable
+
+
+@dataclass(frozen=True)
+class FileFinding:
+    path: Path
+    rule: str
+    severity: str  # "LOW" | "MEDIUM" | "HIGH"
+    message: str
+
+
+@runtime_checkable
+class Agent(Protocol):
+    name: str
+
+    def analyze(self, path: Path, content: str) -> List[FileFinding]: ...
+
+
+@runtime_checkable
+class FileRepository(Protocol):
+    def scan(self, root: Path, patterns: Sequence[str]) -> List[Path]: ...
+    def read_text(self, path: Path) -> str: ...
+    def write_text(self, path: Path, data: str) -> None: ...
+
+
+@runtime_checkable
+class SessionRepository(Protocol):
+    def start_run(self, root: Path) -> str: ...
+    def save_findings(self, run_id: str, findings: List[FileFinding]) -> None: ...
+    def end_run(self, run_id: str, status: str) -> None: ...
+
+
+@runtime_checkable
+class AuditPlanner(Protocol):
+    def select_targets(self, candidates: Sequence[Path]) -> List[Path]: ...
+
+
+@runtime_checkable
+class Auditor(Protocol):
+    def run(self, targets: Sequence[Path] | None = None) -> List[FileFinding]: ...
diff --git a/audit_system/services/__init__.py b/audit_system/services/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..78b7c2754570a07b1213c79bfa4f26ee2f0cb482
--- /dev/null
+++ b/audit_system/services/__init__.py
@@ -0,0 +1 @@
+"""Service layer implementations for audit system."""
diff --git a/audit_system/services/file_repository.py b/audit_system/services/file_repository.py
new file mode 100644
index 0000000000000000000000000000000000000000..d9d5c61f2e6ee96097ec17b836ff1b37dba997f8
--- /dev/null
+++ b/audit_system/services/file_repository.py
@@ -0,0 +1,29 @@
+from __future__ import annotations
+
+from pathlib import Path
+from typing import List, Sequence
+
+from audit_system.core.contracts import FileRepository
+from audit_system.utils.safe_io import read_text_secure, write_text_atomic_secure, safe_join
+
+
+class LocalFileRepository(FileRepository):
+    def __init__(self, project_root: Path) -> None:
+        self.root = project_root
+
+    def scan(self, root: Path, patterns: Sequence[str]) -> List[Path]:
+        base = root.resolve()
+        results: List[Path] = []
+        for pat in patterns:
+            # aceita "**/*.py", etc.
+            for p in base.glob(pat):
+                rp = safe_join(self.root, p)
+                if rp.is_file():
+                    results.append(rp)
+        return sorted(set(results))
+
+    def read_text(self, path: Path) -> str:
+        return read_text_secure(self.root, path)
+
+    def write_text(self, path: Path, data: str) -> None:
+        write_text_atomic_secure(self.root, path, data)
diff --git a/audit_system/services/planner_simple.py b/audit_system/services/planner_simple.py
new file mode 100644
index 0000000000000000000000000000000000000000..71fcfd798457133da12f010cc53d6c3a746c1067
--- /dev/null
+++ b/audit_system/services/planner_simple.py
@@ -0,0 +1,15 @@
+from __future__ import annotations
+
+from pathlib import Path
+from typing import List, Sequence
+
+from audit_system.core.contracts import AuditPlanner
+
+
+class SimpleAuditPlanner(AuditPlanner):
+    """
+    Seleciona todos os candidatos por padr√£o.
+    Pode evoluir para crit√©rios (diffs recentes, tamanho, heur√≠sticas AST, etc.).
+    """
+    def select_targets(self, candidates: Sequence[Path]) -> List[Path]:
+        return list(candidates)
diff --git a/audit_system/services/session_sqlite.py b/audit_system/services/session_sqlite.py
new file mode 100644
index 0000000000000000000000000000000000000000..d78d7d87066a4b64cc5f12659b838f520694f160
--- /dev/null
+++ b/audit_system/services/session_sqlite.py
@@ -0,0 +1,87 @@
+from __future__ import annotations
+
+import sqlite3
+import uuid
+from pathlib import Path
+from typing import List
+
+from audit_system.core.contracts import FileFinding, SessionRepository
+
+
+class SQLiteSessionRepository(SessionRepository):
+    """
+    Reposit√≥rio simples com WAL + foreign_keys e binds parametrizados.
+    """
+    def __init__(self, db_path: Path) -> None:
+        self.db_path = db_path
+        self.db_path.parent.mkdir(parents=True, exist_ok=True)
+        self._init_db()
+
+    def _connect(self) -> sqlite3.Connection:
+        conn = sqlite3.connect(self.db_path, timeout=30.0, isolation_level=None)
+        # Seguran√ßa/confiabilidade
+        conn.execute("PRAGMA foreign_keys = ON;")
+        conn.execute("PRAGMA journal_mode = WAL;")
+        conn.execute("PRAGMA synchronous = NORMAL;")
+        return conn
+
+    def _init_db(self) -> None:
+        conn = self._connect()
+        try:
+            conn.execute(
+                """
+            CREATE TABLE IF NOT EXISTS audit_runs (
+                id TEXT PRIMARY KEY,
+                root TEXT NOT NULL,
+                started_at DATETIME DEFAULT CURRENT_TIMESTAMP,
+                finished_at DATETIME,
+                status TEXT
+            );
+            """
+            )
+            conn.execute(
+                """
+            CREATE TABLE IF NOT EXISTS audit_findings (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                run_id TEXT NOT NULL,
+                path TEXT NOT NULL,
+                rule TEXT NOT NULL,
+                severity TEXT NOT NULL,
+                message TEXT NOT NULL,
+                FOREIGN KEY(run_id) REFERENCES audit_runs(id) ON DELETE CASCADE
+            );
+            """
+            )
+        finally:
+            conn.close()
+
+    def start_run(self, root: Path) -> str:
+        run_id = str(uuid.uuid4())
+        conn = self._connect()
+        try:
+            conn.execute("INSERT INTO audit_runs (id, root, status) VALUES (?, ?, ?);", (run_id, str(root), "RUNNING"))
+        finally:
+            conn.close()
+        return run_id
+
+    def save_findings(self, run_id: str, findings: List[FileFinding]) -> None:
+        if not findings:
+            return
+        conn = self._connect()
+        try:
+            conn.executemany(
+                "INSERT INTO audit_findings (run_id, path, rule, severity, message) VALUES (?, ?, ?, ?, ?);",
+                [(run_id, str(f.path), f.rule, f.severity, f.message) for f in findings],
+            )
+        finally:
+            conn.close()
+
+    def end_run(self, run_id: str, status: str) -> None:
+        conn = self._connect()
+        try:
+            conn.execute(
+                "UPDATE audit_runs SET status = ?, finished_at = CURRENT_TIMESTAMP WHERE id = ?;",
+                (status, run_id),
+            )
+        finally:
+            conn.close()
diff --git a/audit_system/utils/resilience.py b/audit_system/utils/resilience.py
new file mode 100644
index 0000000000000000000000000000000000000000..b264177bb34d4a3dfbb57ead6fcb11d7612b8d42
--- /dev/null
+++ b/audit_system/utils/resilience.py
@@ -0,0 +1,29 @@
+from __future__ import annotations
+
+import random
+import time
+from typing import Callable, Type, Tuple
+
+
+def retry(
+    exceptions: Tuple[Type[BaseException], ...],
+    attempts: int = 3,
+    base_delay: float = 0.25,
+    max_delay: float = 2.0,
+) -> Callable:
+    """
+    Decorator simples de retry com backoff exponencial + jitter.
+    """
+    def decorator(fn: Callable):
+        def wrapper(*args, **kwargs):
+            delay = base_delay
+            for i in range(attempts):
+                try:
+                    return fn(*args, **kwargs)
+                except exceptions as e:
+                    if i == attempts - 1:
+                        raise
+                    time.sleep(min(max_delay, delay + random.random() * 0.1))
+                    delay *= 2
+        return wrapper
+    return decorator
diff --git a/audit_system/utils/safe_io.py b/audit_system/utils/safe_io.py
new file mode 100644
index 0000000000000000000000000000000000000000..7e2d24d0fbeaeb404da18c9ac593b2e673d48aaa
--- /dev/null
+++ b/audit_system/utils/safe_io.py
@@ -0,0 +1,40 @@
+from __future__ import annotations
+
+import os
+import tempfile
+from pathlib import Path
+from typing import Union
+
+
+class PathSecurityError(Exception):
+    pass
+
+
+def safe_join(base: Path, candidate: Path) -> Path:
+    base_r = base.resolve()
+    cand_r = (base / candidate).resolve() if not candidate.is_absolute() else candidate.resolve()
+    if not str(cand_r).startswith(str(base_r) + os.sep) and cand_r != base_r:
+        raise PathSecurityError(f"Traversal detected: {cand_r} not under {base_r}")
+    return cand_r
+
+
+def read_text_secure(base: Path, target: Union[str, Path], encoding: str = "utf-8") -> str:
+    p = safe_join(base, Path(target))
+    return p.read_text(encoding=encoding)
+
+
+def write_text_atomic_secure(
+    base: Path,
+    target: Union[str, Path],
+    data: str,
+    encoding: str = "utf-8",
+) -> None:
+    dest = safe_join(base, Path(target))
+    dest.parent.mkdir(parents=True, exist_ok=True)
+    # escreve em tmp no MESMO dir p/ atomic rename
+    with tempfile.NamedTemporaryFile("w", encoding=encoding, dir=str(dest.parent), delete=False) as tmp:
+        tmp.write(data)
+        tmp.flush()
+        os.fsync(tmp.fileno())
+        tmp_path = Path(tmp.name)
+    tmp_path.replace(dest)
diff --git a/scripts/automated_audit/systematic_file_auditor.py b/scripts/automated_audit/systematic_file_auditor.py
index 2ec78530de046498c86baa91d50329039fcf65d1..168c9a81bb1b947fb79b5155239c5b165c33b5f0 100644
--- a/scripts/automated_audit/systematic_file_auditor.py
+++ b/scripts/automated_audit/systematic_file_auditor.py
@@ -1,3537 +1,48 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
-ü§ñ Systematic File Auditor - S√©tima Camada de Auditoria Automatizada
-
-Sistema automatizado que percorre TODOS os arquivos do projeto sistematicamente,
-analisando linha por linha, otimizando quando poss√≠vel e documentando tudo,
-com rastreamento no banco de dados para resili√™ncia completa.
-
-Uso:
-    python systematic_file_auditor.py [--resume] [--dry-run] [--max-files N]
-                                      [--wave {1,2,3,4,all}]
-                                      [--risk-category {LOW,MEDIUM,HIGH,CRITICAL}]
-                                      [--validate-only] [-v/--verbose]
+CLI do auditor sistem√°tico p√≥s-refactor:
+- Usa DI container para montar orquestrador e servi√ßos
+- Mant√©m compatibilidade via comando √∫nico
 """
-
 from __future__ import annotations
 
-import time
-import sys
-import os
-import logging
 import argparse
 from pathlib import Path
-from typing import List, Dict, Any, Optional, Set, Tuple
-from datetime import datetime
-from dataclasses import dataclass
-from enum import Enum
-import ast
-import json
-import sqlite3
-
-# ---------------------------------------------------------------------------
-# Integra√ß√µes S√©tima Camada (graceful degradation)
-# ---------------------------------------------------------------------------
-try:
-    from .context_validator import ContextValidator
-    from .integration_tester import IntegrationTester
-    from .rollback_context_changes import RollbackManager
-except ImportError:
-    try:
-        from context_validator import ContextValidator  # type: ignore
-        from integration_tester import IntegrationTester  # type: ignore
-        from rollback_context_changes import RollbackManager  # type: ignore
-    except ImportError:
-        ContextValidator = None  # type: ignore
-        IntegrationTester = None  # type: ignore
-        RollbackManager = None  # type: ignore
-
-# ---------------------------------------------------------------------------
-# Intelligent agents integration (graceful degradation)
-# ---------------------------------------------------------------------------
-try:
-    from .intelligent_code_agent import IntelligentCodeAgent, AnalysisDepth, SemanticMode
-    from .intelligent_refactoring_engine import IntelligentRefactoringEngine  
-    from .tdd_intelligent_workflow_agent import TDDIntelligentWorkflowAgent
-    from .meta_agent import MetaAgent, TaskType, run_meta_agent_analysis
-    INTELLIGENT_AGENTS_AVAILABLE = True
-except ImportError:
-    try:
-        from intelligent_code_agent import IntelligentCodeAgent, AnalysisDepth, SemanticMode  # type: ignore
-        from intelligent_refactoring_engine import IntelligentRefactoringEngine  # type: ignore
-        from tdd_intelligent_workflow_agent import TDDIntelligentWorkflowAgent  # type: ignore
-        from meta_agent import MetaAgent, TaskType, run_meta_agent_analysis  # type: ignore
-        INTELLIGENT_AGENTS_AVAILABLE = True
-    except ImportError:
-        # Graceful degradation
-        IntelligentCodeAgent = None
-        IntelligentRefactoringEngine = None 
-        TDDIntelligentWorkflowAgent = None
-        MetaAgent = None
-        TaskType = None
-        run_meta_agent_analysis = None
-        AnalysisDepth = None
-        SemanticMode = None
-        INTELLIGENT_AGENTS_AVAILABLE = False
-
-# ---------------------------------------------------------------------------
-# Caminho do projeto e PYTHONPATH
-# ---------------------------------------------------------------------------
-project_root = Path(__file__).resolve().parent.parent.parent
-sys.path.insert(0, str(project_root))
-
-# ---------------------------------------------------------------------------
-# Banco de dados: API legada + API modular (preferir modular quando existir)
-# ---------------------------------------------------------------------------
-from streamlit_extension.utils.database import DatabaseManager  # type: ignore
-
-try:
-    # API modular (prefer√≠vel pela performance)
-    from streamlit_extension.database import get_connection as get_modular_connection  # type: ignore
-except Exception:  # pragma: no cover
-    get_modular_connection = None  # type: ignore
-
-
-# =============================================================================
-# S√©tima Camada - Dados de Apoio
-# =============================================================================
-class SetimaDataLoader:
-    """Carrega dados de integra√ß√£o da S√©tima Camada a partir de artefatos/relat√≥rios."""
-
-    def __init__(self, audit_dir: Path):
-        self.audit_dir = audit_dir
-        self.logger = logging.getLogger(f"{__name__}.SetimaDataLoader")
-
-        self.dependency_waves = self._load_dependency_data()
-        self.risk_scores = self._load_risk_data()
-        self.pattern_templates = self._load_pattern_data()
-        self.good_patterns = self._load_good_patterns()
-        self.anti_patterns = self._load_anti_patterns()
-
-        self.logger.info(
-            "S√©tima Camada carregada: %d arquivos com pontua√ß√£o de risco",
-            len(self.risk_scores),
-        )
-
-    def _load_dependency_data(self) -> Dict[str, List[str]]:
-        # Em produ√ß√£o, faria parsing de um DEPENDENCY_GRAPH.md
-        return {
-            "WAVE_1_FOUNDATION": [
-                "tests/test_duration_calculator.py",
-                "tests/test_business_calendar.py",
-                "scripts/maintenance/database_maintenance.py",
-                "streamlit_extension/components/analytics_cards.py",
-                "streamlit_extension/utils/path_utils.py",
-                "streamlit_extension/utils/data_utils.py",
-                "streamlit_extension/pages/clients.py",
-                "streamlit_extension/endpoints/health.py",
-            ],
-            "WAVE_2_BUSINESS": [
-                "streamlit_extension/services/analytics_service.py",
-                "streamlit_extension/services/client_service.py",
-                "streamlit_extension/models/task_models.py",
-                "streamlit_extension/repos/tasks_repo.py",
-            ],
-            "WAVE_3_INTEGRATION": [
-                "streamlit_extension/utils/circuit_breaker.py",
-                "streamlit_extension/utils/metrics_collector.py",
-                "duration_system/json_security.py",
-            ],
-            "WAVE_4_CRITICAL": [
-                "streamlit_extension/database/connection.py",
-                "streamlit_extension/streamlit_app.py",
-                "streamlit_extension/middleware/rate_limiting/middleware.py",
-                "streamlit_extension/database/queries.py",
-                "streamlit_extension/database/seed.py",
-                "streamlit_extension/middleware/rate_limiting/core.py",
-                "streamlit_extension/database/schema.py",
-                "streamlit_extension/database/health.py",
-            ],
-        }
-
-    def _load_risk_data(self) -> Dict[str, int]:
-        # Em produ√ß√£o, faria parsing de RISK_ASSESSMENT_MAP.md
-        return {
-            # CRITICAL (106+)
-            "streamlit_extension/database/connection.py": 165,
-            "streamlit_extension/streamlit_app.py": 150,
-            "streamlit_extension/middleware/rate_limiting/middleware.py": 145,
-            "streamlit_extension/database/queries.py": 140,
-            "streamlit_extension/database/seed.py": 135,
-            "streamlit_extension/middleware/rate_limiting/core.py": 130,
-            "streamlit_extension/database/schema.py": 120,
-            "streamlit_extension/database/health.py": 110,
-            # HIGH (71-105)
-            "streamlit_extension/utils/circuit_breaker.py": 90,
-            "streamlit_extension/utils/metrics_collector.py": 85,
-            "duration_system/json_security.py": 88,
-            # MEDIUM (36-70)
-            "streamlit_extension/services/analytics_service.py": 48,
-            "streamlit_extension/services/client_service.py": 52,
-            "streamlit_extension/models/task_models.py": 45,
-            # LOW (0-35)
-            "streamlit_extension/components/analytics_cards.py": 20,
-            "streamlit_extension/utils/path_utils.py": 15,
-            "streamlit_extension/pages/clients.py": 25,
-            "tests/test_duration_calculator.py": 8,
-        }
-
-    def _load_pattern_data(self) -> Dict[str, Any]:
-        return {
-            "import_centralization": {
-                "description": "Centraliza imports para reduzir import hell",
-                "risk_level_safe": ["LOW", "MEDIUM"],
-                "transformation": "from streamlit_extension.utils.import_manager import get_safe_import",
-                "validation_required": True,
-            },
-            "exception_swallowing_fix": {
-                "description": "Evita except gen√©ricos/bare",
-                "risk_level_safe": ["LOW", "MEDIUM", "HIGH"],
-                "transformation": 'except {SpecificException} as e:\n    logger.warning("Operation failed: %s", str(e))\n    return None',
-                "validation_required": False,
-            },
-            "method_decomposition": {
-                "description": "Quebra m√©todos gigantes em menores",
-                "risk_level_safe": ["LOW"],
-                "transformation": "MANUAL_REVIEW_REQUIRED",
-                "validation_required": True,
-            },
-        }
-
-    def _load_good_patterns(self) -> Dict[str, Any]:
-        return {
-            "graceful_import": {
-                "description": "Import seguro com fallback",
-                "template": "try:\n    import {module}\nexcept ImportError:\n    {module} = None",
-                "preserve": True,
-                "files_found": 8,
-            },
-            "structured_logging": {
-                "description": "Logging estruturado com extra/context",
-                "template": 'logger.info("Op: %s", operation, extra={"context": ctx})',
-                "preserve": True,
-                "files_found": 6,
-            },
-        }
-
-    def _load_anti_patterns(self) -> Dict[str, Any]:
-        return {
-            "import_hell": {
-                "description": "Imports din√¢micos complexos c/ estado global",
-                "severity": "HIGH",
-                "fix_template": "import_centralization",
-                "files_found": 3,
-                "auto_fix": True,
-            },
-            "god_method": {
-                "description": "M√©todos >50 linhas, multi-responsabilidade",
-                "severity": "MEDIUM",
-                "fix_template": "method_decomposition",
-                "files_found": 4,
-                "auto_fix": False,
-            },
-            "exception_swallowing": {
-                "description": "Except gen√©ricos que escondem erros",
-                "severity": "HIGH",
-                "fix_template": "exception_swallowing_fix",
-                "files_found": 15,
-                "auto_fix": True,
-            },
-        }
-
-    def get_file_wave(self, file_path: str) -> str:
-        for wave, files in self.dependency_waves.items():
-            if file_path in files:
-                return wave
-        return "WAVE_1_FOUNDATION"
-
-    def get_file_risk_score(self, file_path: str) -> int:
-        return self.risk_scores.get(file_path, 25)
-
-    def get_file_risk_category(self, file_path: str) -> str:
-        score = self.get_file_risk_score(file_path)
-        if score >= 106:
-            return "CRITICAL"
-        if score >= 71:
-            return "HIGH"
-        if score >= 36:
-            return "MEDIUM"
-        return "LOW"
-
-
-# =============================================================================
-# Enums e modelos
-# =============================================================================
-class AuditStatus(Enum):
-    PENDING = "pending"
-    IN_PROGRESS = "in_progress"
-    COMPLETED = "completed"
-    FAILED = "failed"
-    SKIPPED = "skipped"
-
-
-class SessionStatus(Enum):
-    ACTIVE = "active"
-    PAUSED = "paused"
-    COMPLETED = "completed"
-    FAILED = "failed"
-
-
-class RiskCategory(Enum):
-    LOW = "LOW"
-    MEDIUM = "MEDIUM"
-    HIGH = "HIGH"
-    CRITICAL = "CRITICAL"
-
-
-class ModificationWave(Enum):
-    WAVE_1_FOUNDATION = "WAVE_1_FOUNDATION"
-    WAVE_2_BUSINESS = "WAVE_2_BUSINESS"
-    WAVE_3_INTEGRATION = "WAVE_3_INTEGRATION"
-    WAVE_4_CRITICAL = "WAVE_4_CRITICAL"
-
-
-@dataclass
-class FileAuditResult:
-    file_path: str
-    lines_analyzed: int
-    issues_found: int
-    optimizations_applied: int
-    tokens_used: int
-    changes_summary: str
-    syntax_valid: bool
-    backup_created: bool
-    # S√©tima Camada
-    context_quality: float = 0.0
-    risk_score: int = 0
-    risk_category: str = "LOW"
-    modification_wave: str = "WAVE_1_FOUNDATION"
-    patterns_found: List[str] | None = None
-    good_patterns_preserved: List[str] | None = None
-    anti_patterns_fixed: List[str] | None = None
-    integration_tests_passed: bool = True
-    rollback_available: bool = False
-
-    def __post_init__(self) -> None:
-        if self.patterns_found is None:
-            self.patterns_found = []
-        if self.good_patterns_preserved is None:
-            self.good_patterns_preserved = []
-        if self.anti_patterns_fixed is None:
-            self.anti_patterns_fixed = []
-
-
-@dataclass
-class AuditSession:
-    session_id: int
-    session_start: datetime
-    current_file_index: int
-    total_files: int
-    files_completed: int
-    total_tokens_used: int
-    session_status: SessionStatus
-    estimated_completion: Optional[datetime] = None
-
-
-# =============================================================================
-# Auditor principal
-# =============================================================================
-class EnhancedSystematicFileAuditor:
-    """Auditor com integra√ß√£o S√©tima Camada e salvaguardas enterprise."""
-
-    def __init__(self, project_root: Path, audit_dir: Path, *, dry_run: bool = False, validate_only: bool = False):
-        self.project_root = project_root
-        self.audit_dir = audit_dir
-        self.dry_run = dry_run
-        self.validate_only = validate_only
-        self.logger = logging.getLogger(f"{__name__}.EnhancedSystematicFileAuditor")
-
-        # N√∫cleo
-        self.db_manager = DatabaseManager()
-        self.session_manager = EnterpriseSessionManager(self.db_manager, self.project_root)
-        self.token_manager = SmartTokenBudgetManager()
-        self.file_manager = FileListManager(project_root)
-
-        # S√©tima Camada
-        self.context_validator = ContextValidator() if ContextValidator else None  # type: ignore
-        self.integration_tester = IntegrationTester() if IntegrationTester else None  # type: ignore
-        self.rollback_manager = RollbackManager() if RollbackManager else None  # type: ignore
-        self.setima_data = SetimaDataLoader(audit_dir)
-
-        self.setima_integration_available = all(
-            [
-                self.context_validator is not None,
-                self.integration_tester is not None,
-                self.rollback_manager is not None,
-            ]
-        )
-
-        # Initialize Intelligent Agents
-        self.intelligent_agents_available = INTELLIGENT_AGENTS_AVAILABLE
-        self.intelligent_code_agent = None
-        self.refactoring_engine = None
-        self.tdd_workflow_agent = None
-        self.meta_agent = None
-        
-        if INTELLIGENT_AGENTS_AVAILABLE:
-            try:
-                self.intelligent_code_agent = IntelligentCodeAgent(
-                    project_root=self.project_root,
-                    analysis_depth=AnalysisDepth.ADVANCED,
-                    semantic_mode=SemanticMode.CONSERVATIVE,
-                    dry_run=self.dry_run
-                )
-                
-                self.refactoring_engine = IntelligentRefactoringEngine(
-                    dry_run=self.dry_run
-                )
-                
-                self.tdd_workflow_agent = TDDIntelligentWorkflowAgent(project_root=self.project_root)
-                
-                # Initialize MetaAgent for intelligent coordination
-                self.meta_agent = MetaAgent(
-                    project_root=self.project_root,
-                    token_budget=self.token_manager.max_tokens_per_hour,
-                    dry_run=self.dry_run
-                )
-                
-                self.logger.info("‚úÖ Intelligent agents initialized successfully (including MetaAgent)")
-            except Exception as e:
-                self.logger.error("‚ùå Failed to initialize intelligent agents: %s", e)
-                self.intelligent_agents_available = False
-        else:
-            self.logger.warning("‚ö†Ô∏è Intelligent agents not available - using legacy analysis")
-
-        self.validation_pipeline = self._setup_validation_pipeline()
-        self.logger.info("Auditor inicializado (S√©tima Camada: %s, IA: %s)", 
-                        "completa" if self.setima_integration_available else "parcial",
-                        "dispon√≠vel" if self.intelligent_agents_available else "indispon√≠vel")
-
-    # ------------------------------------------------------------------ utils
-    def _setup_validation_pipeline(self) -> Dict[str, Any]:
-        return {
-            "min_context_quality": 50.0,
-            "integration_test_required": True,
-            "critical_validation_required": True,
-            "rollback_on_failure": True,
-        }
-
-    # --------------------------------------------------------- auditoria file
-    def audit_file_enhanced(self, file_path: str) -> FileAuditResult:
-        try:
-            self.logger.info("Iniciando auditoria (enhanced) de %s", file_path)
-
-            # 1) Contexto
-            context_quality = self._validate_context_quality(file_path)
-            if context_quality < self.validation_pipeline["min_context_quality"]:
-                return self._create_skip_result(file_path, f"Low context quality: {context_quality:.1f}%")
-
-            # 2) Risco
-            risk_score = self.setima_data.get_file_risk_score(file_path)
-            risk_category = self.setima_data.get_file_risk_category(file_path)
-
-            # 3) Onda
-            modification_wave = self.setima_data.get_file_wave(file_path)
-
-            # 4) Padr√µes
-            patterns_found_dicts = self._detect_patterns(file_path)
-            anti_patterns = [p for p in patterns_found_dicts if p.get("is_anti_pattern", False)]
-            good_patterns = [p for p in patterns_found_dicts if not p.get("is_anti_pattern", False)]
-
-            # 5) Seguran√ßa extra p/ cr√≠ticos
-            if risk_category == "CRITICAL":
-                safety_check = self._validate_critical_modification_safety(file_path)
-                if not safety_check["is_safe"]:
-                    return self._create_defer_result(file_path, f"Critical safety check failed: {safety_check['reason']}")
-
-            # 6) Backup (respeita dry-run/validate-only)
-            backup_created = False
-            if not self.dry_run and not self.validate_only:
-                backup_created = self._create_file_backup(file_path)
-
-            # 7) Execu√ß√£o inteligente com IA ou fallback para contexto-aware
-            if self.intelligent_agents_available:
-                audit_result = self._execute_intelligent_audit(
-                    file_path=file_path,
-                    risk_score=risk_score,
-                    context_quality=context_quality
-                )
-            else:
-                # Fallback para an√°lise contexto-aware (legado)
-                audit_result = self._execute_context_aware_audit(
-                    file_path=file_path,
-                    risk_score=risk_score,
-                    patterns=patterns_found_dicts,
-                    context_quality=context_quality,
-                    wave_info=modification_wave,
-                )
-
-            # 8) Valida√ß√µes p√≥s-modifica√ß√£o
-            if audit_result.get("modified", False):
-                validation_result = self._validate_post_modification(file_path)
-                if not validation_result["passed"]:
-                    if not self.dry_run and not self.validate_only:
-                        self._rollback_file_changes(file_path)
-                    return self._create_rollback_result(file_path, validation_result["errors"])
-
-            # 9) Resultado enriquecido
-            return FileAuditResult(
-                file_path=file_path,
-                lines_analyzed=audit_result.get("lines_analyzed", 0),
-                issues_found=len(anti_patterns),
-                optimizations_applied=audit_result.get("optimizations_applied", 0),
-                tokens_used=audit_result.get("tokens_used", 0),
-                changes_summary=audit_result.get("changes_summary", "Enhanced audit completed"),
-                syntax_valid=audit_result.get("syntax_valid", True),
-                backup_created=backup_created,
-                context_quality=context_quality,
-                risk_score=risk_score,
-                risk_category=risk_category,
-                modification_wave=modification_wave,
-                patterns_found=[p.get("name", "unknown") for p in patterns_found_dicts],
-                good_patterns_preserved=[p.get("name", "unknown") for p in good_patterns],
-                anti_patterns_fixed=[p.get("name", "unknown") for p in anti_patterns if audit_result.get("modified")],
-                integration_tests_passed=True,
-                rollback_available=backup_created,
-            )
-        except Exception as e:  # pragma: no cover - trilha de seguran√ßa
-            self.logger.error("Erro cr√≠tico na auditoria de %s: %s", file_path, e)
-            self._emergency_rollback(file_path)
-            return self._create_emergency_result(file_path, str(e))
-
-    # -------------------------------------------------------------- valida√ß√£o
-    def _validate_context_quality(self, file_path: str) -> float:
-        """An√°lise sofisticada de qualidade usando AST parsing com fallback heur√≠stico."""
-        try:
-            content = self._read_file_safely(file_path)
-            if not content:
-                return 20.0
-            
-            # Tentar an√°lise AST primeiro
-            try:
-                return self._validate_context_quality_ast_based(file_path, content)
-            except SyntaxError:
-                self.logger.debug("Syntax error em %s, usando fallback heur√≠stico", file_path)
-                return self._validate_context_quality_fallback(content)
-                
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao avaliar contexto de %s: %s", file_path, e)
-            return 40.0
-
-    def _validate_context_quality_ast_based(self, file_path: str, content: str) -> float:
-        """An√°lise precisa usando AST parsing para m√©tricas estruturais."""
-        tree = ast.parse(content)
-        
-        # M√©tricas estruturais
-        total_functions = 0
-        functions_with_docstrings = 0
-        functions_with_type_hints = 0
-        functions_with_return_hints = 0
-        total_classes = 0
-        classes_with_docstrings = 0
-        has_module_docstring = bool(ast.get_docstring(tree))
-        
-        # M√©tricas de complexidade
-        total_lines = content.count('\n') + 1
-        max_function_complexity = 0
-        error_handling_blocks = 0
-        logging_calls = 0
-        
-        # An√°lise estrutural
-        for node in ast.walk(tree):
-            if isinstance(node, ast.FunctionDef):
-                total_functions += 1
-                
-                # Docstring
-                if ast.get_docstring(node):
-                    functions_with_docstrings += 1
-                
-                # Type hints nos argumentos
-                if any(arg.annotation for arg in node.args.args):
-                    functions_with_type_hints += 1
-                
-                # Return type hint
-                if node.returns:
-                    functions_with_return_hints += 1
-                
-                # Complexidade (aproximada por n√∫mero de decis√µes)
-                complexity = self._calculate_function_complexity(node)
-                max_function_complexity = max(max_function_complexity, complexity)
-            
-            elif isinstance(node, ast.ClassDef):
-                total_classes += 1
-                if ast.get_docstring(node):
-                    classes_with_docstrings += 1
-            
-            elif isinstance(node, ast.Try):
-                error_handling_blocks += 1
-            
-            elif isinstance(node, ast.Call):
-                # Detectar logging calls
-                if (isinstance(node.func, ast.Attribute) and 
-                    isinstance(node.func.value, ast.Name) and
-                    node.func.value.id in ['logger', 'logging']):
-                    logging_calls += 1
-        
-        # C√°lculo de qualidade baseado em m√©tricas reais
-        quality = 30.0  # Base score
-        
-        # Documenta√ß√£o (30 pontos m√°ximo)
-        if has_module_docstring:
-            quality += 10.0
-        
-        if total_functions > 0:
-            doc_ratio = functions_with_docstrings / total_functions
-            quality += doc_ratio * 15.0  # 15 pontos para 100% docs
-        
-        if total_classes > 0:
-            class_doc_ratio = classes_with_docstrings / total_classes
-            quality += class_doc_ratio * 5.0  # 5 pontos para class docs
-        
-        # Type hints (25 pontos m√°ximo)
-        if total_functions > 0:
-            hint_ratio = functions_with_type_hints / total_functions
-            return_hint_ratio = functions_with_return_hints / total_functions
-            quality += hint_ratio * 15.0  # 15 pontos para argument hints
-            quality += return_hint_ratio * 10.0  # 10 pontos para return hints
-        
-        # Error handling (15 pontos m√°ximo)
-        if error_handling_blocks > 0:
-            quality += min(error_handling_blocks * 3.0, 15.0)
-        
-        # Logging (10 pontos m√°ximo)
-        if logging_calls > 0:
-            quality += min(logging_calls * 2.0, 10.0)
-        
-        # Complexidade (penalidade/b√¥nus - 10 pontos)
-        if max_function_complexity <= 5:
-            quality += 10.0  # Baixa complexidade
-        elif max_function_complexity <= 10:
-            quality += 5.0   # Complexidade moderada
-        elif max_function_complexity > 20:
-            quality -= 5.0   # Penalidade por alta complexidade
-        
-        # Tamanho do arquivo (10 pontos m√°ximo)
-        if 20 <= total_lines <= 300:
-            quality += 10.0  # Tamanho ideal
-        elif 300 < total_lines <= 500:
-            quality += 5.0   # Aceit√°vel
-        elif total_lines > 1000:
-            quality -= 5.0   # Penalidade por arquivo muito grande
-        
-        return min(quality, 100.0)
-    
-    def _calculate_function_complexity(self, func_node: ast.FunctionDef) -> int:
-        """Calcula complexidade ciclom√°tica aproximada de uma fun√ß√£o."""
-        complexity = 1  # Base complexity
-        
-        for node in ast.walk(func_node):
-            # Decis√µes que aumentam complexidade
-            if isinstance(node, (ast.If, ast.While, ast.For, ast.Try)):
-                complexity += 1
-            elif isinstance(node, ast.ExceptHandler):
-                complexity += 1
-            elif isinstance(node, ast.With):
-                complexity += 1
-            # Operadores l√≥gicos
-            elif isinstance(node, (ast.And, ast.Or)):
-                complexity += 1
-        
-        return complexity
-
-    def _validate_context_quality_fallback(self, content: str) -> float:
-        """Fallback heur√≠stico para arquivos com syntax errors."""
-        lines = content.count('\n') + 1
-        has_docstrings = '"""' in content or "'''" in content
-        has_type_hints = (': ' in content) and ('->' in content)
-        has_logging = ('logger.' in content) or ('logging' in content)
-        has_error_handling = ('try:' in content and 'except' in content)
-        
-        quality = 40.0  # Base score mais baixo para fallback
-        
-        if has_docstrings:
-            quality += 12.0
-        if has_type_hints:
-            quality += 12.0
-        if has_logging:
-            quality += 8.0
-        if has_error_handling:
-            quality += 8.0
-        if 20 <= lines <= 500:
-            quality += 5.0
-        
-        return min(quality, 85.0)  # M√°ximo menor para fallback
-
-    def _read_file_safely(self, file_path: str) -> str:
-        """L√™ arquivo com detec√ß√£o de encoding e tratamento robusto."""
-        full_path = self.project_root / file_path
-        if not full_path.exists():
-            return ""
-        
-        # Tentar encodings comuns
-        encodings = ["utf-8", "latin1", "cp1252", "iso-8859-1"]
-        
-        for encoding in encodings:
-            try:
-                return full_path.read_text(encoding=encoding)
-            except UnicodeDecodeError:
-                continue
-        
-        # Fallback com errors='ignore'
-        return full_path.read_text(encoding="utf-8", errors="ignore")
-
-    def _detect_patterns(self, file_path: str) -> List[Dict[str, Any]]:
-        """Detec√ß√£o robusta de patterns usando AST parsing com fallback."""
-        try:
-            content = self._read_file_safely(file_path)
-            if not content:
-                return []
-            
-            # Tentar AST parsing primeiro
-            try:
-                return self._detect_patterns_ast_based(file_path, content)
-            except SyntaxError:
-                self.logger.debug("Syntax error em %s, usando fallback heur√≠stico", file_path)
-                return self._detect_patterns_fallback(file_path, content)
-                
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao detectar padr√µes em %s: %s", file_path, e)
-            return []
-
-    def _detect_patterns_ast_based(self, file_path: str, content: str) -> List[Dict[str, Any]]:
-        """Detec√ß√£o precisa usando AST parsing."""
-        patterns: List[Dict[str, Any]] = []
-        tree = ast.parse(content)
-        
-        # Contadores para an√°lise
-        try_blocks = 0
-        import_errors = 0
-        bare_excepts = 0
-        exception_swallows = 0
-        god_methods = 0
-        functions_with_docs = 0
-        total_functions = 0
-        has_graceful_imports = False
-        has_structured_logging = False
-        
-        for node in ast.walk(tree):
-            # An√°lise de try/except blocks
-            if isinstance(node, ast.Try):
-                try_blocks += 1
-                for handler in node.handlers:
-                    if handler.type is None:  # bare except
-                        bare_excepts += 1
-                        patterns.append({
-                            "name": "bare_except",
-                            "type": "anti_pattern",
-                            "is_anti_pattern": True,
-                            "severity": "HIGH",
-                            "auto_fixable": True,
-                            "line": handler.lineno,
-                            "details": "Bare except clause detected"
-                        })
-                    elif (isinstance(handler.type, ast.Name) and 
-                          handler.type.id == "Exception" and 
-                          len(handler.body) == 1 and 
-                          isinstance(handler.body[0], ast.Return)):
-                        exception_swallows += 1
-                        patterns.append({
-                            "name": "exception_swallowing",
-                            "type": "anti_pattern", 
-                            "is_anti_pattern": True,
-                            "severity": "HIGH",
-                            "auto_fixable": True,
-                            "line": handler.lineno,
-                            "details": "Exception swallowing pattern detected"
-                        })
-                    elif (isinstance(handler.type, ast.Name) and 
-                          handler.type.id == "ImportError"):
-                        import_errors += 1
-            
-            # An√°lise de fun√ß√µes
-            elif isinstance(node, ast.FunctionDef):
-                total_functions += 1
-                
-                # Verificar docstring
-                if ast.get_docstring(node):
-                    functions_with_docs += 1
-                
-                # Detectar god methods
-                function_lines = node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 0
-                if function_lines > 50:
-                    god_methods += 1
-                    patterns.append({
-                        "name": "god_method",
-                        "type": "anti_pattern",
-                        "is_anti_pattern": True,
-                        "severity": "MEDIUM",
-                        "auto_fixable": False,
-                        "line": node.lineno,
-                        "details": f"Function '{node.name}' has {function_lines} lines"
-                    })
-            
-            # Detectar structured logging
-            elif isinstance(node, ast.Call):
-                if (isinstance(node.func, ast.Attribute) and 
-                    isinstance(node.func.value, ast.Name) and 
-                    node.func.value.id == "logger"):
-                    # Verificar se tem extra= keyword
-                    for keyword in node.keywords:
-                        if keyword.arg == "extra":
-                            has_structured_logging = True
-                            break
-        
-        # Detectar import hell pattern
-        if try_blocks > 3 and import_errors >= 2:
-            patterns.append({
-                "name": "import_hell",
-                "type": "anti_pattern",
-                "is_anti_pattern": True,
-                "severity": "HIGH",
-                "auto_fixable": True,
-                "details": f"{try_blocks} try blocks, {import_errors} ImportError handlers"
-            })
-        
-        # Detectar graceful imports
-        if import_errors > 0 and "_AVAILABLE" in content:
-            has_graceful_imports = True
-            patterns.append({
-                "name": "graceful_import",
-                "type": "good_pattern",
-                "is_anti_pattern": False,
-                "preserve": True,
-                "details": "Graceful import pattern with fallback"
-            })
-        
-        # Detectar structured logging
-        if has_structured_logging:
-            patterns.append({
-                "name": "structured_logging",
-                "type": "good_pattern",
-                "is_anti_pattern": False,
-                "preserve": True,
-                "details": "Structured logging with extra context"
-            })
-        
-        return patterns
-
-    def _detect_patterns_fallback(self, file_path: str, content: str) -> List[Dict[str, Any]]:
-        """Fallback heur√≠stico para arquivos com syntax errors."""
-        patterns: List[Dict[str, Any]] = []
-        
-        # Anti-patterns b√°sicos por string matching
-        if "except Exception:" in content or "except:" in content:
-            patterns.append({
-                "name": "exception_swallowing",
-                "type": "anti_pattern",
-                "is_anti_pattern": True,
-                "severity": "HIGH",
-                "auto_fixable": True,
-                "details": "Detected via string matching (fallback)"
-            })
-        
-        if content.count("try:") > 3 and "ImportError" in content:
-            patterns.append({
-                "name": "import_hell",
-                "type": "anti_pattern",
-                "is_anti_pattern": True,
-                "severity": "HIGH", 
-                "auto_fixable": True,
-                "details": "Detected via string matching (fallback)"
-            })
-        
-        # Good patterns
-        if "try:" in content and "except ImportError:" in content and "_AVAILABLE" in content:
-            patterns.append({
-                "name": "graceful_import",
-                "type": "good_pattern",
-                "is_anti_pattern": False,
-                "preserve": True,
-                "details": "Detected via string matching (fallback)"
-            })
-        
-        if "logger." in content and "extra=" in content:
-            patterns.append({
-                "name": "structured_logging",
-                "type": "good_pattern",
-                "is_anti_pattern": False,
-                "preserve": True,
-                "details": "Detected via string matching (fallback)"
-            })
-        
-        return patterns
-
-    def _validate_critical_modification_safety(self, file_path: str) -> Dict[str, Any]:
-        try:
-            checks = {
-                "database_integrity_check": True,
-                "security_constraints_check": True,
-                "performance_impact_check": True,
-                "rollback_readiness_check": True,
-            }
-            failed = [k for k, ok in checks.items() if not ok]
-            return {
-                "is_safe": not failed,
-                "reason": f"Failed checks: {failed}" if failed else "All safety checks passed",
-                "checks_performed": list(checks.keys()),
-                "failed_checks": failed,
-            }
-        except Exception as e:  # pragma: no cover
-            return {"is_safe": False, "reason": f"Safety validation error: {e}", "checks_performed": [], "failed_checks": ["safety_validation_error"]}
-
-    # --------------------------------------------------------------- execu√ß√£o
-    def _execute_intelligent_audit(self, file_path: str, risk_score: int, context_quality: float) -> Dict[str, Any]:
-        """Execute intelligent audit using MetaAgent coordination with AI agents."""
-        try:
-            # Step 1: Pre-analysis token estimation
-            estimated_tokens = self.token_manager.estimate_file_tokens(file_path, self.project_root)
-            
-            # Step 2: Token availability check with intelligent throttling
-            if not self.token_manager.can_proceed(estimated_tokens, file_path):
-                sleep_time = self.token_manager.calculate_intelligent_sleep_time()
-                if sleep_time > 0:
-                    self.logger.info("üõ°Ô∏è Token limit protection: sleeping %d seconds for %s", int(sleep_time), file_path)
-                    return {
-                        "deferred": True,
-                        "reason": f"Token limit protection - sleeping {int(sleep_time)}s",
-                        "sleep_time": sleep_time,
-                        "estimated_tokens": estimated_tokens
-                    }
-            
-            # Step 3: Execute MetaAgent-coordinated analysis
-            if self.intelligent_agents_available and self.meta_agent:
-                self.logger.info("üß† Using MetaAgent coordination for %s", file_path)
-                
-                # Determine task type based on risk and context
-                task_type = self._determine_task_type(file_path, risk_score, context_quality)
-                
-                # Get available tokens for this analysis
-                available_tokens = min(
-                    estimated_tokens * 2,  # Allow some overhead
-                    self.token_manager.get_available_tokens()
-                )
-                
-                # Create execution plan using MetaAgent
-                execution_plan = self.meta_agent.create_execution_plan(
-                    file_path=file_path,
-                    task_type=task_type,
-                    available_tokens=available_tokens
-                )
-                
-                self.logger.debug("üìã MetaAgent plan: %d agents, %d estimated tokens, %.1fs estimated time",
-                                len(execution_plan.agents),
-                                execution_plan.total_estimated_tokens,
-                                execution_plan.total_estimated_time)
-                
-                # Execute the plan
-                execution_results = self.meta_agent.execute_plan(execution_plan)
-                
-                # Process results from all agents
-                analysis_result = None
-                refactoring_results = None
-                tdd_optimizations = None
-                god_code_analysis = None
-                total_tokens_used = 0
-                
-                for result in execution_results:
-                    total_tokens_used += result.tokens_used
-                    
-                    if result.agent_type.value == "intelligent_code_agent" and result.success:
-                        analysis_result = result.result_data
-                    elif result.agent_type.value == "refactoring_engine" and result.success:
-                        refactoring_results = result.result_data
-                    elif result.agent_type.value == "tdd_workflow_agent" and result.success:
-                        tdd_optimizations = result.result_data
-                    elif result.agent_type.value == "god_code_agent" and result.success:
-                        god_code_analysis = result.result_data
-                
-                # Record actual token usage
-                self.token_manager.record_usage(total_tokens_used, file_path)
-                
-                # Determine if any modifications were made
-                modifications_made = any(
-                    result.success and result.result_data.get("applied_refactorings") 
-                    for result in execution_results
-                ) and not self.dry_run
-                
-                return {
-                    "intelligent_analysis": True,
-                    "meta_agent_coordination": True,
-                    "execution_plan": {
-                        "agents_planned": [agent.agent_type.value for agent in execution_plan.agents],
-                        "estimated_tokens": execution_plan.total_estimated_tokens,
-                        "estimated_time": execution_plan.total_estimated_time
-                    },
-                    "execution_results": [
-                        {
-                            "agent": result.agent_type.value,
-                            "success": result.success,
-                            "execution_time": result.execution_time,
-                            "tokens_used": result.tokens_used,
-                            "warnings": result.warnings,
-                            "errors": result.errors
-                        }
-                        for result in execution_results
-                    ],
-                    "analysis_result": analysis_result,
-                    "refactoring_results": refactoring_results,
-                    "tdd_optimizations": tdd_optimizations,
-                    "god_code_analysis": god_code_analysis,
-                    "tokens_used": total_tokens_used,
-                    "modified": modifications_made,
-                    "lines_analyzed": analysis_result.get("total_lines", 0) if analysis_result else 0,
-                    "issues_found": len(analysis_result.get("line_analyses", [])) if analysis_result else 0,
-                    "optimizations_applied": len(refactoring_results.get("applied_refactorings", [])) if refactoring_results else 0,
-                    "agents_executed": len([r for r in execution_results if r.success]),
-                    "success_rate": len([r for r in execution_results if r.success]) / len(execution_results) if execution_results else 0
-                }
-                
-            elif self.intelligent_agents_available and self.intelligent_code_agent:
-                # Fallback to manual agent coordination (legacy intelligent mode)
-                self.logger.info("üîÑ MetaAgent unavailable - using manual agent coordination")
-                return self._execute_manual_agent_coordination(file_path, estimated_tokens)
-            else:
-                # Fallback to legacy analysis
-                return self._execute_legacy_audit_fallback(file_path, estimated_tokens)
-                
-        except Exception as e:
-            self.logger.error("‚ùå Intelligent audit failed for %s: %s", file_path, e)
-            return {
-                "error": str(e),
-                "fallback_to_legacy": True,
-                "tokens_used": 100  # Minimal token usage for error
-            }
-
-    def _determine_task_type(self, file_path: str, risk_score: int, context_quality: float) -> 'TaskType':
-        """Determine appropriate task type based on file characteristics and risk assessment."""
-        if not TaskType:
-            return None  # Fallback if TaskType not available
-            
-        # Analyze file name and path patterns
-        file_path_lower = file_path.lower()
-        
-        # God code detection for large/complex files
-        try:
-            with open(file_path, 'r', encoding='utf-8') as f:
-                content = f.read()
-                line_count = len([line for line in content.split('\n') if line.strip()])
-                
-            # God code indicators
-            if line_count > 500 or 'class' in content and content.count('def ') > 20:
-                return TaskType.GOD_CODE_DETECTION
-        except:
-            pass
-        
-        # TDD optimization for test files
-        if 'test' in file_path_lower or '/tests/' in file_path_lower:
-            return TaskType.TDD_OPTIMIZATION
-        
-        # Performance analysis for high-risk files
-        if risk_score > 80:
-            return TaskType.PERFORMANCE_ANALYSIS
-        
-        # Security analysis for security-critical files
-        if any(keyword in file_path_lower for keyword in ['auth', 'security', 'password', 'crypto']):
-            return TaskType.SECURITY_ANALYSIS
-        
-        # Architecture review for core system files
-        if any(keyword in file_path_lower for keyword in ['service', 'manager', 'controller', 'middleware']):
-            return TaskType.ARCHITECTURE_REVIEW
-        
-        # Code refactoring for medium-risk files with good context
-        if 50 <= risk_score <= 80 and context_quality >= 70.0:
-            return TaskType.CODE_REFACTORING
-        
-        # Default to comprehensive audit
-        return TaskType.COMPREHENSIVE_AUDIT
-    
-    def _execute_manual_agent_coordination(self, file_path: str, estimated_tokens: int) -> Dict[str, Any]:
-        """Fallback to manual agent coordination when MetaAgent is not available."""
-        try:
-            self.logger.info("üîÑ Manual agent coordination for %s", file_path)
-            
-            # Use the original intelligent agents manually
-            analysis_result = self.intelligent_code_agent.analyze_file_intelligently(file_path)
-            
-            # Apply intelligent refactorings if available
-            refactoring_results = None
-            available_refactorings = getattr(analysis_result, 'refactorings', None) or []
-            if self.refactoring_engine and available_refactorings and not self.dry_run:
-                refactoring_results = self.refactoring_engine.apply_intelligent_refactorings(analysis_result)
-            
-            # TDD workflow optimizations
-            tdd_optimizations = None
-            if self.tdd_workflow_agent and analysis_result:
-                try:
-                    # Try different possible method names
-                    if hasattr(self.tdd_workflow_agent, 'analyze_tdd_opportunities'):
-                        tdd_optimizations = self.tdd_workflow_agent.analyze_tdd_opportunities(analysis_result)
-                    elif hasattr(self.tdd_workflow_agent, 'analyze_file'):
-                        tdd_optimizations = self.tdd_workflow_agent.analyze_file(file_path)
-                    elif hasattr(self.tdd_workflow_agent, 'get_tdd_improvements'):
-                        tdd_optimizations = self.tdd_workflow_agent.get_tdd_improvements(analysis_result)
-                except Exception as tdd_error:
-                    self.logger.debug("TDD workflow analysis failed: %s", tdd_error)
-                    tdd_optimizations = None
-            
-            # Calculate actual token usage
-            actual_tokens = getattr(analysis_result, 'tokens_used', estimated_tokens)
-            if refactoring_results:
-                actual_tokens += getattr(refactoring_results, 'tokens_used', 0)
-            
-            # Record token usage
-            self.token_manager.record_usage(actual_tokens, file_path)
-            
-            return {
-                "intelligent_analysis": True,
-                "meta_agent_coordination": False,
-                "manual_coordination": True,
-                "analysis_result": analysis_result,
-                "refactoring_results": refactoring_results,
-                "tdd_optimizations": tdd_optimizations,
-                "tokens_used": actual_tokens,
-                "modified": bool(refactoring_results and not self.dry_run),
-                "lines_analyzed": getattr(analysis_result, 'total_lines', 0),
-                "issues_found": len(getattr(analysis_result, 'line_analyses', [])),
-                "optimizations_applied": len(getattr(refactoring_results, 'applied_refactorings', [])) if refactoring_results else 0
-            }
-            
-        except Exception as e:
-            self.logger.error("‚ùå Manual agent coordination failed for %s: %s", file_path, e)
-            return {
-                "error": str(e),
-                "fallback_to_legacy": True,
-                "tokens_used": 100
-            }
-
-    def _execute_legacy_audit_fallback(self, file_path: str, estimated_tokens: int) -> Dict[str, Any]:
-        """Fallback to legacy analysis when intelligent agents are not available."""
-        try:
-            self.logger.info("üîÑ Falling back to legacy analysis for %s", file_path)
-            
-            # Use basic patterns detection as fallback
-            patterns_found = self._detect_patterns(file_path)
-            
-            # Simple token tracking
-            self.token_manager.record_usage(estimated_tokens, file_path)
-            
-            # Basic analysis without modifications
-            content = self._read_file_safely(file_path)
-            lines_count = len(content.splitlines()) if content else 0
-            
-            return {
-                "intelligent_analysis": False,
-                "legacy_analysis": True,
-                "patterns_found": patterns_found,
-                "tokens_used": estimated_tokens,
-                "modified": False,
-                "lines_analyzed": lines_count,
-                "issues_found": len([p for p in patterns_found if p.get("is_anti_pattern", False)]),
-                "optimizations_applied": 0,
-                "changes_summary": "Legacy analysis completed (no modifications)"
-            }
-            
-        except Exception as e:
-            self.logger.error("‚ùå Legacy audit fallback failed for %s: %s", file_path, e)
-            return {
-                "error": str(e),
-                "legacy_analysis": True,
-                "tokens_used": 50  # Minimal token usage for error
-            }
-
-    def _execute_context_aware_audit(
-        self,
-        file_path: str,
-        risk_score: int,
-        patterns: List[Dict[str, Any]],
-        context_quality: float,
-        wave_info: str,
-    ) -> Dict[str, Any]:
-        """Sistema inteligente de fixes com auto-import e valida√ß√£o AST."""
-        try:
-            content = self._read_file_safely(file_path)
-            if not content:
-                return {
-                    "modified": False,
-                    "error": "File not found",
-                    "lines_analyzed": 0,
-                    "optimizations_applied": 0,
-                    "tokens_used": 0,
-                }
-
-            lines_analyzed = content.count("\n") + 1
-            
-            # Executar sistema de fixes inteligente
-            return self._execute_smart_fixes(file_path, content, patterns, risk_score, lines_analyzed)
-            
-        except Exception as e:  # pragma: no cover
-            self.logger.error("Falha na auditoria contexto-aware de %s: %s", file_path, e)
-            return {
-                "modified": False,
-                "error": str(e),
-                "lines_analyzed": 0,
-                "optimizations_applied": 0,
-                "tokens_used": 50,
-            }
-
-    def _execute_smart_fixes(
-        self,
-        file_path: str,
-        content: str,
-        patterns: List[Dict[str, Any]],
-        risk_score: int,
-        lines_analyzed: int,
-    ) -> Dict[str, Any]:
-        """Sistema de fixes que pode adicionar imports e fazer modifica√ß√µes inteligentes."""
-        modifications: List[str] = []
-        optimizations_applied = 0
-        tokens_used = 50
-        modified_content = content
-        
-        # Verificar se precisa de logger e se pode adicionar
-        needs_logger = self._needs_logger_import(patterns)
-        has_logger = self._has_logger_import(content)
-        
-        # Adicionar logger import se necess√°rio e seguro
-        if needs_logger and not has_logger and risk_score <= 70:
-            modified_content, import_added = self._add_logger_import(modified_content)
-            if import_added:
-                modifications.append("Added logger import")
-                optimizations_applied += 1
-        
-        # Aplicar fixes espec√≠ficos por pattern
-        for pattern in patterns:
-            if not pattern.get("is_anti_pattern") or not pattern.get("auto_fixable"):
-                continue
-                
-            pattern_name = pattern.get("name")
-            pattern_line = pattern.get("line", 0)
-            
-            # Exception swallowing fixes (mais inteligentes)
-            if pattern_name in ["exception_swallowing", "bare_except"] and risk_score <= 70:
-                modified_content, fix_applied = self._fix_exception_swallowing(
-                    modified_content, pattern, has_logger or needs_logger
-                )
-                if fix_applied:
-                    modifications.append(f"Fixed {pattern_name} at line {pattern_line}")
-                    optimizations_applied += 1
-            
-            # Import hell fixes
-            elif pattern_name == "import_hell" and risk_score <= 50:
-                modified_content, fix_applied = self._fix_import_hell(modified_content, pattern)
-                if fix_applied:
-                    modifications.append("Simplified import pattern")
-                    optimizations_applied += 1
-            
-            # Outros patterns podem ser adicionados aqui
-        
-        # Aplicar modifica√ß√µes se n√£o for dry-run
-        modified = len(modifications) > 0
-        if modified and not self.dry_run and not self.validate_only:
-            # Validar sintaxe antes de escrever
-            try:
-                ast.parse(modified_content)
-                full_path = self.project_root / file_path
-                full_path.write_text(modified_content, encoding="utf-8")
-            except SyntaxError as e:
-                self.logger.warning("Syntax error ap√≥s fixes em %s: %s", file_path, e)
-                modifications.append(f"ROLLBACK: Syntax error - {e}")
-                modified = False
-        
-        return {
-            "modified": modified,
-            "lines_analyzed": lines_analyzed,
-            "optimizations_applied": optimizations_applied,
-            "tokens_used": tokens_used,
-            "changes_summary": "; ".join(modifications) if modifications else
-                ("No changes applied (dry-run)" if self.dry_run else "No changes applied"),
-            "syntax_valid": True,
-            "modifications": modifications,
-        }
-
-    def _needs_logger_import(self, patterns: List[Dict[str, Any]]) -> bool:
-        """Verifica se os patterns requerem logger."""
-        return any(
-            p.get("name") in ["exception_swallowing", "bare_except"] and p.get("auto_fixable")
-            for p in patterns if p.get("is_anti_pattern")
-        )
-
-    def _has_logger_import(self, content: str) -> bool:
-        """Verifica se arquivo j√° tem logger importado ou configurado."""
-        return (
-            "import logging" in content or
-            "from logging import" in content or
-            "logger = logging.getLogger" in content or
-            "logger." in content
-        )
-
-    def _add_logger_import(self, content: str) -> tuple[str, bool]:
-        """Adiciona import de logger de forma inteligente."""
-        try:
-            tree = ast.parse(content)
-            
-            # Encontrar melhor posi√ß√£o para adicionar import
-            lines = content.split('\n')
-            insert_position = 0
-            
-            # Procurar √∫ltima linha de import
-            for i, line in enumerate(lines):
-                stripped = line.strip()
-                if (stripped.startswith('import ') or 
-                    stripped.startswith('from ') or
-                    stripped.startswith('#') or
-                    stripped.startswith('"""') or
-                    stripped.startswith("'''") or
-                    not stripped):
-                    insert_position = i + 1
-                else:
-                    break
-            
-            # Adicionar logger import
-            logger_import = "import logging"
-            logger_setup = "logger = logging.getLogger(__name__)"
-            
-            # Inserir nas posi√ß√µes corretas
-            lines.insert(insert_position, logger_import)
-            lines.insert(insert_position + 1, logger_setup)
-            lines.insert(insert_position + 2, "")  # Linha em branco
-            
-            return '\n'.join(lines), True
-            
-        except (SyntaxError, Exception):
-            # Fallback simples
-            import_section = "import logging\nlogger = logging.getLogger(__name__)\n\n"
-            return import_section + content, True
-
-    def _fix_exception_swallowing(
-        self, 
-        content: str, 
-        pattern: Dict[str, Any], 
-        has_logger: bool
-    ) -> tuple[str, bool]:
-        """Fix inteligente para exception swallowing usando AST quando poss√≠vel."""
-        if not has_logger:
-            return content, False
-            
-        try:
-            # Tentar fix baseado em AST primeiro
-            tree = ast.parse(content)
-            lines = content.split('\n')
-            
-            for node in ast.walk(tree):
-                if isinstance(node, ast.ExceptHandler):
-                    # Bare except
-                    if node.type is None and hasattr(node, 'lineno'):
-                        line_idx = node.lineno - 1
-                        if line_idx < len(lines):
-                            original_line = lines[line_idx]
-                            if 'except:' in original_line:
-                                # Substituir bare except
-                                indent = len(original_line) - len(original_line.lstrip())
-                                new_line = ' ' * indent + 'except Exception as e:'
-                                new_handler = ' ' * (indent + 4) + 'logger.warning("Operation failed: %s", str(e))'
-                                
-                                lines[line_idx] = new_line
-                                # Inserir handler se n√£o existir
-                                if line_idx + 1 < len(lines):
-                                    lines.insert(line_idx + 1, new_handler)
-                                else:
-                                    lines.append(new_handler)
-                                
-                                return '\n'.join(lines), True
-                    
-                    # Exception swallowing
-                    elif (isinstance(node.type, ast.Name) and 
-                          node.type.id == "Exception" and
-                          len(node.body) == 1 and 
-                          isinstance(node.body[0], ast.Return)):
-                        
-                        line_idx = node.lineno - 1
-                        if line_idx < len(lines):
-                            # Encontrar linha do handler
-                            for i in range(line_idx, min(line_idx + 3, len(lines))):
-                                if 'return' in lines[i] and 'except' not in lines[i]:
-                                    # Substituir return por logging
-                                    indent = len(lines[i]) - len(lines[i].lstrip())
-                                    new_handler = ' ' * indent + 'logger.warning("Operation failed: %s", str(e))'
-                                    lines[i] = new_handler
-                                    return '\n'.join(lines), True
-            
-            return content, False
-            
-        except (SyntaxError, Exception):
-            # Fallback para string replacement
-            return self._fix_exception_swallowing_fallback(content, has_logger)
-
-    def _fix_exception_swallowing_fallback(
-        self, 
-        content: str, 
-        has_logger: bool
-    ) -> tuple[str, bool]:
-        """Fallback simples para fix de exception swallowing."""
-        if not has_logger:
-            return content, False
-            
-        modified = content
-        changes_made = False
-        
-        # Fix bare except
-        if "except:" in modified:
-            modified = modified.replace(
-                "except:",
-                'except Exception as e:\n        logger.warning("Operation failed: %s", str(e))'
-            )
-            changes_made = True
-        
-        # Fix exception swallowing (heur√≠stica simples)
-        if "except Exception:" in modified and "return None" in modified:
-            modified = modified.replace(
-                "except Exception:",
-                'except Exception as e:\n        logger.warning("Operation failed: %s", str(e))'
-            )
-            changes_made = True
-        
-        return modified, changes_made
-
-    def _fix_import_hell(self, content: str, pattern: Dict[str, Any]) -> tuple[str, bool]:
-        """Fix para import hell patterns (simplificado)."""
-        # Por enquanto, apenas log o problema - fix manual recomendado
-        return content, False
-
-    # ------------------------------------------------------------- salvaguardas
-    def _create_file_backup(self, file_path: str) -> bool:
-        try:
-            full_path = self.project_root / file_path
-            if not full_path.exists():
-                return False
-            ts = datetime.now().strftime("%Y%m%d-%H%M%S")
-            backup_path = full_path.with_suffix(full_path.suffix + f".backup.{ts}")
-            backup_path.write_text(full_path.read_text(encoding="utf-8", errors="ignore"))
-            return True
-        except Exception as e:  # pragma: no cover
-            self.logger.error("Falha ao criar backup de %s: %s", file_path, e)
-            return False
-
-    def _validate_post_modification(self, file_path: str) -> Dict[str, Any]:
-        try:
-            # 1) Testes de integra√ß√£o (se dispon√≠veis)
-            if self.validation_pipeline["integration_test_required"] and self.integration_tester is not None:
-                try:
-                    test_result = self.integration_tester.test_file_integration(file_path)  # type: ignore
-                    if hasattr(test_result, "passed"):
-                        return {"passed": bool(getattr(test_result, "passed")), "errors": getattr(test_result, "errors", [])}
-                except Exception as e:  # pragma: no cover
-                    self.logger.warning("Teste de integra√ß√£o falhou em %s: %s", file_path, e)
-
-            # 2) Valida√ß√£o sint√°tica (AST)
-            full_path = self.project_root / file_path
-            try:
-                content = full_path.read_text(encoding="utf-8", errors="ignore")
-                ast.parse(content)
-                return {"passed": True, "errors": []}
-            except SyntaxError as e:
-                return {"passed": False, "errors": [f"Syntax error: {e}"]}
-        except Exception as e:  # pragma: no cover
-            return {"passed": False, "errors": [f"Validation error: {e}"]}
-
-    def _rollback_file_changes(self, file_path: str) -> bool:
-        try:
-            full_path = self.project_root / file_path
-            # procura o backup mais recente
-            backups = sorted(
-                full_path.parent.glob(full_path.name + ".backup.*"),
-                key=lambda p: p.stat().st_mtime,
-                reverse=True,
-            )
-            if backups:
-                full_path.write_text(backups[0].read_text(encoding="utf-8", errors="ignore"), encoding="utf-8")
-                backups[0].unlink(missing_ok=True)
-                self.logger.info("Rollback aplicado em %s", file_path)
-                return True
-            return False
-        except Exception as e:  # pragma: no cover
-            self.logger.error("Rollback falhou em %s: %s", file_path, e)
-            return False
-
-    def _emergency_rollback(self, file_path: str) -> None:
-        try:
-            if self.rollback_manager is not None:
-                self.rollback_manager.emergency_rollback(file_path)  # type: ignore
-                self.logger.warning("Emergency rollback (gestor externo) aplicado em %s", file_path)
-            else:
-                self._rollback_file_changes(file_path)
-                self.logger.warning("Emergency rollback (fallback simples) aplicado em %s", file_path)
-        except Exception as e:  # pragma: no cover
-            self.logger.critical("Emergency rollback falhou em %s: %s", file_path, e)
-
-    # -------------------------------------------------------- resultados helper
-    def _create_skip_result(self, file_path: str, reason: str) -> FileAuditResult:
-        return FileAuditResult(
-            file_path=file_path,
-            lines_analyzed=0,
-            issues_found=0,
-            optimizations_applied=0,
-            tokens_used=0,
-            changes_summary=f"SKIPPED: {reason}",
-            syntax_valid=True,
-            backup_created=False,
-            context_quality=0.0,
-            risk_score=0,
-            risk_category="UNKNOWN",
-            modification_wave="NONE",
-        )
-
-    def _create_defer_result(self, file_path: str, reason: str) -> FileAuditResult:
-        return FileAuditResult(
-            file_path=file_path,
-            lines_analyzed=0,
-            issues_found=0,
-            optimizations_applied=0,
-            tokens_used=0,
-            changes_summary=f"DEFERRED: {reason}",
-            syntax_valid=True,
-            backup_created=False,
-            context_quality=0.0,
-            risk_score=self.setima_data.get_file_risk_score(file_path),
-            risk_category=self.setima_data.get_file_risk_category(file_path),
-            modification_wave=self.setima_data.get_file_wave(file_path),
-        )
-
-    def _create_rollback_result(self, file_path: str, errors: List[str]) -> FileAuditResult:
-        return FileAuditResult(
-            file_path=file_path,
-            lines_analyzed=0,
-            issues_found=len(errors),
-            optimizations_applied=0,
-            tokens_used=10,
-            changes_summary=f"ROLLED BACK: {'; '.join(errors)}",
-            syntax_valid=False,
-            backup_created=True,
-            rollback_available=False,
-            integration_tests_passed=False,
-        )
+from typing import List
 
-    def _create_emergency_result(self, file_path: str, error: str) -> FileAuditResult:
-        return FileAuditResult(
-            file_path=file_path,
-            lines_analyzed=0,
-            issues_found=1,
-            optimizations_applied=0,
-            tokens_used=5,
-            changes_summary=f"EMERGENCY ROLLBACK: {error}",
-            syntax_valid=False,
-            backup_created=True,
-            rollback_available=False,
-        )
+from audit_system.core.container import build_default_auditor
+from audit_system.core.contracts import FileFinding
 
-    # ---------------------------------------------------------- execu√ß√£o waves
-    def execute_risk_based_audit(self) -> Dict[str, Any]:
-        """Executa as 4 ondas em sequ√™ncia, com salvaguardas por n√≠vel."""
-        start = time.time()
-        execution_results: Dict[str, Any] = {
-            "WAVE_1_LOW": [],
-            "WAVE_2_MEDIUM": [],
-            "WAVE_3_HIGH": [],
-            "WAVE_4_CRITICAL": [],
-            "execution_summary": {},
-        }
 
-        try:
-            # Wave 1: paraleliz√°vel (aqui sequ√™ncia simples por clareza)
-            wave_1 = self.setima_data.dependency_waves.get("WAVE_1_FOUNDATION", [])
-            execution_results["WAVE_1_LOW"] = self._execute_wave_parallel(wave_1)
+def _parse_args() -> argparse.Namespace:
+    ap = argparse.ArgumentParser(description="Systematic file audit (refactor: DI + orchestrator)")
+    ap.add_argument("--root", type=Path, default=Path.cwd(), help="Raiz do projeto (default: cwd)")
+    ap.add_argument("--db", type=Path, default=None, help="Caminho do DB SQLite de sess√µes (opcional)")
+    ap.add_argument("targets", nargs="*", type=Path, help="Arquivos/diret√≥rios espec√≠ficos a auditar (opcional)")
+    return ap.parse_args()
 
-            # Wave 2: coordena√ß√£o
-            wave_2 = self.setima_data.dependency_waves.get("WAVE_2_BUSINESS", [])
-            execution_results["WAVE_2_MEDIUM"] = self._execute_wave_coordinated(wave_2)
 
-            # Wave 3: sequencial
-            wave_3 = self.setima_data.dependency_waves.get("WAVE_3_INTEGRATION", [])
-            execution_results["WAVE_3_HIGH"] = self._execute_wave_sequential(wave_3)
+def _resolve_targets(root: Path, targets: List[Path]) -> List[Path]:
+    if not targets:
+        return []
+    resolved: List[Path] = []
+    for t in targets:
+        p = (root / t) if not t.is_absolute() else t
+        resolved.append(p.resolve())
+    return resolved
 
-            # Wave 4: cr√≠tico (um por vez + backup completo)
-            wave_4 = self.setima_data.dependency_waves.get("WAVE_4_CRITICAL", [])
-            execution_results["WAVE_4_CRITICAL"] = self._execute_wave_critical(wave_4)
 
-            # Summary
-            execution_results["execution_summary"] = self._generate_execution_summary(execution_results, time.time() - start)
-            return execution_results
-        except Exception as e:  # pragma: no cover
-            self.logger.error("Falha na execu√ß√£o por ondas: %s", e)
-            return {"error": str(e), "execution_aborted": True, "emergency_rollback_triggered": True}
-
-    def execute_intelligent_risk_based_audit(self, resume_session_id: Optional[str] = None) -> Dict[str, Any]:
-        """
-        Execute intelligent audit using AI agents with advanced batch processing,
-        token management, and session persistence.
-        """
-        start_time = time.time()
-        
-        try:
-            # Step 1: Try to resume existing session
-            resumed, resume_info = self._try_resume_session(resume_session_id)
-            
-            if resumed:
-                all_files = self.file_manager.get_all_python_files()
-                remaining_files = self.session_manager.get_remaining_files(all_files)
-                self.logger.info("üîÑ Resumed session with %d remaining files", len(remaining_files))
-            else:
-                # Start new session
-                all_files = self.file_manager.get_all_python_files()
-                remaining_files = all_files.copy()
-                
-                session_config = {
-                    "intelligent_mode": self.intelligent_agents_available,
-                    "dry_run": self.dry_run,
-                    "validate_only": self.validate_only,
-                    "audit_mode": "risk_based_intelligent"
-                }
-                
-                session_id = self._start_new_audit_session(len(all_files), session_config)
-                self.logger.info("üöÄ Started new intelligent audit session: %s", session_id)
-            
-            # Step 2: Prioritize files using intelligent risk-token analysis
-            prioritized_files = self.token_manager.prioritize_files_by_risk_and_tokens(
-                remaining_files, self.project_root, self.setima_data
-            )
-            
-            execution_results = {
-                "intelligent_analysis": True,
-                "files_processed": [],
-                "files_deferred": [],
-                "files_failed": [],
-                "token_consumption": {},
-                "performance_metrics": {},
-                "execution_summary": {}
-            }
-            
-            # Step 3: Execute intelligent batch processing
-            batch_number = 0
-            total_files = len(prioritized_files)
-            
-            while prioritized_files:
-                batch_number += 1
-                
-                # Calculate adaptive batch size
-                remaining_count = len(prioritized_files)
-                available_time = 1.0  # 1 hour default session
-                batch_size = self.token_manager.calculate_adaptive_batch_size(
-                    remaining_count, available_time
-                )
-                
-                # Get current batch
-                current_batch = prioritized_files[:batch_size]
-                prioritized_files = prioritized_files[batch_size:]
-                
-                self.logger.info(
-                    "üéØ Processing batch %d: %d files (remaining: %d)",
-                    batch_number, len(current_batch), len(prioritized_files)
-                )
-                
-                # Process batch with intelligent throttling
-                batch_results = self._process_intelligent_batch(current_batch, batch_number)
-                
-                # Consolidate results
-                execution_results["files_processed"].extend(batch_results.get("files_processed", []))
-                execution_results["files_deferred"].extend(batch_results.get("files_deferred", []))
-                execution_results["files_failed"].extend(batch_results.get("files_failed", []))
-                
-                # Update token consumption tracking
-                execution_results["token_consumption"].update(batch_results.get("token_consumption", {}))
-                
-                # Save checkpoint after each batch
-                current_batch_paths = [f[0] for f in current_batch]  # Extract file paths
-                token_stats = self.token_manager.get_comprehensive_usage_stats()
-                self.session_manager.save_checkpoint(current_batch_paths, token_stats)
-                
-                # Intelligent throttling between batches
-                sleep_time = self.token_manager.calculate_intelligent_sleep_time()
-                if sleep_time > 0:
-                    self.logger.info("‚è±Ô∏è Intelligent throttling: sleeping %d seconds", int(sleep_time))
-                    time.sleep(sleep_time)
-                
-                # System health check between batches
-                health = self._validate_system_health()
-                if not health["healthy"]:
-                    self.logger.warning("‚ö†Ô∏è System health degraded: %s", health["issues"])
-                    if batch_number > 3:  # Allow some initial instability
-                        self.logger.error("üö® System unhealthy after batch %d - stopping execution", batch_number)
-                        break
-                
-                # Progress reporting
-                progress = self.session_manager.get_audit_progress(total_files)
-                self.logger.info(
-                    "üìä Progress: %.1f%% complete (%d/%d files, efficiency: %s)",
-                    progress.completion_percentage, progress.completed_files, 
-                    progress.total_files, progress.efficiency_rating
-                )
-            
-            # Step 4: Generate comprehensive execution summary
-            execution_time = time.time() - start_time
-            final_stats = self._generate_intelligent_execution_summary(execution_results, execution_time)
-            
-            # Step 5: Finalize session
-            self.session_manager.finalize_session(final_stats)
-            
-            execution_results["execution_summary"] = final_stats
-            execution_results["session_finalized"] = True
-            
-            return execution_results
-            
-        except Exception as e:
-            self.logger.error("‚ùå Critical error in intelligent audit execution: %s", e)
-            return {
-                "error": str(e),
-                "execution_aborted": True,
-                "emergency_recovery_needed": True,
-                "execution_time": time.time() - start_time
-            }
-
-    def _process_intelligent_batch(
-        self, 
-        batch_files: List[Tuple[str, int, int, str]], 
-        batch_number: int
-    ) -> Dict[str, Any]:
-        """Process a batch of files using intelligent agents."""
-        
-        batch_start_time = time.time()
-        batch_results = {
-            "files_processed": [],
-            "files_deferred": [],
-            "files_failed": [],
-            "token_consumption": {},
-            "batch_metrics": {}
-        }
-        
-        for i, (file_path, estimated_tokens, risk_score, priority_reasons) in enumerate(batch_files):
-            file_start_time = time.time()
-            
-            self.logger.info(
-                "üîç [%d/%d] Analyzing %s (tokens: %d, risk: %d, priority: %s)",
-                i + 1, len(batch_files), file_path, estimated_tokens, risk_score, priority_reasons
-            )
-            
-            try:
-                # Execute intelligent audit
-                audit_result = self._execute_intelligent_audit(
-                    file_path, risk_score, context_quality=85.0  # Default high quality for IA
-                )
-                
-                # Process result
-                if audit_result.get("deferred"):
-                    batch_results["files_deferred"].append({
-                        "file_path": file_path,
-                        "reason": audit_result.get("reason"),
-                        "sleep_time": audit_result.get("sleep_time", 0)
-                    })
-                    
-                    # Sleep if needed
-                    sleep_time = audit_result.get("sleep_time", 0)
-                    if sleep_time > 0:
-                        self.logger.info("üí§ File deferred - sleeping %d seconds", int(sleep_time))
-                        time.sleep(sleep_time)
-                        
-                    # Retry after sleep
-                    audit_result = self._execute_intelligent_audit(file_path, risk_score, 85.0)
-                
-                if audit_result.get("error"):
-                    batch_results["files_failed"].append({
-                        "file_path": file_path,
-                        "error": audit_result.get("error"),
-                        "fallback_used": audit_result.get("fallback_to_legacy", False)
-                    })
-                else:
-                    batch_results["files_processed"].append({
-                        "file_path": file_path,
-                        "tokens_used": audit_result.get("tokens_used", 0),
-                        "lines_analyzed": audit_result.get("lines_analyzed", 0),
-                        "issues_found": audit_result.get("issues_found", 0),
-                        "optimizations_applied": audit_result.get("optimizations_applied", 0),
-                        "intelligent_analysis": audit_result.get("intelligent_analysis", False),
-                        "processing_time": time.time() - file_start_time
-                    })
-                    
-                    # Track token consumption per file
-                    batch_results["token_consumption"][file_path] = audit_result.get("tokens_used", 0)
-                
-                # Record result in session
-                audit_result["processing_time_seconds"] = time.time() - file_start_time
-                self.session_manager.record_file_result(file_path, audit_result)
-                
-            except Exception as e:
-                self.logger.error("‚ùå Critical error processing %s: %s", file_path, e)
-                batch_results["files_failed"].append({
-                    "file_path": file_path,
-                    "error": str(e),
-                    "critical_failure": True
-                })
-        
-        # Calculate batch metrics
-        batch_duration = time.time() - batch_start_time
-        batch_results["batch_metrics"] = {
-            "batch_number": batch_number,
-            "batch_duration_seconds": batch_duration,
-            "files_in_batch": len(batch_files),
-            "files_processed": len(batch_results["files_processed"]),
-            "files_failed": len(batch_results["files_failed"]),
-            "files_deferred": len(batch_results["files_deferred"]),
-            "total_tokens_consumed": sum(batch_results["token_consumption"].values()),
-            "average_processing_time": batch_duration / len(batch_files) if batch_files else 0
-        }
-        
-        self.logger.info(
-            "‚úÖ Batch %d complete: %d processed, %d failed, %d deferred (%.1f seconds)",
-            batch_number, len(batch_results["files_processed"]), len(batch_results["files_failed"]),
-            len(batch_results["files_deferred"]), batch_duration
-        )
-        
-        return batch_results
-
-    def _generate_intelligent_execution_summary(
-        self, 
-        execution_results: Dict[str, Any], 
-        execution_time: float
-    ) -> Dict[str, Any]:
-        """Generate comprehensive execution summary with IA metrics."""
-        
-        # Calculate totals
-        files_processed = len(execution_results.get("files_processed", []))
-        files_failed = len(execution_results.get("files_failed", []))
-        files_deferred = len(execution_results.get("files_deferred", []))
-        total_files = files_processed + files_failed + files_deferred
-        
-        # Calculate token metrics
-        total_tokens = sum(execution_results.get("token_consumption", {}).values())
-        avg_tokens_per_file = total_tokens / files_processed if files_processed > 0 else 0
-        
-        # Calculate success metrics
-        success_rate = (files_processed / total_files * 100) if total_files > 0 else 0
-        failure_rate = (files_failed / total_files * 100) if total_files > 0 else 0
-        
-        # Get comprehensive token stats
-        token_stats = self.token_manager.get_comprehensive_usage_stats()
-        session_summary = self.token_manager.get_session_summary()
-        
-        # Calculate intelligence metrics
-        intelligent_analysis_count = sum(
-            1 for f in execution_results.get("files_processed", [])
-            if f.get("intelligent_analysis", False)
-        )
-        intelligence_usage_rate = (intelligent_analysis_count / files_processed * 100) if files_processed > 0 else 0
-        
-        return {
-            # Executive Summary
-            "executive_summary": {
-                "total_files_audited": total_files,
-                "success_rate_percent": round(success_rate, 2),
-                "intelligent_analysis_rate_percent": round(intelligence_usage_rate, 2),
-                "total_execution_time_hours": round(execution_time / 3600, 2),
-                "average_time_per_file_minutes": round(execution_time / 60 / total_files, 2) if total_files > 0 else 0
-            },
-            
-            # File Processing Results
-            "file_processing": {
-                "successfully_processed": files_processed,
-                "failed_processing": files_failed,
-                "deferred_processing": files_deferred,
-                "total_lines_analyzed": sum(f.get("lines_analyzed", 0) for f in execution_results.get("files_processed", [])),
-                "total_issues_found": sum(f.get("issues_found", 0) for f in execution_results.get("files_processed", [])),
-                "total_optimizations_applied": sum(f.get("optimizations_applied", 0) for f in execution_results.get("files_processed", []))
-            },
-            
-            # Token Consumption Analysis
-            "token_analysis": {
-                "total_tokens_consumed": total_tokens,
-                "average_tokens_per_file": round(avg_tokens_per_file, 2),
-                "efficiency_rating": session_summary.get("session_overview", {}).get("efficiency_rating", "unknown"),
-                "hourly_usage_percent": token_stats.get("hourly_usage_percent", 0),
-                "daily_usage_percent": token_stats.get("daily_usage_percent", 0),
-                "session_usage_percent": token_stats.get("session_usage_percent", 0)
-            },
-            
-            # Performance Metrics  
-            "performance_metrics": {
-                "files_per_hour": round(total_files / (execution_time / 3600), 2) if execution_time > 0 else 0,
-                "tokens_per_minute": token_stats.get("session_rate", 0),
-                "average_processing_time_seconds": round(execution_time / total_files, 2) if total_files > 0 else 0,
-                "efficiency_score": token_stats.get("efficiency_score", 100.0)
-            },
-            
-            # AI Usage Statistics
-            "artificial_intelligence": {
-                "intelligent_code_agent_usage": intelligent_analysis_count,
-                "refactoring_engine_usage": sum(1 for f in execution_results.get("files_processed", []) if f.get("optimizations_applied", 0) > 0),
-                "tdd_workflow_optimizations": sum(1 for f in execution_results.get("files_processed", []) if "tdd" in str(f).lower()),
-                "fallback_to_legacy": sum(1 for f in execution_results.get("files_failed", []) if f.get("fallback_used", False))
-            },
-            
-            # System Health
-            "system_health": {
-                "session_completed_successfully": True,
-                "no_critical_failures": files_failed == 0 or failure_rate < 10,
-                "token_limits_respected": token_stats.get("hourly_usage_percent", 0) <= 100,
-                "adaptive_throttling_used": token_stats.get("adaptive_throttling", False),
-                "checkpoints_saved": True
-            },
-            
-            # Recommendations
-            "recommendations": self._generate_execution_recommendations(
-                success_rate, intelligence_usage_rate, avg_tokens_per_file, execution_time
-            )
-        }
-
-    def _generate_execution_recommendations(
-        self, 
-        success_rate: float, 
-        intelligence_usage_rate: float, 
-        avg_tokens_per_file: float, 
-        execution_time: float
-    ) -> List[str]:
-        """Generate execution recommendations based on metrics."""
-        recommendations = []
-        
-        if success_rate < 90:
-            recommendations.append("Consider increasing batch size for better success rates")
-        
-        if intelligence_usage_rate < 50:
-            recommendations.append("Increase IA usage for better code analysis quality") 
-            
-        if avg_tokens_per_file > 2000:
-            recommendations.append("Consider breaking down large files for better token efficiency")
-            
-        if execution_time > 3600:  # More than 1 hour
-            recommendations.append("Consider running audit in smaller sessions for better management")
-            
-        if not recommendations:
-            recommendations.append("Execution performed optimally - continue with current settings")
-            
-        return recommendations
-
-    def _execute_wave_parallel(self, files: List[str]) -> List[FileAuditResult]:
-        results: List[FileAuditResult] = []
-        for file_path in files:
-            if self._file_exists_in_project(file_path):
-                results.append(self.audit_file_enhanced(file_path))
-                self.logger.info("Wave 1 - %s ‚Üí %s", file_path, results[-1].changes_summary)
-        return results
-
-    def _execute_wave_coordinated(self, files: List[str]) -> List[FileAuditResult]:
-        results: List[FileAuditResult] = []
-        for file_path in files:
-            if self._file_exists_in_project(file_path):
-                dep = self._check_wave_dependencies(file_path)
-                if dep["safe_to_modify"]:
-                    results.append(self.audit_file_enhanced(file_path))
-                    self.logger.info("Wave 2 - %s ‚Üí %s", file_path, results[-1].changes_summary)
-                else:
-                    self.logger.warning("Wave 2 - DEFER %s: %s", file_path, dep["reason"])
-        return results
-
-    def _execute_wave_sequential(self, files: List[str]) -> List[FileAuditResult]:
-        results: List[FileAuditResult] = []
-        for file_path in files:
-            if self._file_exists_in_project(file_path):
-                result = self.audit_file_enhanced(file_path)
-                results.append(result)
-                if result.issues_found > 0:
-                    health = self._validate_system_health()
-                    if not health["healthy"]:
-                        self.logger.error("Sa√∫de degradada ap√≥s %s. Interrompendo wave.", file_path)
-                        break
-                self.logger.info("Wave 3 - %s ‚Üí %s", file_path, result.changes_summary)
-        return results
-
-    def _execute_wave_critical(self, files: List[str]) -> List[FileAuditResult]:
-        results: List[FileAuditResult] = []
-        for file_path in files:
-            if self._file_exists_in_project(file_path):
-                backup_id = self._create_full_system_backup()
-                try:
-                    result = self.audit_file_enhanced(file_path)
-                    if result.integration_tests_passed:
-                        results.append(result)
-                        self.logger.info("Wave 4 - SUCCESS %s ‚Üí %s", file_path, result.changes_summary)
-                    else:
-                        self._restore_system_backup(backup_id)
-                        self.logger.error("Wave 4 - ROLLBACK %s (testes falharam)", file_path)
-                except Exception as e:  # pragma: no cover
-                    self._restore_system_backup(backup_id)
-                    self.logger.critical("Wave 4 - EMERGENCY ROLLBACK %s: %s", file_path, e)
-        return results
-
-    # ---------------------------------------------------------------- sistema
-    def _file_exists_in_project(self, file_path: str) -> bool:
-        return (self.project_root / file_path).exists()
-
-    def _check_wave_dependencies(self, file_path: str) -> Dict[str, Any]:
-        return {"safe_to_modify": True, "reason": "Dependencies satisfied", "dependencies_checked": []}
-
-    def _try_resume_session(self, resume_session_id: Optional[str] = None) -> Tuple[bool, Dict[str, Any]]:
-        """Try to resume an existing session."""
-        resumed, session_id, resume_info = self.session_manager.resume_session(resume_session_id)
-        
-        if resumed:
-            self.logger.info("üîÑ Resumed session: %s", resume_info)
-            return True, resume_info
-        
-        return False, {}
-
-    def _start_new_audit_session(self, total_files: int, config: Dict[str, Any]) -> str:
-        """Start a new audit session with tracking."""
-        session_id = self.session_manager.start_new_session(total_files, config)
-        self.logger.info("üöÄ Started new audit session: %s", session_id)
-        return session_id
-
-    def _validate_system_health(self) -> Dict[str, Any]:
-        try:
-            # Teste simples de conectividade do banco de dados
-            try:
-                # Usar m√©todo seguro da API legada
-                result = self.db_manager.get_all_epics()  # type: ignore
-                if result is not None:
-                    return {"healthy": True, "checks_passed": ["database_connectivity"], "issues": []}
-                else:
-                    return {"healthy": False, "checks_passed": [], "issues": ["Database returned None"]}
-            except Exception as e:
-                # Fallback: tentar conex√£o direta
-                try:
-                    framework_db_path = self.db_manager.framework_db_path if hasattr(self.db_manager, 'framework_db_path') else "framework.db"
-                    with sqlite3.connect(framework_db_path) as conn:
-                        cursor = conn.cursor()
-                        cursor.execute("SELECT 1")
-                        cursor.fetchone()
-                    return {"healthy": True, "checks_passed": ["database_connectivity"], "issues": []}
-                except Exception as e2:
-                    return {"healthy": False, "checks_passed": [], "issues": [f"Database connectivity failed: {e2}"]}
-        except Exception as e:
-            return {"healthy": False, "checks_passed": [], "issues": [f"Health check error: {e}"]}
-
-    def _create_full_system_backup(self) -> str:
-        bid = f"critical_backup_{int(time.time())}"
-        self.logger.info("Criando backup completo do sistema: %s", bid)
-        # Em produ√ß√£o: snapshot real
-        return bid
-
-    def _restore_system_backup(self, backup_id: str) -> bool:
-        self.logger.warning("Restaurando sistema a partir do backup: %s", backup_id)
-        # Em produ√ß√£o: restaura√ß√£o real
-        return True
-
-    def _generate_execution_summary(self, results: Dict[str, Any], duration_seconds: float) -> Dict[str, Any]:
-        """Gera relat√≥rio detalhado com m√©tricas abrangentes."""
-        waves = [k for k in results.keys() if k.startswith("WAVE_")]
-        
-        # M√©tricas b√°sicas
-        total_files = sum(len(results[w]) for w in waves)
-        total_optimizations = 0
-        successes = 0
-        rollbacks = 0
-        
-        # M√©tricas detalhadas
-        pattern_metrics = {}
-        quality_metrics = {"total_quality": 0.0, "avg_quality": 0.0, "quality_distribution": {}}
-        risk_metrics = {"total_risk": 0, "avg_risk": 0.0, "risk_distribution": {}}
-        performance_metrics = {"files_per_second": 0.0, "avg_file_size": 0, "total_lines": 0}
-        
-        for w in waves:
-            for r in results[w]:
-                if isinstance(r, FileAuditResult):
-                    total_optimizations += r.optimizations_applied
-                    
-                    if r.syntax_valid and not r.changes_summary.startswith("ROLLED BACK"):
-                        successes += 1
-                    elif r.changes_summary.startswith("ROLLED BACK"):
-                        rollbacks += 1
-                    
-                    # Pattern metrics
-                    for pattern in r.patterns_found or []:
-                        pattern_metrics[pattern] = pattern_metrics.get(pattern, 0) + 1
-                    
-                    # Quality metrics
-                    quality_metrics["total_quality"] += r.context_quality
-                    quality_range = self._get_quality_range(r.context_quality)
-                    quality_metrics["quality_distribution"][quality_range] = \
-                        quality_metrics["quality_distribution"].get(quality_range, 0) + 1
-                    
-                    # Risk metrics
-                    risk_metrics["total_risk"] += r.risk_score
-                    risk_metrics["risk_distribution"][r.risk_category] = \
-                        risk_metrics["risk_distribution"].get(r.risk_category, 0) + 1
-                    
-                    # Performance metrics
-                    performance_metrics["total_lines"] += r.lines_analyzed
-        
-        # Calcular m√©dias
-        if total_files > 0:
-            quality_metrics["avg_quality"] = round(quality_metrics["total_quality"] / total_files, 2)
-            risk_metrics["avg_risk"] = round(risk_metrics["total_risk"] / total_files, 2)
-            performance_metrics["avg_file_size"] = round(performance_metrics["total_lines"] / total_files, 0)
-        
-        if duration_seconds > 0:
-            performance_metrics["files_per_second"] = round(total_files / duration_seconds, 2)
-        
-        success_rate = (successes / max(total_files, 1)) * 100.0
-        rollback_rate = (rollbacks / max(total_files, 1)) * 100.0
-        
-        return {
-            # M√©tricas b√°sicas
-            "total_files_processed": total_files,
-            "total_optimizations_applied": total_optimizations,
-            "waves_completed": len(waves),
-            "duration_seconds": round(duration_seconds, 3),
-            "success_rate": round(success_rate, 2),
-            "rollback_rate": round(rollback_rate, 2),
-            
-            # Distribui√ß√£o por waves
-            "risk_distribution": {
-                "low_risk_processed": len(results.get("WAVE_1_LOW", [])),
-                "medium_risk_processed": len(results.get("WAVE_2_MEDIUM", [])),
-                "high_risk_processed": len(results.get("WAVE_3_HIGH", [])),
-                "critical_risk_processed": len(results.get("WAVE_4_CRITICAL", [])),
-            },
-            
-            # M√©tricas detalhadas
-            "pattern_metrics": dict(sorted(pattern_metrics.items(), key=lambda x: x[1], reverse=True)),
-            "quality_metrics": quality_metrics,
-            "risk_metrics": risk_metrics,
-            "performance_metrics": performance_metrics,
-            
-            # Assessment geral
-            "assessment": self._generate_assessment(
-                success_rate, quality_metrics["avg_quality"], risk_metrics["avg_risk"], total_optimizations
-            ),
-            
-            # Timestamp
-            "generated_at": datetime.now().isoformat(),
-        }
-
-    def _get_quality_range(self, quality: float) -> str:
-        """Categoriza qualidade em ranges."""
-        if quality >= 90:
-            return "excellent"
-        elif quality >= 75:
-            return "good"
-        elif quality >= 60:
-            return "acceptable"
-        elif quality >= 40:
-            return "poor"
-        else:
-            return "critical"
-
-    def _generate_assessment(
-        self, 
-        success_rate: float, 
-        avg_quality: float, 
-        avg_risk: float, 
-        total_optimizations: int
-    ) -> Dict[str, Any]:
-        """Gera assessment geral da execu√ß√£o."""
-        grade = "F"
-        status = "FAILED"
-        recommendations = []
-        
-        # Calcular grade baseado em m√∫ltiplos fatores
-        score = 0
-        
-        # Success rate (40 pontos m√°ximo)
-        if success_rate >= 95:
-            score += 40
-        elif success_rate >= 85:
-            score += 30
-        elif success_rate >= 70:
-            score += 20
-        elif success_rate >= 50:
-            score += 10
-        
-        # Quality (30 pontos m√°ximo)
-        if avg_quality >= 85:
-            score += 30
-        elif avg_quality >= 70:
-            score += 20
-        elif avg_quality >= 55:
-            score += 15
-        elif avg_quality >= 40:
-            score += 10
-        
-        # Risk management (20 pontos m√°ximo)
-        if avg_risk <= 35:
-            score += 20
-        elif avg_risk <= 70:
-            score += 15
-        elif avg_risk <= 105:
-            score += 10
-        elif avg_risk <= 140:
-            score += 5
-        
-        # Optimizations (10 pontos m√°ximo)
-        if total_optimizations >= 20:
-            score += 10
-        elif total_optimizations >= 10:
-            score += 7
-        elif total_optimizations >= 5:
-            score += 5
-        elif total_optimizations >= 1:
-            score += 3
-        
-        # Determinar grade e status
-        if score >= 90:
-            grade, status = "A+", "EXCELLENT"
-        elif score >= 85:
-            grade, status = "A", "EXCELLENT"
-        elif score >= 80:
-            grade, status = "B+", "GOOD"
-        elif score >= 75:
-            grade, status = "B", "GOOD"
-        elif score >= 70:
-            grade, status = "C+", "ACCEPTABLE"
-        elif score >= 65:
-            grade, status = "C", "ACCEPTABLE"
-        elif score >= 60:
-            grade, status = "D+", "POOR"
-        elif score >= 55:
-            grade, status = "D", "POOR"
-        else:
-            grade, status = "F", "FAILED"
-        
-        # Gerar recomenda√ß√µes
-        if success_rate < 85:
-            recommendations.append("Investigate failed files and improve error handling")
-        if avg_quality < 70:
-            recommendations.append("Focus on improving code documentation and type hints")
-        if avg_risk > 70:
-            recommendations.append("Address high-risk files with additional safety measures")
-        if total_optimizations < 5:
-            recommendations.append("Review pattern detection to find more optimization opportunities")
-        
-        return {
-            "grade": grade,
-            "status": status,
-            "score": score,
-            "max_score": 100,
-            "recommendations": recommendations,
-            "summary": f"Grade {grade} ({status}) - {score}/100 points"
-        }
-
-
-# =============================================================================
-# Enterprise Session Manager - Persistent session with intelligent recovery
-# =============================================================================
-@dataclass
-class SessionCheckpoint:
-    """Detailed session checkpoint for recovery."""
-    session_id: str
-    timestamp: datetime
-    files_completed: List[str]
-    files_failed: List[str]
-    files_deferred: List[str]
-    current_batch: List[str]
-    token_consumption_history: Dict[str, int]
-    performance_metrics: Dict[str, Any]
-    session_config: Dict[str, Any]
-    recovery_metadata: Dict[str, Any]
-
-
-@dataclass
-class AuditProgress:
-    """Comprehensive audit progress tracking."""
-    total_files: int
-    completed_files: int
-    failed_files: int
-    deferred_files: int
-    completion_percentage: float
-    estimated_remaining_time_hours: float
-    total_tokens_consumed: int
-    average_tokens_per_file: float
-    efficiency_rating: str
-
-
-class EnterpriseSessionManager:
-    """
-    Enterprise-grade session management with intelligent recovery and distributed execution support.
-    """
-    
-    def __init__(self, db_manager: DatabaseManager, project_root: Path):
-        self.db_manager = db_manager
-        self.project_root = project_root
-        self.logger = logging.getLogger(f"{__name__}.EnterpriseSessionManager")
-        
-        # Session state
-        self.current_session_id: Optional[str] = None
-        self.session_start_time: Optional[datetime] = None
-        self.checkpoints: List[SessionCheckpoint] = []
-        
-        # Progress tracking
-        self.files_completed: Set[str] = set()
-        self.files_failed: Set[str] = set()
-        self.files_deferred: Set[str] = set()
-        self.file_results: Dict[str, Dict[str, Any]] = {}
-        
-        # Recovery state
-        self.is_resumed_session = False
-        self.original_session_id: Optional[str] = None
-        
-        self._ensure_session_tables()
-        self.logger.info("EnterpriseSessionManager initialized")
-    
-    def _get_db_connection(self):
-        """Get database connection consistently."""
-        framework_db_path = self.db_manager.framework_db_path if hasattr(self.db_manager, 'framework_db_path') else "framework.db"
-        return sqlite3.connect(framework_db_path)
-    
-    def _ensure_session_tables(self) -> None:
-        """Ensure session tracking tables exist in database."""
-        try:
-            # Use existing database connection - DatabaseManager returns sqlite connection
-            framework_db_path = self.db_manager.framework_db_path if hasattr(self.db_manager, 'framework_db_path') else "framework.db"
-            
-            with sqlite3.connect(framework_db_path) as conn:
-                cursor = conn.cursor()
-                
-                # Session metadata table
-                cursor.execute("""
-                    CREATE TABLE IF NOT EXISTS audit_sessions (
-                        session_id TEXT PRIMARY KEY,
-                        start_time TEXT NOT NULL,
-                        end_time TEXT,
-                        status TEXT NOT NULL DEFAULT 'active',
-                        total_files INTEGER,
-                        completed_files INTEGER DEFAULT 0,
-                        failed_files INTEGER DEFAULT 0,
-                        deferred_files INTEGER DEFAULT 0,
-                        total_tokens_consumed INTEGER DEFAULT 0,
-                        session_config TEXT,
-                        metadata TEXT,
-                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
-                        updated_at TEXT DEFAULT CURRENT_TIMESTAMP
-                    )
-                """)
-                
-                # File processing results table
-                cursor.execute("""
-                    CREATE TABLE IF NOT EXISTS audit_file_results (
-                        id INTEGER PRIMARY KEY AUTOINCREMENT,
-                        session_id TEXT NOT NULL,
-                        file_path TEXT NOT NULL,
-                        status TEXT NOT NULL,
-                        tokens_used INTEGER DEFAULT 0,
-                        lines_analyzed INTEGER DEFAULT 0,
-                        issues_found INTEGER DEFAULT 0,
-                        optimizations_applied INTEGER DEFAULT 0,
-                        processing_time_seconds REAL DEFAULT 0,
-                        analysis_result TEXT,
-                        error_message TEXT,
-                        timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
-                        FOREIGN KEY (session_id) REFERENCES audit_sessions (session_id)
-                    )
-                """)
-                
-                # Checkpoint table for recovery
-                cursor.execute("""
-                    CREATE TABLE IF NOT EXISTS audit_checkpoints (
-                        id INTEGER PRIMARY KEY AUTOINCREMENT,
-                        session_id TEXT NOT NULL,
-                        checkpoint_data TEXT NOT NULL,
-                        timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
-                        FOREIGN KEY (session_id) REFERENCES audit_sessions (session_id)
-                    )
-                """)
-                
-                conn.commit()
-                self.logger.debug("Session tables ensured in database")
-                
-        except Exception as e:
-            self.logger.error("Failed to ensure session tables: %s", e)
-    
-    def start_new_session(self, total_files: int, session_config: Dict[str, Any]) -> str:
-        """Start a new audit session with comprehensive tracking."""
-        self.current_session_id = f"audit_{int(time.time())}_{os.getpid()}"
-        self.session_start_time = datetime.now()
-        self.is_resumed_session = False
-        
-        try:
-            framework_db_path = self.db_manager.framework_db_path if hasattr(self.db_manager, 'framework_db_path') else "framework.db"
-            
-            with sqlite3.connect(framework_db_path) as conn:
-                cursor = conn.cursor()
-                cursor.execute("""
-                    INSERT INTO audit_sessions 
-                    (session_id, start_time, status, total_files, session_config, metadata)
-                    VALUES (?, ?, ?, ?, ?, ?)
-                """, (
-                    self.current_session_id,
-                    self.session_start_time.isoformat(),
-                    'active',
-                    total_files,
-                    json.dumps(session_config),
-                    json.dumps({
-                        "pid": os.getpid(),
-                        "hostname": os.uname().nodename,
-                        "python_version": sys.version,
-                        "intelligent_mode": session_config.get("intelligent_mode", False)
-                    })
-                ))
-                conn.commit()
-                
-            self.logger.info("‚úÖ New audit session started: %s (%d files)", 
-                           self.current_session_id, total_files)
-            
-            return self.current_session_id
-            
-        except Exception as e:
-            self.logger.error("Failed to start new session: %s", e)
-            raise
-    
-    def resume_session(self, session_id: Optional[str] = None) -> Tuple[bool, Optional[str], Dict[str, Any]]:
-        """Resume the most recent incomplete session or specific session."""
-        try:
-            framework_db_path = self.db_manager.framework_db_path if hasattr(self.db_manager, 'framework_db_path') else "framework.db"
-            with sqlite3.connect(framework_db_path) as conn:
-                cursor = conn.cursor()
-                
-                if session_id:
-                    # Resume specific session
-                    cursor.execute("""
-                        SELECT session_id, start_time, total_files, completed_files, 
-                               failed_files, deferred_files, session_config, metadata
-                        FROM audit_sessions 
-                        WHERE session_id = ? AND status = 'active'
-                    """, (session_id,))
-                else:
-                    # Resume most recent incomplete session
-                    cursor.execute("""
-                        SELECT session_id, start_time, total_files, completed_files,
-                               failed_files, deferred_files, session_config, metadata
-                        FROM audit_sessions 
-                        WHERE status = 'active'
-                        ORDER BY start_time DESC LIMIT 1
-                    """)
-                
-                session_row = cursor.fetchone()
-                
-                if not session_row:
-                    self.logger.info("No active session found to resume")
-                    return False, None, {}
-                
-                # Load session data
-                session_data = dict(session_row)
-                self.current_session_id = session_data['session_id']
-                self.original_session_id = self.current_session_id
-                self.session_start_time = datetime.fromisoformat(session_data['start_time'])
-                self.is_resumed_session = True
-                
-                # Load file completion status
-                cursor.execute("""
-                    SELECT file_path, status 
-                    FROM audit_file_results 
-                    WHERE session_id = ?
-                """, (self.current_session_id,))
-                
-                for file_path, status in cursor.fetchall():
-                    if status == 'completed':
-                        self.files_completed.add(file_path)
-                    elif status == 'failed':
-                        self.files_failed.add(file_path)
-                    elif status == 'deferred':
-                        self.files_deferred.add(file_path)
-                
-                resume_info = {
-                    "session_id": self.current_session_id,
-                    "files_completed": len(self.files_completed),
-                    "files_failed": len(self.files_failed),
-                    "files_deferred": len(self.files_deferred),
-                    "total_files": session_data['total_files'],
-                    "session_config": json.loads(session_data.get('session_config', '{}')),
-                    "metadata": json.loads(session_data.get('metadata', '{}'))
-                }
-                
-                self.logger.info("‚úÖ Resumed session %s: %d completed, %d failed, %d deferred",
-                               self.current_session_id, len(self.files_completed), 
-                               len(self.files_failed), len(self.files_deferred))
-                
-                return True, self.current_session_id, resume_info
-                
-        except Exception as e:
-            self.logger.error("Failed to resume session: %s", e)
-            return False, None, {}
-    
-    def save_checkpoint(self, current_batch: List[str], token_manager_stats: Dict[str, Any]) -> bool:
-        """Save detailed checkpoint for recovery."""
-        if not self.current_session_id:
-            return False
-            
-        try:
-            checkpoint = SessionCheckpoint(
-                session_id=self.current_session_id,
-                timestamp=datetime.now(),
-                files_completed=list(self.files_completed),
-                files_failed=list(self.files_failed),
-                files_deferred=list(self.files_deferred),
-                current_batch=current_batch,
-                token_consumption_history=getattr(token_manager_stats, 'file_consumption_history', {}),
-                performance_metrics=getattr(token_manager_stats, 'performance_metrics', {}),
-                session_config={
-                    "is_resumed": self.is_resumed_session,
-                    "original_session_id": self.original_session_id
-                },
-                recovery_metadata={
-                    "checkpoint_count": len(self.checkpoints) + 1,
-                    "last_checkpoint": datetime.now().isoformat()
-                }
-            )
-            
-            # Save to database
-            framework_db_path = self.db_manager.framework_db_path if hasattr(self.db_manager, 'framework_db_path') else "framework.db"
-            with sqlite3.connect(framework_db_path) as conn:
-                cursor = conn.cursor()
-                cursor.execute("""
-                    INSERT INTO audit_checkpoints (session_id, checkpoint_data)
-                    VALUES (?, ?)
-                """, (self.current_session_id, json.dumps(checkpoint.__dict__, default=str)))
-                
-                # Update session progress
-                cursor.execute("""
-                    UPDATE audit_sessions 
-                    SET completed_files = ?, failed_files = ?, deferred_files = ?,
-                        total_tokens_consumed = ?, updated_at = CURRENT_TIMESTAMP
-                    WHERE session_id = ?
-                """, (
-                    len(self.files_completed),
-                    len(self.files_failed), 
-                    len(self.files_deferred),
-                    token_manager_stats.get('tokens_used_today', 0),
-                    self.current_session_id
-                ))
-                
-                conn.commit()
-            
-            self.checkpoints.append(checkpoint)
-            self.logger.debug("Checkpoint saved for session %s", self.current_session_id)
-            return True
-            
-        except Exception as e:
-            self.logger.error("Failed to save checkpoint: %s", e)
-            return False
-    
-    def record_file_result(self, file_path: str, result: Dict[str, Any]) -> bool:
-        """Record detailed result for a file."""
-        if not self.current_session_id:
-            return False
-            
-        try:
-            status = "completed" if result.get('modified', False) or not result.get('error') else "failed"
-            if result.get('deferred'):
-                status = "deferred"
-            
-            # Update local tracking
-            if status == "completed":
-                self.files_completed.add(file_path)
-                self.files_failed.discard(file_path)
-                self.files_deferred.discard(file_path)
-            elif status == "failed":
-                self.files_failed.add(file_path)
-                self.files_completed.discard(file_path)
-                self.files_deferred.discard(file_path)
-            elif status == "deferred":
-                self.files_deferred.add(file_path)
-                self.files_completed.discard(file_path)
-                self.files_failed.discard(file_path)
-            
-            # Store detailed result
-            self.file_results[file_path] = result
-            
-            # Save to database
-            framework_db_path = self.db_manager.framework_db_path if hasattr(self.db_manager, 'framework_db_path') else "framework.db"
-            with sqlite3.connect(framework_db_path) as conn:
-                cursor = conn.cursor()
-                cursor.execute("""
-                    INSERT OR REPLACE INTO audit_file_results
-                    (session_id, file_path, status, tokens_used, lines_analyzed, 
-                     issues_found, optimizations_applied, processing_time_seconds, 
-                     analysis_result, error_message)
-                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
-                """, (
-                    self.current_session_id,
-                    file_path,
-                    status,
-                    result.get('tokens_used', 0),
-                    result.get('lines_analyzed', 0),
-                    result.get('issues_found', 0),
-                    result.get('optimizations_applied', 0),
-                    result.get('processing_time_seconds', 0),
-                    json.dumps(result, default=str),
-                    result.get('error', '')
-                ))
-                conn.commit()
-            
-            return True
-            
-        except Exception as e:
-            self.logger.error("Failed to record file result for %s: %s", file_path, e)
-            return False
-    
-    def get_remaining_files(self, all_files: List[str]) -> List[str]:
-        """Get list of files that still need processing."""
-        processed_files = self.files_completed | self.files_failed
-        return [f for f in all_files if f not in processed_files]
-    
-    def get_audit_progress(self, total_files: int) -> AuditProgress:
-        """Get comprehensive audit progress information."""
-        completed = len(self.files_completed)
-        failed = len(self.files_failed)
-        deferred = len(self.files_deferred)
-        
-        completion_percentage = (completed / total_files * 100) if total_files > 0 else 0
-        
-        # Estimate remaining time
-        if self.session_start_time and completed > 0:
-            elapsed_hours = (datetime.now() - self.session_start_time).total_seconds() / 3600
-            rate_files_per_hour = completed / elapsed_hours
-            remaining_files = total_files - completed
-            estimated_remaining_hours = remaining_files / rate_files_per_hour if rate_files_per_hour > 0 else 0
-        else:
-            estimated_remaining_hours = 0
-        
-        # Calculate efficiency rating
-        total_tokens = sum(r.get('tokens_used', 0) for r in self.file_results.values())
-        avg_tokens_per_file = total_tokens / completed if completed > 0 else 0
-        
-        if avg_tokens_per_file < 5000:
-            efficiency = "excellent"
-        elif avg_tokens_per_file < 10000:
-            efficiency = "good"  
-        elif avg_tokens_per_file < 20000:
-            efficiency = "fair"
-        else:
-            efficiency = "needs_optimization"
-        
-        return AuditProgress(
-            total_files=total_files,
-            completed_files=completed,
-            failed_files=failed,
-            deferred_files=deferred,
-            completion_percentage=completion_percentage,
-            estimated_remaining_time_hours=estimated_remaining_hours,
-            total_tokens_consumed=total_tokens,
-            average_tokens_per_file=avg_tokens_per_file,
-            efficiency_rating=efficiency
-        )
-    
-    def finalize_session(self, final_stats: Dict[str, Any]) -> bool:
-        """Finalize the audit session with comprehensive results."""
-        if not self.current_session_id:
-            return False
-            
-        try:
-            framework_db_path = self.db_manager.framework_db_path if hasattr(self.db_manager, 'framework_db_path') else "framework.db"
-            with sqlite3.connect(framework_db_path) as conn:
-                cursor = conn.cursor()
-                cursor.execute("""
-                    UPDATE audit_sessions 
-                    SET end_time = ?, status = 'completed',
-                        metadata = ?
-                    WHERE session_id = ?
-                """, (
-                    datetime.now().isoformat(),
-                    json.dumps(final_stats, default=str),
-                    self.current_session_id
-                ))
-                conn.commit()
-            
-            self.logger.info("‚úÖ Session %s finalized successfully", self.current_session_id)
-            return True
-            
-        except Exception as e:
-            self.logger.error("Failed to finalize session: %s", e)
-            return False
-
-
-# =============================================================================
-# Tracking no banco (leve, sem schema novo)
-# =============================================================================
-class DatabaseTracker:
-    def __init__(self, db_manager: DatabaseManager):
-        self.db_manager = db_manager
-        self.logger = logging.getLogger(f"{__name__}.DatabaseTracker")
-        self._initialize_tables()
-
-    def _initialize_tables(self) -> None:
-        try:
-            existing = self.db_manager.query("SELECT name FROM sqlite_master WHERE type='table'", database="framework")  # type: ignore
-            _ = [row["name"] for row in existing]  # n√£o usado ainda, mas mant√©m verifica√ß√£o
-            self.logger.info("Tabelas verificadas/ok (usando estrutura existente)")
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao verificar/ini tabelas: %s (usando estrutura existente)", e)
-
-    def create_session(self, total_files: int) -> int:
-        try:
-            sid = int(datetime.now().timestamp())
-            self.logger.info("Sess√£o criada %s para %d arquivos", sid, total_files)
-            return sid
-        except Exception:  # pragma: no cover
-            return int(time.time())
-
-    def get_current_session(self) -> Optional[AuditSession]:
-        return None
-
-    def update_session_progress(self, session_id: int, file_index: int, tokens_used: int) -> None:
-        try:
-            self.logger.debug("Sess√£o %s: arquivo %s, tokens %s", session_id, file_index, tokens_used)
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao atualizar progresso da sess√£o: %s", e)
-
-    def initialize_file_list(self, file_paths: List[str]) -> None:
-        try:
-            self.file_list = file_paths
-            self.file_status = {p: AuditStatus.PENDING.value for p in file_paths}
-            self.logger.info("Lista de %d arquivos inicializada", len(file_paths))
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao inicializar lista de arquivos: %s", e)
-
-    def mark_file_in_progress(self, file_path: str) -> None:
-        try:
-            if hasattr(self, "file_status"):
-                self.file_status[file_path] = AuditStatus.IN_PROGRESS.value
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao marcar IN_PROGRESS: %s", e)
-
-    def mark_file_completed(self, file_path: str, result: FileAuditResult) -> None:
-        try:
-            if hasattr(self, "file_status"):
-                self.file_status[file_path] = AuditStatus.COMPLETED.value
-            self.logger.info("Conclu√≠da auditoria de %s: %s", file_path, result.changes_summary)
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao marcar COMPLETED: %s", e)
-
-    def mark_file_failed(self, file_path: str, error_message: str) -> None:
-        try:
-            if hasattr(self, "file_status"):
-                self.file_status[file_path] = AuditStatus.FAILED.value
-            self.logger.error("Falha na auditoria de %s: %s", file_path, error_message)
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao marcar FAILED: %s", e)
-
-    def get_pending_files(self) -> List[str]:
-        try:
-            if hasattr(self, "file_status"):
-                return [p for p, s in self.file_status.items() if s == AuditStatus.PENDING.value]
-            return []
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao obter pendentes: %s", e)
-            return []
-
-    def get_audit_summary(self) -> Dict[str, Any]:
-        try:
-            if hasattr(self, "file_status"):
-                status_counts: Dict[str, int] = {}
-                for s in self.file_status.values():
-                    status_counts[s] = status_counts.get(s, 0) + 1
-                total = len(self.file_status)
-                completed = status_counts.get("completed", 0)
-                rate = (completed / total * 100.0) if total else 0.0
-                return {
-                    "statistics": [{"status": k, "count": v} for k, v in status_counts.items()],
-                    "total_files": total,
-                    "completion_rate": rate,
-                    "last_updated": datetime.now().isoformat(),
-                }
-            return {"statistics": [], "total_files": 0, "completion_rate": 0.0, "last_updated": datetime.now().isoformat()}
-        except Exception as e:  # pragma: no cover
-            self.logger.warning("Falha ao gerar resumo da auditoria: %s", e)
-            return {"error": str(e), "total_files": 0, "completion_rate": 0.0}
-
-
-# =============================================================================
-# Smart Token Budget Manager - Advanced token management with AI analysis estimation
-# =============================================================================
-class SmartTokenBudgetManager:
-    def __init__(self, max_tokens_per_hour: int = 40000):
-        self.max_tokens_per_hour = max_tokens_per_hour
-        self.daily_budget = max_tokens_per_hour * 20  # 800K tokens per day
-        self.session_limit = int(max_tokens_per_hour * 0.8)  # 32K per session (safety buffer)
-        
-        # Current usage tracking
-        self.tokens_used_this_hour = 0
-        self.tokens_used_today = 0
-        self.tokens_used_this_session = 0
-        
-        # Time tracking
-        self.hour_start = time.time()
-        self.day_start = time.time()
-        self.session_start = time.time()
-        
-        # Advanced tracking
-        self.token_history: List[Dict[str, Any]] = []
-        self.file_consumption_history: Dict[str, int] = {}
-        self.performance_metrics: Dict[str, Any] = {}
-        
-        # AI-specific settings
-        self.intelligent_mode = True
-        self.adaptive_throttling = True
-        self.risk_based_scaling = True
-        self.predictive_analysis = True
-        
-        self.logger = logging.getLogger(f"{__name__}.SmartTokenBudgetManager")
-        self.logger.info(
-            "SmartTokenBudgetManager initialized: %d tokens/hour, %d daily budget, intelligent mode: %s",
-            max_tokens_per_hour, self.daily_budget, self.intelligent_mode
-        )
-
-    def _reset_time_windows_if_needed(self) -> None:
-        """Reset time windows for hour, day, and session if needed."""
-        current_time = time.time()
-        
-        # Reset hour window
-        if current_time - self.hour_start >= 3600:
-            self.tokens_used_this_hour = 0
-            self.hour_start = current_time
-            self.logger.info("Hourly token window reset")
-        
-        # Reset day window (24 hours)
-        if current_time - self.day_start >= 86400:
-            self.tokens_used_today = 0
-            self.day_start = current_time
-            self.logger.info("Daily token budget reset: %d tokens available", self.daily_budget)
-        
-        # Session timeout (6 hours max)
-        if current_time - self.session_start >= 21600:
-            self.tokens_used_this_session = 0
-            self.session_start = current_time
-            self.logger.info("Session token window reset")
-
-    def estimate_file_tokens(self, file_path: str, project_root: Path) -> int:
-        """
-        Estimate tokens required for intelligent analysis of a file.
-        Based on file size, complexity, and historical data.
-        """
-        try:
-            full_path = project_root / file_path
-            if not full_path.exists():
-                return 1000  # Default for missing files
-            
-            # Get file metrics
-            with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
-                content = f.read()
-            
-            lines = content.count('\n') + 1
-            
-            # Base estimation: tokens per line for IA analysis
-            base_tokens_per_line = 25 if self.intelligent_mode else 5
-            base_estimate = lines * base_tokens_per_line
-            
-            # Complexity multipliers
-            complexity_multiplier = 1.0
-            
-            # AST analysis complexity
-            try:
-                ast_tree = ast.parse(content)
-                ast_nodes = len(list(ast.walk(ast_tree)))
-                if ast_nodes > 500:
-                    complexity_multiplier *= 1.5  # Complex files
-                elif ast_nodes > 200:
-                    complexity_multiplier *= 1.2  # Medium complexity
-            except SyntaxError:
-                complexity_multiplier *= 0.8  # Syntax errors = simpler analysis
-            
-            # File type multipliers
-            if 'test' in file_path.lower():
-                complexity_multiplier *= 0.7  # Test files are usually simpler
-            elif any(critical in file_path.lower() for critical in ['database', 'security', 'auth', 'middleware']):
-                complexity_multiplier *= 1.8  # Critical files need deeper analysis
-            elif file_path.endswith('__init__.py'):
-                complexity_multiplier *= 0.3  # Init files are usually simple
-            
-            # Size-based adjustments
-            if lines > 1000:
-                complexity_multiplier *= 1.4  # Large files
-            elif lines < 50:
-                complexity_multiplier *= 0.6  # Small files
-            
-            # Historical data adjustment
-            if file_path in self.file_consumption_history:
-                historical_usage = self.file_consumption_history[file_path]
-                # Average with historical data (70% historical, 30% estimation)
-                estimated = int(base_estimate * complexity_multiplier)
-                final_estimate = int(historical_usage * 0.7 + estimated * 0.3)
-            else:
-                final_estimate = int(base_estimate * complexity_multiplier)
-            
-            # Safety bounds
-            final_estimate = max(500, min(final_estimate, 50000))  # Between 500-50K tokens
-            
-            self.logger.debug("Token estimation for %s: %d tokens (lines: %d, multiplier: %.2f)", 
-                             file_path, final_estimate, lines, complexity_multiplier)
-            
-            return final_estimate
-            
-        except Exception as e:
-            self.logger.warning("Failed to estimate tokens for %s: %s", file_path, e)
-            return 5000  # Conservative fallback
-
-    def can_proceed(self, estimated_tokens: int, file_path: Optional[str] = None) -> bool:
-        """Enhanced token checking with multiple budget levels."""
-        self._reset_time_windows_if_needed()
-        
-        # Check session limit (most restrictive)
-        if self.tokens_used_this_session + estimated_tokens > self.session_limit:
-            self.logger.warning(
-                "Session token limit would be exceeded: %d + %d > %d for file %s",
-                self.tokens_used_this_session, estimated_tokens, self.session_limit, file_path
-            )
-            return False
-        
-        # Check hourly limit
-        if self.tokens_used_this_hour + estimated_tokens > self.max_tokens_per_hour:
-            self.logger.warning(
-                "Hourly token limit would be exceeded: %d + %d > %d for file %s",
-                self.tokens_used_this_hour, estimated_tokens, self.max_tokens_per_hour, file_path
-            )
-            return False
-        
-        # Check daily budget (soft limit - warning only)
-        if self.tokens_used_today + estimated_tokens > self.daily_budget:
-            self.logger.warning(
-                "Daily token budget would be exceeded: %d + %d > %d for file %s - proceeding with caution",
-                self.tokens_used_today, estimated_tokens, self.daily_budget, file_path
-            )
-            # Still proceed but log warning
-        
-        return True
-
-    def calculate_intelligent_sleep_time(self) -> float:
-        """Calculate adaptive sleep time based on multiple factors."""
-        self._reset_time_windows_if_needed()
-        
-        current_time = time.time()
-        
-        # Calculate different usage rates
-        hour_elapsed = current_time - self.hour_start
-        session_elapsed = current_time - self.session_start
-        
-        if hour_elapsed < 60:
-            return 0.0  # No throttling in first minute
-        
-        # Hourly rate calculation
-        hour_usage_rate = self.tokens_used_this_hour / (hour_elapsed / 60)
-        hour_target_rate = self.max_tokens_per_hour / 60
-        
-        # Session rate calculation  
-        session_usage_rate = self.tokens_used_this_session / (session_elapsed / 60)
-        session_target_rate = self.session_limit / (6 * 60)  # 6-hour session target
-        
-        # Calculate sleep times for each constraint
-        sleep_times = []
-        
-        # Hourly constraint
-        if hour_usage_rate > hour_target_rate:
-            hour_excess = hour_usage_rate - hour_target_rate
-            hour_sleep = (hour_excess / hour_target_rate) * 60
-            sleep_times.append(("hourly", hour_sleep))
-        
-        # Session constraint
-        if session_usage_rate > session_target_rate:
-            session_excess = session_usage_rate - session_target_rate
-            session_sleep = (session_excess / session_target_rate) * 30
-            sleep_times.append(("session", session_sleep))
-        
-        # Adaptive throttling based on history
-        if self.adaptive_throttling and len(self.token_history) >= 5:
-            recent_usage = sum(entry["tokens"] for entry in self.token_history[-5:])
-            if recent_usage > self.max_tokens_per_hour * 0.3:  # 30% of hourly in recent 5 files
-                adaptive_sleep = min(60.0, recent_usage / 1000)
-                sleep_times.append(("adaptive", adaptive_sleep))
-        
-        if sleep_times:
-            # Use the maximum sleep time (most restrictive)
-            sleep_reason, sleep_duration = max(sleep_times, key=lambda x: x[1])
-            final_sleep = min(sleep_duration, 600.0)  # Max 10 minutes
-            
-            self.logger.info(
-                "Intelligent throttling: %d seconds (reason: %s, hourly rate: %.1f/min, session rate: %.1f/min)",
-                int(final_sleep), sleep_reason, hour_usage_rate, session_usage_rate
-            )
-            return final_sleep
-        
-        return 0.0
-    
-    def get_available_tokens(self) -> int:
-        """Get the number of tokens available for the current session."""
-        self._reset_time_windows_if_needed()
-        
-        # Calculate available tokens for different time windows
-        available_hourly = self.max_tokens_per_hour - self.tokens_used_this_hour
-        available_session = self.session_limit - self.tokens_used_this_session
-        available_daily = self.daily_budget - self.tokens_used_today
-        
-        # Return the most restrictive limit
-        available = min(available_hourly, available_session, available_daily)
-        return max(0, available)
-
-    def calculate_adaptive_batch_size(self, remaining_files: int, available_time_hours: float = 1.0) -> int:
-        """Calculate optimal batch size based on current consumption and available time."""
-        if remaining_files <= 0:
-            return 0
-        
-        self._reset_time_windows_if_needed()
-        
-        # Calculate available tokens for this batch
-        available_hourly = self.max_tokens_per_hour - self.tokens_used_this_hour
-        available_session = self.session_limit - self.tokens_used_this_session
-        available_tokens = min(available_hourly, available_session)
-        
-        # Estimate average tokens per file from history
-        if self.file_consumption_history:
-            avg_tokens_per_file = sum(self.file_consumption_history.values()) / len(self.file_consumption_history)
-        else:
-            avg_tokens_per_file = 8000  # Conservative estimate for IA analysis
-        
-        # Calculate batch size with safety buffer
-        max_batch_by_tokens = max(1, int(available_tokens * 0.8 / avg_tokens_per_file))
-        max_batch_by_time = max(1, min(10, int(available_time_hours * 6)))  # ~10 minutes per file
-        
-        optimal_batch = min(max_batch_by_tokens, max_batch_by_time, remaining_files)
-        
-        self.logger.debug(
-            "Adaptive batch size: %d files (tokens available: %d, avg per file: %d, time constraint: %d)",
-            optimal_batch, available_tokens, int(avg_tokens_per_file), max_batch_by_time
-        )
-        
-        return max(1, optimal_batch)
-
-    def record_usage(self, tokens_used: int, file_path: Optional[str] = None) -> None:
-        """Enhanced usage recording with multiple tracking levels."""
-        self._reset_time_windows_if_needed()
-        
-        # Update all counters
-        self.tokens_used_this_hour += tokens_used
-        self.tokens_used_today += tokens_used
-        self.tokens_used_this_session += tokens_used
-        
-        # Record in history with enhanced metadata
-        usage_entry = {
-            "timestamp": time.time(),
-            "tokens": tokens_used,
-            "file_path": file_path,
-            "cumulative_hour": self.tokens_used_this_hour,
-            "cumulative_today": self.tokens_used_today,
-            "cumulative_session": self.tokens_used_this_session
-        }
-        
-        self.token_history.append(usage_entry)
-        
-        # Keep history manageable (last 200 entries)
-        if len(self.token_history) > 200:
-            self.token_history = self.token_history[-200:]
-        
-        # Update file-specific consumption history
-        if file_path:
-            self.file_consumption_history[file_path] = tokens_used
-        
-        # Update performance metrics
-        self._update_performance_metrics(tokens_used, file_path)
-        
-        self.logger.debug(
-            "Token usage recorded: +%d tokens for %s (hour: %d, today: %d, session: %d)",
-            tokens_used, file_path or "unknown", 
-            self.tokens_used_this_hour, self.tokens_used_today, self.tokens_used_this_session
-        )
-
-    def _update_performance_metrics(self, tokens_used: int, file_path: Optional[str]) -> None:
-        """Update performance metrics for analysis."""
-        if not hasattr(self, 'performance_metrics'):
-            self.performance_metrics = {
-                "total_files_processed": 0,
-                "total_tokens_consumed": 0,
-                "average_tokens_per_file": 0,
-                "peak_tokens_per_file": 0,
-                "efficiency_score": 100.0
-            }
-        
-        self.performance_metrics["total_tokens_consumed"] += tokens_used
-        
-        if file_path:
-            self.performance_metrics["total_files_processed"] += 1
-            
-            # Update average
-            total_files = self.performance_metrics["total_files_processed"] 
-            self.performance_metrics["average_tokens_per_file"] = (
-                self.performance_metrics["total_tokens_consumed"] / total_files
-            )
-            
-            # Update peak
-            if tokens_used > self.performance_metrics["peak_tokens_per_file"]:
-                self.performance_metrics["peak_tokens_per_file"] = tokens_used
-            
-            # Calculate efficiency score (lower tokens per file = higher efficiency)
-            expected_tokens_per_file = 10000  # Baseline expectation
-            actual_average = self.performance_metrics["average_tokens_per_file"]
-            
-            if actual_average > 0:
-                efficiency = min(100.0, (expected_tokens_per_file / actual_average) * 100)
-                self.performance_metrics["efficiency_score"] = efficiency
-
-    def get_comprehensive_usage_stats(self) -> Dict[str, Any]:
-        """Get detailed usage statistics across all tracking levels."""
-        self._reset_time_windows_if_needed()
-        
-        current_time = time.time()
-        
-        # Time calculations
-        hour_elapsed = current_time - self.hour_start
-        day_elapsed = current_time - self.day_start  
-        session_elapsed = current_time - self.session_start
-        
-        # Rate calculations
-        hour_rate = self.tokens_used_this_hour / (hour_elapsed / 60) if hour_elapsed > 0 else 0.0
-        day_rate = self.tokens_used_today / (day_elapsed / 60) if day_elapsed > 0 else 0.0
-        session_rate = self.tokens_used_this_session / (session_elapsed / 60) if session_elapsed > 0 else 0.0
-        
-        # Projected usage
-        projected_hour_end = (hour_rate * 60) if hour_rate > 0 else 0
-        projected_day_end = (day_rate * 1440) if day_rate > 0 else 0  # 1440 min/day
-        
-        return {
-            # Current usage
-            "tokens_used_this_hour": self.tokens_used_this_hour,
-            "tokens_used_today": self.tokens_used_today,
-            "tokens_used_this_session": self.tokens_used_this_session,
-            
-            # Limits
-            "hourly_limit": self.max_tokens_per_hour,
-            "daily_budget": self.daily_budget,
-            "session_limit": self.session_limit,
-            
-            # Usage percentages
-            "hourly_usage_percent": (self.tokens_used_this_hour / self.max_tokens_per_hour) * 100.0,
-            "daily_usage_percent": (self.tokens_used_today / self.daily_budget) * 100.0,
-            "session_usage_percent": (self.tokens_used_this_session / self.session_limit) * 100.0,
-            
-            # Rates (tokens per minute)
-            "hourly_rate": hour_rate,
-            "daily_rate": day_rate,
-            "session_rate": session_rate,
-            
-            # Time elapsed
-            "hour_elapsed_minutes": hour_elapsed / 60.0,
-            "day_elapsed_hours": day_elapsed / 3600.0,
-            "session_elapsed_hours": session_elapsed / 3600.0,
-            
-            # Projections
-            "projected_hour_consumption": projected_hour_end,
-            "projected_day_consumption": projected_day_end,
-            
-            # Throttling
-            "estimated_sleep_time": self.calculate_intelligent_sleep_time(),
-            "should_throttle": projected_hour_end > self.max_tokens_per_hour * 0.9,
-            
-            # Performance metrics
-            "performance_metrics": getattr(self, 'performance_metrics', {}),
-            
-            # Health indicators
-            "efficiency_score": getattr(self, 'performance_metrics', {}).get('efficiency_score', 100.0),
-            "files_processed": len(self.file_consumption_history),
-            "average_tokens_per_file": sum(self.file_consumption_history.values()) / len(self.file_consumption_history) if self.file_consumption_history else 0,
-            
-            # System status
-            "intelligent_mode": self.intelligent_mode,
-            "adaptive_throttling": self.adaptive_throttling,
-            "risk_based_scaling": self.risk_based_scaling
-        }
-
-    def prioritize_files_by_risk_and_tokens(
-        self, 
-        files: List[str], 
-        project_root: Path, 
-        setima_data: Any
-    ) -> List[Tuple[str, int, int, str]]:
-        """
-        Prioritize files for processing based on risk score and estimated token consumption.
-        Returns list of (file_path, estimated_tokens, risk_score, priority_reason).
-        """
-        file_priorities = []
-        
-        for file_path in files:
-            # Get risk score from setima_data
-            risk_score = setima_data.get_file_risk_score(file_path) if setima_data else 25
-            risk_category = setima_data.get_file_risk_category(file_path) if setima_data else "MEDIUM"
-            
-            # Estimate token consumption
-            estimated_tokens = self.estimate_file_tokens(file_path, project_root)
-            
-            # Calculate priority score (higher = process first)
-            priority_score = 0
-            priority_reasons = []
-            
-            # Risk-based priority (critical files first)
-            if risk_category == "CRITICAL":
-                priority_score += 1000
-                priority_reasons.append("critical_risk")
-            elif risk_category == "HIGH":
-                priority_score += 800
-                priority_reasons.append("high_risk")
-            elif risk_category == "MEDIUM":
-                priority_score += 500
-                priority_reasons.append("medium_risk")
-            else:
-                priority_score += 200
-                priority_reasons.append("low_risk")
-            
-            # Token efficiency priority (process smaller files first to build momentum)
-            if estimated_tokens < 2000:
-                priority_score += 300
-                priority_reasons.append("small_efficient")
-            elif estimated_tokens < 5000:
-                priority_score += 200
-                priority_reasons.append("medium_efficient")
-            elif estimated_tokens > 15000:
-                priority_score -= 100
-                priority_reasons.append("large_expensive")
-            
-            # File type priorities
-            if file_path.endswith('__init__.py'):
-                priority_score += 100
-                priority_reasons.append("init_file")
-            elif 'test' in file_path.lower():
-                priority_score += 50
-                priority_reasons.append("test_file")
-            elif any(critical in file_path.lower() for critical in ['database', 'security', 'auth']):
-                priority_score += 150
-                priority_reasons.append("critical_component")
-            
-            # Dependency-based priority (foundation files first)
-            wave = setima_data.get_file_wave(file_path) if setima_data else "WAVE_1_FOUNDATION"
-            if wave == "WAVE_1_FOUNDATION":
-                priority_score += 400
-                priority_reasons.append("foundation")
-            elif wave == "WAVE_4_CRITICAL":
-                priority_score += 600
-                priority_reasons.append("critical_wave")
-            
-            file_priorities.append((
-                file_path, 
-                estimated_tokens, 
-                risk_score, 
-                ",".join(priority_reasons),
-                priority_score
-            ))
-        
-        # Sort by priority score (descending)
-        file_priorities.sort(key=lambda x: x[4], reverse=True)
-        
-        # Return without internal priority_score
-        return [(fp[0], fp[1], fp[2], fp[3]) for fp in file_priorities]
-    
-    def get_session_summary(self) -> Dict[str, Any]:
-        """Generate comprehensive session summary."""
-        stats = self.get_comprehensive_usage_stats()
-        
-        # Calculate efficiency metrics
-        files_processed = len(self.file_consumption_history)
-        if files_processed > 0:
-            avg_tokens = sum(self.file_consumption_history.values()) / files_processed
-            efficiency_rating = "excellent" if avg_tokens < 5000 else \
-                               "good" if avg_tokens < 10000 else \
-                               "fair" if avg_tokens < 20000 else "needs_optimization"
-        else:
-            avg_tokens = 0
-            efficiency_rating = "no_data"
-        
-        # Calculate time remaining estimates
-        available_tokens_hour = max(0, self.max_tokens_per_hour - self.tokens_used_this_hour)
-        available_tokens_session = max(0, self.session_limit - self.tokens_used_this_session)
-        
-        files_remaining_hour = int(available_tokens_hour / avg_tokens) if avg_tokens > 0 else 0
-        files_remaining_session = int(available_tokens_session / avg_tokens) if avg_tokens > 0 else 0
-        
-        return {
-            "session_overview": {
-                "files_processed": files_processed,
-                "total_tokens_consumed": self.tokens_used_this_session,
-                "average_tokens_per_file": int(avg_tokens),
-                "efficiency_rating": efficiency_rating,
-                "session_duration_hours": stats["session_elapsed_hours"]
-            },
-            
-            "capacity_remaining": {
-                "tokens_available_hour": available_tokens_hour,
-                "tokens_available_session": available_tokens_session,
-                "estimated_files_remaining_hour": files_remaining_hour,
-                "estimated_files_remaining_session": files_remaining_session
-            },
-            
-            "throttling_status": {
-                "should_throttle": stats["should_throttle"],
-                "recommended_sleep_seconds": int(stats["estimated_sleep_time"]),
-                "adaptive_throttling_enabled": self.adaptive_throttling
-            },
-            
-            "performance_indicators": {
-                "efficiency_score": stats["efficiency_score"],
-                "processing_rate_files_per_hour": files_processed / max(0.1, stats["session_elapsed_hours"]),
-                "token_consumption_rate": stats["session_rate"]
-            }
-        }
-
-    # Legacy compatibility methods
-    def calculate_sleep_time(self) -> float:
-        """Legacy compatibility - redirects to intelligent sleep time calculation."""
-        return self.calculate_intelligent_sleep_time()
-    
-    def get_usage_stats(self) -> Dict[str, Any]:
-        """Legacy compatibility - redirects to comprehensive stats."""
-        return self.get_comprehensive_usage_stats()
-
-
-# =============================================================================
-# Gerenciador de lista de arquivos ‚Äì repo inteiro (com exclus√µes)
-# =============================================================================
-class FileListManager:
-    """Busca .py no reposit√≥rio inteiro, com exclus√µes padr√£o e ordem determin√≠stica."""
-
-    DEFAULT_EXCLUDES = {
-        ".git",
-        ".venv",
-        "venv",
-        "__pycache__",
-        ".mypy_cache",
-        ".pytest_cache",
-        ".idea",
-        ".vscode",
-        "node_modules",
-        "dist",
-        "build",
-        ".ruff_cache",
-        ".tox",
-    }
-
-    def __init__(self, project_root: Path):
-        self.project_root = project_root
-        self.logger = logging.getLogger(f"{__name__}.FileListManager")
-
-    def get_all_python_files(self) -> List[str]:
-        python_files: List[str] = []
-        for root, dirs, files in os.walk(self.project_root):
-            # aplica exclus√µes de diret√≥rios in-place
-            dirs[:] = [d for d in dirs if d not in self.DEFAULT_EXCLUDES]
-            for fname in files:
-                if not fname.endswith(".py"):
-                    continue
-                full = Path(root) / fname
-                rel = str(full.relative_to(self.project_root))
-                # ignorar este pr√≥prio arquivo
-                if rel.endswith(os.path.basename(__file__)):
-                    continue
-                python_files.append(rel)
-
-        python_files.sort()
-        self.logger.info("Foram encontrados %d arquivos Python", len(python_files))
-        return python_files
-
-    def get_file_info(self, file_path: str) -> Dict[str, Any]:
-        full_path = self.project_root / file_path
-        if not full_path.exists():
-            return {"exists": False}
-        stat = full_path.stat()
-        try:
-            with full_path.open("r", encoding="utf-8") as f:
-                lines = sum(1 for _ in f)
-        except Exception:
-            lines = 0
-        return {
-            "exists": True,
-            "size_bytes": stat.st_size,
-            "lines": lines,
-            "modified": datetime.fromtimestamp(stat.st_mtime),
-            "readable": os.access(full_path, os.R_OK),
-            "writable": os.access(full_path, os.W_OK),
-        }
-
-
-# =============================================================================
-# Logging
-# =============================================================================
-def setup_logging(verbose: bool = False) -> logging.Logger:
-    level = logging.DEBUG if verbose else logging.INFO
-    logging.basicConfig(
-        level=level,
-        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
-        handlers=[logging.FileHandler("systematic_audit.log"), logging.StreamHandler(sys.stdout)],
-    )
-    return logging.getLogger(__name__)
-
-
-# =============================================================================
-# CLI
-# =============================================================================
 def main() -> int:
-    parser = argparse.ArgumentParser(description="Enhanced Systematic File Auditor - S√©tima Camada")
-    parser.add_argument("--resume", nargs='?', const=True, help="Resume previous session or specific session ID")
-    parser.add_argument("--dry-run", action="store_true", help="Simular sem escrever no disco")
-    parser.add_argument("--max-files", type=int, help="M√°ximo de arquivos a processar")
-    parser.add_argument("--verbose", "-v", action="store_true", help="Logging detalhado (debug)")
-    parser.add_argument("--wave", choices=["1", "2", "3", "4", "all"], default="all", help="Executa wave espec√≠fica")
-    parser.add_argument("--risk-category", choices=["LOW", "MEDIUM", "HIGH", "CRITICAL"], help="Filtra por categoria de risco")
-    parser.add_argument("--validate-only", action="store_true", help="Apenas valida sistema, sem modifica√ß√µes")
-    parser.add_argument("--meta-agent", action="store_true", help="Enable MetaAgent intelligent coordination (experimental)")
-    parser.add_argument("--legacy-agents", action="store_true", help="Use legacy manual agent coordination instead of MetaAgent")
-    args = parser.parse_args()
-
-    logger = setup_logging(args.verbose)
-    logger.info("ü§ñ Enhanced Systematic File Auditor - S√©tima Camada INICIANDO")
-
-    try:
-        audit_dir = Path(__file__).parent
-        auditor = EnhancedSystematicFileAuditor(project_root, audit_dir, dry_run=args.dry_run, validate_only=args.validate_only)
-
-        logger.info("‚úÖ Auditor inicializado | Integra√ß√£o S√©tima Camada: %s", "Full" if auditor.setima_integration_available else "Parcial")
-
-        # Lista de arquivos (repo inteiro)
-        all_files = auditor.file_manager.get_all_python_files()
-        if args.max_files:
-            all_files = all_files[: args.max_files]
-
-        logger.info("üìÅ Processando %d arquivos (an√°lise baseada em risco)", len(all_files))
-        logger.info(
-            "üîó Integra√ß√£o: Context=%s | IntegrationTester=%s | RollbackMgr=%s | RiskScores=%d | Waves=%d",
-            "ON" if auditor.context_validator else "OFF",
-            "ON" if auditor.integration_tester else "OFF",
-            "ON" if auditor.rollback_manager else "OFF",
-            len(auditor.setima_data.risk_scores),
-            len(auditor.setima_data.dependency_waves),
-        )
-
-        if args.validate_only:
-            logger.info("üß™ VALIDATION ONLY MODE - Validando sa√∫de do sistema")
-            v = auditor._validate_system_health()
-            logger.info("System Health: %s", "‚úÖ Healthy" if v["healthy"] else "‚ùå Issues detected")
-            if not v["healthy"]:
-                logger.error("Issues: %s", v["issues"])
-                return 1
-            logger.info("‚úÖ Valida√ß√£o conclu√≠da com sucesso")
-            return 0
-
-        if args.dry_run:
-            logger.info("üß™ DRY RUN MODE - Amostra de an√°lise de risco")
-            sample = all_files[: min(10, len(all_files))]
-            for fp in sample:
-                risk_score = auditor.setima_data.get_file_risk_score(fp)
-                risk_category = auditor.setima_data.get_file_risk_category(fp)
-                wave = auditor.setima_data.get_file_wave(fp)
-                context_q = auditor._validate_context_quality(fp)
-                logger.info("üìä %s | Risk=%s (%s) | Wave=%s | Context=%.1f%%", fp, risk_score, risk_category, wave, context_q)
-            logger.info("‚úÖ Demonstra√ß√£o conclu√≠da (dry-run)")
-            return 0
-
-        # Prepare session resumption ID
-        resume_session_id = None
-        if args.resume:
-            resume_session_id = args.resume if isinstance(args.resume, str) else None
-
-        # Execu√ß√£o real
-        start = time.time()
-
-        # Filtro por categoria de risco (se fornecido)
-        risk_filter = args.risk_category
-
-        if args.wave == "all":
-            # Use intelligent audit with AI agents and batch processing
-            if auditor.intelligent_agents_available:
-                logger.info("ü§ñ Using intelligent AI-powered audit execution")
-                results = auditor.execute_intelligent_risk_based_audit(resume_session_id)
-            else:
-                logger.info("üìä Using legacy wave-based audit execution (AI not available)")
-                # Fallback to original method if AI not available
-                results = auditor.execute_risk_based_audit()
-
-            # Se filtro de risco solicitado, filtra apenas para reporting (execu√ß√£o j√° ocorreu)
-            if risk_filter:
-                if "intelligent_analysis" in results and results["intelligent_analysis"]:
-                    # Filter intelligent results structure
-                    def _filter_intelligent_files(files_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
-                        filtered = []
-                        for file_data in files_list:
-                            file_path = file_data.get("file_path", "")
-                            file_risk_category = auditor.setima_data.get_file_risk_category(file_path)
-                            if file_risk_category == risk_filter:
-                                filtered.append(file_data)
-                        return filtered
-                    
-                    results["files_processed"] = _filter_intelligent_files(results.get("files_processed", []))
-                    results["files_failed"] = _filter_intelligent_files(results.get("files_failed", []))
-                    results["files_deferred"] = _filter_intelligent_files(results.get("files_deferred", []))
-                    
-                    logger.info("üîç Risk filter applied: showing only %s category files", risk_filter)
-                else:
-                    # Filter legacy wave-based results structure
-                    def _filter_by_cat(lst: List[FileAuditResult]) -> List[FileAuditResult]:
-                        return [r for r in lst if r.risk_category == risk_filter]
-
-                    results["WAVE_1_LOW"] = _filter_by_cat(results.get("WAVE_1_LOW", []))
-                    results["WAVE_2_MEDIUM"] = _filter_by_cat(results.get("WAVE_2_MEDIUM", []))
-                    results["WAVE_3_HIGH"] = _filter_by_cat(results.get("WAVE_3_HIGH", []))
-                    results["WAVE_4_CRITICAL"] = _filter_by_cat(results.get("WAVE_4_CRITICAL", []))
-                    results["execution_summary"] = auditor._generate_execution_summary(results, time.time() - start)
-
-        else:
-            # Executa apenas uma wave espec√≠fica
-            wave_map = {
-                "1": "WAVE_1_FOUNDATION",
-                "2": "WAVE_2_BUSINESS",
-                "3": "WAVE_3_INTEGRATION",
-                "4": "WAVE_4_CRITICAL",
-            }
-            wave_key = wave_map[args.wave]
-            target_files = auditor.setima_data.dependency_waves.get(wave_key, [])
-
-            # Aplica filtro de risco ANTES da execu√ß√£o
-            if risk_filter:
-                target_files = [
-                    fp for fp in target_files if auditor.setima_data.get_file_risk_category(fp) == risk_filter
-                ]
-
-            if args.max_files:
-                target_files = target_files[: args.max_files]
-
-            logger.info("‚ö° Executando apenas a wave %s (%d arquivos)", wave_key, len(target_files))
-
-            if wave_key == "WAVE_1_FOUNDATION":
-                wave_results = auditor._execute_wave_parallel(target_files)
-                results = {"WAVE_1_LOW": wave_results, "execution_summary": auditor._generate_execution_summary({"WAVE_1_LOW": wave_results}, time.time() - start)}
-            elif wave_key == "WAVE_2_BUSINESS":
-                wave_results = auditor._execute_wave_coordinated(target_files)
-                results = {"WAVE_2_MEDIUM": wave_results, "execution_summary": auditor._generate_execution_summary({"WAVE_2_MEDIUM": wave_results}, time.time() - start)}
-            elif wave_key == "WAVE_3_INTEGRATION":
-                wave_results = auditor._execute_wave_sequential(target_files)
-                results = {"WAVE_3_HIGH": wave_results, "execution_summary": auditor._generate_execution_summary({"WAVE_3_HIGH": wave_results}, time.time() - start)}
-            else:  # WAVE_4_CRITICAL
-                wave_results = auditor._execute_wave_critical(target_files)
-                results = {"WAVE_4_CRITICAL": wave_results, "execution_summary": auditor._generate_execution_summary({"WAVE_4_CRITICAL": wave_results}, time.time() - start)}
-
-        # Exibe resumo
-        summary = results.get("execution_summary", {})
-        logger.info("üéØ Resumo de Execu√ß√£o ‚Üí Files: %s | Opts: %s | Waves: %s | Dura√ß√£o: %ss | Sucesso: %s%%",
-                    summary.get("total_files_processed", 0),
-                    summary.get("total_optimizations_applied", 0),
-                    summary.get("waves_completed", 0),
-                    summary.get("duration_seconds", 0),
-                    summary.get("success_rate", 0.0))
-
-        # Finalize session with execution stats
-        final_stats = {
-            "execution_summary": summary,
-            "total_execution_time": time.time() - start,
-            "session_completed": True,
-            "intelligent_mode": auditor.intelligent_agents_available
-        }
-        auditor.session_manager.finalize_session(final_stats)
-
-        logger.info("‚úÖ Execu√ß√£o finalizada")
-        return 0
-
-    except Exception as e:  # pragma: no cover
-        logger.error("‚ùå Falha ao iniciar/rodar auditor: %s", e, exc_info=True)
-        return 1
+    args = _parse_args()
+    auditor = build_default_auditor(args.root, db_path=args.db)
+    selected = _resolve_targets(args.root, list(args.targets))
+    findings: List[FileFinding] = auditor.run(selected if selected else None)
+    # sa√≠da simples no stdout
+    for f in findings:
+        print(f"{f.severity:>6} | {f.rule:<18} | {args.root.as_posix()}/" + f.path.relative_to(args.root).as_posix() + f" ‚Äì {f.message}")
+    return 0
 
 
 if __name__ == "__main__":
-    sys.exit(main())
+    raise SystemExit(main())
diff --git a/tests/integration/test_audit_flow.py b/tests/integration/test_audit_flow.py
new file mode 100644
index 0000000000000000000000000000000000000000..2e28a922de170af0eede30cd6267cad7d423397f
--- /dev/null
+++ b/tests/integration/test_audit_flow.py
@@ -0,0 +1,18 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+from audit_system.core.container import build_default_auditor
+
+
+def test_integration_basic_flow(tmp_path: Path):
+    root = tmp_path / "proj"
+    pkg = root / "pkg"
+    pkg.mkdir(parents=True)
+    (pkg / "__init__.py").write_text("")
+    (pkg / "mod.py").write_text("# TODO: fix\nx=1\n", encoding="utf-8")
+
+    auditor = build_default_auditor(root)
+    findings = auditor.run()
+    # Deve achar pelo menos o TODO
+    assert any(f.rule == "TODO_PRESENT" for f in findings)
diff --git a/tests/unit/test_safe_io.py b/tests/unit/test_safe_io.py
new file mode 100644
index 0000000000000000000000000000000000000000..fbc57c01346f94af8c1438d58a227677233784c1
--- /dev/null
+++ b/tests/unit/test_safe_io.py
@@ -0,0 +1,23 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pytest
+
+from audit_system.utils.safe_io import safe_join, PathSecurityError, write_text_atomic_secure, read_text_secure
+
+
+def test_safe_join_blocks_traversal(tmp_path: Path):
+    base = tmp_path / "root"
+    base.mkdir()
+    outside = Path("..") / "evil.txt"
+    with pytest.raises(PathSecurityError):
+        safe_join(base, outside)
+
+
+def test_atomic_write_and_read(tmp_path: Path):
+    base = tmp_path / "root"
+    base.mkdir()
+    rel = Path("a.txt")
+    write_text_atomic_secure(base, rel, "hello")
+    assert read_text_secure(base, rel) == "hello"
diff --git a/tests/unit/test_session_sqlite.py b/tests/unit/test_session_sqlite.py
new file mode 100644
index 0000000000000000000000000000000000000000..7034da4ff49ece13befe50eba49d78f4e5b24c94
--- /dev/null
+++ b/tests/unit/test_session_sqlite.py
@@ -0,0 +1,27 @@
+from __future__ import annotations
+
+import sqlite3
+from pathlib import Path
+
+from audit_system.core.contracts import FileFinding
+from audit_system.services.session_sqlite import SQLiteSessionRepository
+
+
+def test_sqlite_wal_enabled(tmp_path: Path):
+    db = tmp_path / "audit.db"
+    repo = SQLiteSessionRepository(db)
+    # validate WAL
+    conn = sqlite3.connect(db)
+    try:
+        mode = conn.execute("PRAGMA journal_mode;").fetchone()[0]
+    finally:
+        conn.close()
+    assert mode.lower() == "wal"
+
+
+def test_save_and_end_run(tmp_path: Path):
+    db = tmp_path / "audit.db"
+    repo = SQLiteSessionRepository(db)
+    run_id = repo.start_run(tmp_path)
+    repo.save_findings(run_id, [FileFinding(tmp_path / "f.py", "RULE", "LOW", "ok")])
+    repo.end_run(run_id, "OK")
 
EOF
)