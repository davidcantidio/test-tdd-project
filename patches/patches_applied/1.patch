 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/monitoring/CLAUDE.md b/monitoring/CLAUDE.md
index 913a3702747a9fe652257f88d780d6406439078c..a76f375bbfbdd4c9819beed60d26aa47f4cd408a 100644
--- a/monitoring/CLAUDE.md
+++ b/monitoring/CLAUDE.md
@@ -1,31 +1,31 @@
 # 🤖 CLAUDE.md - Observability Stack
 
 **Module:** monitoring/  
 **Purpose:** Comprehensive monitoring, observability, and alerting system  
 **Architecture:** Prometheus + Grafana + structured logging with correlation tracking  
-**Last Updated:** 2025-08-17
+**Last Updated:** 2025-08-18
 
 ---
 
 ## 📊 **Monitoring System Overview**
 
 Enterprise-grade observability stack featuring:
 - **Prometheus Metrics**: Comprehensive metrics collection and storage
 - **Grafana Dashboards**: Visual monitoring and alerting interfaces
 - **Structured Logging**: JSON logging with correlation ID tracking
 - **Health Monitoring**: Real-time system health validation
 - **Alerting System**: Multi-channel alert routing and notification
 - **Performance Tracking**: Application and infrastructure performance metrics
 
 ---
 
 ## 🏗️ **Observability Architecture**
 
 ### **Component Structure**
 ```
 monitoring/
 ├── structured_logging.py         # 📊 JSON logging with correlation IDs
 ├── health_check.py              # 🏥 System health validation
 ├── graceful_shutdown.py         # 🔄 Graceful shutdown handling
 ├── prometheus.yml               # 📈 Prometheus configuration
 ├── grafana_dashboards.json      # 📊 Grafana dashboard definitions
@@ -866,26 +866,36 @@ spec:
 - **Use Counter for Events**: Login attempts, API calls, errors
 - **Use Gauge for State**: Active users, memory usage, queue size
 - **Use Histogram for Timing**: Response times, request durations
 - **Label Judiciously**: Avoid high cardinality labels
 
 ### **Alerting Strategy** 
 - **Alert on Symptoms**: User-facing issues, not technical details
 - **Avoid Alert Fatigue**: Set appropriate thresholds and time windows
 - **Escalation Paths**: Define clear escalation procedures
 - **Runbooks**: Document response procedures for each alert
 
 ### **Logging Strategy**
 - **Structured Format**: Use JSON for machine readability
 - **Correlation IDs**: Track requests across services
 - **Appropriate Levels**: DEBUG for development, INFO for business events
 - **Security Awareness**: Sanitize sensitive data in logs
 
 ### **Dashboard Design**
 - **Business Metrics First**: Focus on user-impacting metrics
 - **Layered Detail**: Overview → detailed → diagnostic
 - **Time Context**: Appropriate time ranges for different metrics
 - **Actionable Alerts**: Connect metrics to specific actions
 
 ---
 
-*This comprehensive monitoring system provides enterprise-grade observability with real-time health monitoring, performance tracking, and proactive alerting for production-ready deployment.*
\ No newline at end of file
+*This comprehensive monitoring system provides enterprise-grade observability with real-time health monitoring, performance tracking, and proactive alerting for production-ready deployment.*
+
+---
+
+### 🔄 Update (2025-08-18)
+- Default health server port set to **8599** (`--port`), metrics path `/metrics` (Prometheus exposition format).
+- Prometheus **retention** is configured via **CLI flags** (e.g. `--storage.tsdb.retention.time=30d`), not in `prometheus.yml`.
+- Exported metrics used by dashboards & alerts:
+  - `tdd_framework_health_status` (gauge) — 1 healthy / 0 unhealthy
+  - `tdd_framework_component_up{component="..."}` (gauge)
+  - `tdd_framework_uptime_seconds` (counter)
diff --git a/monitoring/alert_rules.yml b/monitoring/alert_rules.yml
index 2803f062057c2212937068f8b0d2e0ea6c013456..abdfa2caf63df7d9f334c7b99eef8e201d45c840 100644
--- a/monitoring/alert_rules.yml
+++ b/monitoring/alert_rules.yml
@@ -1,252 +1,34 @@
-# Prometheus Alert Rules for TDD Framework
-# 
-# These rules define alerts for:
-# - High error rates
-# - Performance degradation
-# - Security incidents
-# - System resource issues
-# - Application availability
-
+# Prometheus Alert Rules for TDD Framework (minimal, valid)
 groups:
-  - name: tdd_framework_performance
-    interval: 30s
-    rules:
-      # High Error Rate Alert
-      - alert: HighErrorRate
-        expr: rate(tdd_framework_errors_total[5m]) > 0.1
-        for: 2m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "High error rate detected"
-          description: "Error rate is {{ $value }} errors/second for {{ $labels.component }}"
-          runbook_url: "https://docs.tddframework.dev/runbooks/high-error-rate"
-
-      # Critical Error Rate Alert
-      - alert: CriticalErrorRate
-        expr: rate(tdd_framework_errors_total[5m]) > 0.5
-        for: 1m
-        labels:
-          severity: critical
-          service: tdd-framework
-        annotations:
-          summary: "Critical error rate detected"
-          description: "Error rate is {{ $value }} errors/second for {{ $labels.component }}"
-          runbook_url: "https://docs.tddframework.dev/runbooks/critical-error-rate"
-
-      # High Response Time Alert
-      - alert: HighResponseTime
-        expr: histogram_quantile(0.95, rate(tdd_framework_request_duration_seconds_bucket[5m])) > 2.0
-        for: 5m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "High response time detected"
-          description: "95th percentile response time is {{ $value }}s for {{ $labels.endpoint }}"
-          runbook_url: "https://docs.tddframework.dev/runbooks/high-response-time"
-
-      # Very High Response Time Alert
-      - alert: VeryHighResponseTime
-        expr: histogram_quantile(0.95, rate(tdd_framework_request_duration_seconds_bucket[5m])) > 5.0
-        for: 2m
-        labels:
-          severity: critical
-          service: tdd-framework
-        annotations:
-          summary: "Very high response time detected"
-          description: "95th percentile response time is {{ $value }}s for {{ $labels.endpoint }}"
-          runbook_url: "https://docs.tddframework.dev/runbooks/very-high-response-time"
-
-      # Low Cache Hit Ratio Alert
-      - alert: LowCacheHitRatio
-        expr: tdd_framework_cache_hit_ratio < 0.7
-        for: 10m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "Low cache hit ratio"
-          description: "Cache hit ratio is {{ $value | humanizePercentage }}"
-          runbook_url: "https://docs.tddframework.dev/runbooks/low-cache-hit-ratio"
-
-  - name: tdd_framework_security
+  - name: tdd_framework_health
     interval: 30s
     rules:
-      # High Security Events Alert
-      - alert: HighSecurityEvents
-        expr: rate(tdd_framework_security_events_total[5m]) > 0.5
+      - alert: TDDFrameworkTargetDown
+        expr: up{job="tdd-framework-health"} == 0
         for: 1m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "High security event rate"
-          description: "Security event rate is {{ $value }} events/second for {{ $labels.event_type }}"
-          runbook_url: "https://docs.tddframework.dev/runbooks/high-security-events"
-
-      # Critical Security Events Alert
-      - alert: CriticalSecurityEvents
-        expr: increase(tdd_framework_security_events_total{severity="critical"}[5m]) > 0
-        for: 0m
         labels:
           severity: critical
           service: tdd-framework
         annotations:
-          summary: "Critical security event detected"
-          description: "{{ $value }} critical security events in the last 5 minutes"
-          runbook_url: "https://docs.tddframework.dev/runbooks/critical-security-events"
+          summary: "TDD health endpoint is down"
+          description: "The Prometheus target for the TDD Framework health server is down."
 
-      # Failed Authentication Attempts
-      - alert: HighFailedAuthAttempts
-        expr: rate(tdd_framework_auth_attempts_total{status="failed"}[5m]) > 0.2
+      - alert: TDDFrameworkUnhealthy
+        expr: tdd_framework_health_status == 0
         for: 2m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "High failed authentication rate"
-          description: "Failed authentication rate is {{ $value }} attempts/second"
-          runbook_url: "https://docs.tddframework.dev/runbooks/high-failed-auth"
-
-      # Brute Force Attack Detection
-      - alert: BruteForceAttack
-        expr: rate(tdd_framework_auth_attempts_total{status="failed"}[1m]) > 1.0
-        for: 30s
-        labels:
-          severity: critical
-          service: tdd-framework
-        annotations:
-          summary: "Potential brute force attack"
-          description: "{{ $value }} failed login attempts per second"
-          runbook_url: "https://docs.tddframework.dev/runbooks/brute-force-attack"
-
-  - name: tdd_framework_system
-    interval: 60s
-    rules:
-      # High Memory Usage Alert
-      - alert: HighMemoryUsage
-        expr: tdd_framework_memory_usage_bytes / 1024 / 1024 / 1024 > 2.0
-        for: 5m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "High memory usage"
-          description: "Memory usage is {{ $value | humanize }}GB"
-          runbook_url: "https://docs.tddframework.dev/runbooks/high-memory-usage"
-
-      # High CPU Usage Alert
-      - alert: HighCPUUsage
-        expr: tdd_framework_cpu_usage_percent > 80
-        for: 5m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "High CPU usage"
-          description: "CPU usage is {{ $value | humanizePercentage }}"
-          runbook_url: "https://docs.tddframework.dev/runbooks/high-cpu-usage"
-
-      # Too Many Database Connections
-      - alert: HighDatabaseConnections
-        expr: tdd_framework_database_connections > 50
-        for: 3m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "High database connection count"
-          description: "{{ $value }} active database connections"
-          runbook_url: "https://docs.tddframework.dev/runbooks/high-db-connections"
-
-      # Too Many Active Sessions
-      - alert: HighActiveSessions
-        expr: tdd_framework_active_sessions > 100
-        for: 5m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "High active session count"
-          description: "{{ $value }} active user sessions"
-          runbook_url: "https://docs.tddframework.dev/runbooks/high-active-sessions"
-
-  - name: tdd_framework_availability
-    interval: 30s
-    rules:
-      # Application Down Alert
-      - alert: ApplicationDown
-        expr: up{job="tdd-framework"} == 0
-        for: 1m
         labels:
           severity: critical
           service: tdd-framework
         annotations:
-          summary: "TDD Framework application is down"
-          description: "The TDD Framework application has been down for more than 1 minute"
-          runbook_url: "https://docs.tddframework.dev/runbooks/application-down"
-
-      # Metrics Endpoint Down
-      - alert: MetricsEndpointDown
-        expr: up{job="tdd-framework", instance="localhost:8000"} == 0
-        for: 2m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "Metrics endpoint unreachable"
-          description: "The metrics endpoint has been unreachable for more than 2 minutes"
-          runbook_url: "https://docs.tddframework.dev/runbooks/metrics-endpoint-down"
-
-      # Low Request Rate (possible issue)
-      - alert: LowRequestRate
-        expr: rate(tdd_framework_requests_total[10m]) < 0.01
-        for: 15m
-        labels:
-          severity: info
-          service: tdd-framework
-        annotations:
-          summary: "Very low request rate"
-          description: "Request rate is {{ $value }} requests/second over the last 10 minutes"
-          runbook_url: "https://docs.tddframework.dev/runbooks/low-request-rate"
+          summary: "Framework status UNHEALTHY"
+          description: "Health service reports UNHEALTHY for over 2 minutes."
 
-  - name: tdd_framework_business_logic
-    interval: 60s
-    rules:
-      # High Failed Client Creation Rate
-      - alert: HighFailedClientCreation
-        expr: rate(tdd_framework_errors_total{component="client_management", error_type="creation_failed"}[10m]) > 0.1
+      - alert: TDDFrameworkComponentDown
+        expr: sum by (component) (tdd_framework_component_up == 0) > 0
         for: 5m
         labels:
           severity: warning
           service: tdd-framework
         annotations:
-          summary: "High client creation failure rate"
-          description: "Client creation failure rate is {{ $value }} failures/second"
-          runbook_url: "https://docs.tddframework.dev/runbooks/client-creation-failures"
-
-      # Database Operation Failures
-      - alert: DatabaseOperationFailures
-        expr: rate(tdd_framework_errors_total{component="database"}[5m]) > 0.05
-        for: 3m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "High database operation failure rate"
-          description: "Database operation failure rate is {{ $value }} failures/second"
-          runbook_url: "https://docs.tddframework.dev/runbooks/database-operation-failures"
-
-      # Long Running Operations
-      - alert: LongRunningOperations
-        expr: histogram_quantile(0.99, rate(tdd_framework_request_duration_seconds_bucket{endpoint=~".*create.*|.*update.*"}[5m])) > 10.0
-        for: 2m
-        labels:
-          severity: warning
-          service: tdd-framework
-        annotations:
-          summary: "Long running CRUD operations"
-          description: "99th percentile for {{ $labels.endpoint }} is {{ $value }}s"
-          runbook_url: "https://docs.tddframework.dev/runbooks/long-running-operations"
\ No newline at end of file
+          summary: "One or more components are down"
+          description: "Components reporting down: {{ $labels.component }}"
diff --git a/monitoring/graceful_shutdown.py b/monitoring/graceful_shutdown.py
index 58f212f8f302c2e7c82d5aa25940f8dc6081be58..79821bfe90efc8d616bae458ae57fabd839a40b0 100644
--- a/monitoring/graceful_shutdown.py
+++ b/monitoring/graceful_shutdown.py
@@ -1,480 +1,131 @@
 #!/usr/bin/env python3
+# -*- coding: utf-8 -*-
 """
-🛑 Graceful Shutdown System
+🔄 Graceful Shutdown Manager
 
-Addresses report.md requirement: "Implement graceful shutdown for connections"
-
-This module provides:
-- Signal-based shutdown handling
-- Database connection cleanup
-- Background thread termination
-- Health check integration
-- Resource cleanup coordination
-- Process state management
+- Handles SIGINT/SIGTERM
+- Runs registered cleanup callbacks with a timeout
+- Integrates with readiness checks (health server will report not ready)
 """
+from __future__ import annotations
 
-import os
-import sys
-import time
+import logging
 import signal
 import threading
-import logging
-from datetime import datetime, timezone
-from typing import Dict, Any, List, Optional, Callable, Set
-from pathlib import Path
-from contextlib import contextmanager
+import time
 from dataclasses import dataclass, field
+from datetime import datetime
+from typing import Callable, Dict, List, Optional, Set
 
-# Add project root to path
-sys.path.append(str(Path(__file__).parent.parent))
-
-try:
-    from config.environment import get_config, is_production
-    CONFIG_AVAILABLE = True
-except ImportError:
-    CONFIG_AVAILABLE = False
-    get_config = None
-    is_production = lambda: False
-
-try:
-    from monitoring.structured_logging import get_structured_logger, application_logger
-    LOGGING_AVAILABLE = True
-except ImportError:
-    LOGGING_AVAILABLE = False
-    application_logger = None
-
-try:
-    from streamlit_extension.utils.database import DatabaseManager
-    DATABASE_AVAILABLE = True
-except ImportError:
-    DATABASE_AVAILABLE = False
-    DatabaseManager = None
-
-logger = logging.getLogger(__name__)
-
+Callback = Callable[[], None]
 
 @dataclass
 class ShutdownContext:
-    """Context for shutdown operations."""
-    shutdown_initiated: bool = False
-    shutdown_reason: str = ""
-    shutdown_start_time: Optional[datetime] = None
+    initiated: bool = False
+    reason: str = ""
+    started_at: Optional[datetime] = None
     timeout_seconds: int = 30
-    force_shutdown: bool = False
-    shutdown_callbacks: List[Callable] = field(default_factory=list)
+    callbacks: List[Callback] = field(default_factory=list)
     active_resources: Set[str] = field(default_factory=set)
-    cleanup_results: Dict[str, bool] = field(default_factory=dict)
-
+    results: Dict[str, bool] = field(default_factory=dict)
 
 class GracefulShutdownManager:
-    """Manages graceful shutdown of application resources."""
-    
-    def __init__(self, timeout_seconds: int = 30):
-        self.context = ShutdownContext(timeout_seconds=timeout_seconds)
-        self.cleanup_handlers: Dict[str, Callable] = {}
-        self.background_threads: Set[threading.Thread] = set()
-        self.database_connections: Set[Any] = set()
-        self.server_instances: Set[Any] = set()
-        self._shutdown_lock = threading.Lock()
-        self._original_handlers = {}
-        
-        # Set up logging
-        if LOGGING_AVAILABLE:
-            self.logger = get_structured_logger("graceful_shutdown")
-        else:
-            self.logger = logger
-        
-        self._setup_signal_handlers()
-    
-    def _setup_signal_handlers(self):
-        """Setup signal handlers for graceful shutdown."""
-        def signal_handler(signum, frame):
-            signal_name = signal.Signals(signum).name
-            self.logger.info(f"Received {signal_name} signal, initiating graceful shutdown")
-            self.shutdown(reason=f"Signal {signal_name}")
-        
-        # Handle common shutdown signals
-        signals_to_handle = [signal.SIGTERM, signal.SIGINT]
-        
-        # Add SIGHUP for Unix systems
-        if hasattr(signal, 'SIGHUP'):
-            signals_to_handle.append(signal.SIGHUP)
-        
-        for sig in signals_to_handle:
-            try:
-                self._original_handlers[sig] = signal.signal(sig, signal_handler)
-                self.logger.debug(f"Registered handler for {signal.Signals(sig).name}")
-            except (OSError, ValueError) as e:
-                self.logger.warning(f"Could not register handler for {signal.Signals(sig).name}: {e}")
-    
-    def register_cleanup_handler(self, name: str, handler: Callable) -> None:
-        """Register a cleanup handler for shutdown."""
-        self.cleanup_handlers[name] = handler
-        self.context.active_resources.add(name)
-        self.logger.debug(f"Registered cleanup handler: {name}")
-    
-    def register_database_connection(self, connection: Any) -> None:
-        """Register a database connection for cleanup."""
-        self.database_connections.add(connection)
-        self.logger.debug(f"Registered database connection: {id(connection)}")
-    
-    def register_background_thread(self, thread: threading.Thread) -> None:
-        """Register a background thread for cleanup."""
-        self.background_threads.add(thread)
-        self.logger.debug(f"Registered background thread: {thread.name}")
-    
-    def register_server_instance(self, server: Any) -> None:
-        """Register a server instance for cleanup."""
-        self.server_instances.add(server)
-        self.logger.debug(f"Registered server instance: {type(server).__name__}")
-    
-    def add_shutdown_callback(self, callback: Callable) -> None:
-        """Add a callback to execute during shutdown."""
-        self.context.shutdown_callbacks.append(callback)
-        self.logger.debug(f"Added shutdown callback: {callback.__name__}")
-    
-    def is_shutdown_initiated(self) -> bool:
-        """Check if shutdown has been initiated."""
-        return self.context.shutdown_initiated
-    
-    def shutdown(self, reason: str = "Manual shutdown", timeout_seconds: Optional[int] = None) -> bool:
-        """Initiate graceful shutdown process."""
-        with self._shutdown_lock:
-            if self.context.shutdown_initiated:
-                self.logger.warning("Shutdown already initiated, ignoring duplicate request")
-                return True
-            
-            self.context.shutdown_initiated = True
-            self.context.shutdown_reason = reason
-            self.context.shutdown_start_time = datetime.now(timezone.utc)
-            
-            if timeout_seconds:
-                self.context.timeout_seconds = timeout_seconds
-        
-        self.logger.info(f"Graceful shutdown initiated: {reason}")
-        
-        if LOGGING_AVAILABLE and application_logger:
-            application_logger.log_system_event(
-                "shutdown_initiated",
-                f"Graceful shutdown started: {reason}",
-                {
-                    "reason": reason,
-                    "timeout_seconds": self.context.timeout_seconds,
-                    "active_resources": list(self.context.active_resources)
-                }
-            )
-        
-        return self._execute_shutdown()
-    
-    def _execute_shutdown(self) -> bool:
-        """Execute the shutdown process."""
-        success = True
-        
+    def __init__(self, timeout_seconds: int = 30) -> None:
+        self._lock = threading.Lock()
+        self.ctx = ShutdownContext(timeout_seconds=timeout_seconds)
+        self._installed = False
+
+    # Public API
+    def register(self, fn: Callback) -> None:
+        with self._lock:
+            self.ctx.callbacks.append(fn)
+
+    def unregister(self, fn: Callback) -> None:
+        with self._lock:
+            self.ctx.callbacks = [f for f in self.ctx.callbacks if f is not fn]
+
+    def initiate(self, reason: str = "signal") -> None:
+        with self._lock:
+            if self.ctx.initiated:
+                return
+            self.ctx.initiated = True
+            self.ctx.reason = reason
+            self.ctx.started_at = datetime.utcnow()
+            logging.warning("Graceful shutdown initiated: %s", reason)
+
+        # run callbacks outside the lock
+        self._run_callbacks()
+
+    def is_shutting_down(self) -> bool:
+        return self.ctx.initiated
+
+    # Internal
+    def _run_callbacks(self) -> None:
+        deadline = time.time() + self.ctx.timeout_seconds
+        for i, fn in enumerate(list(self.ctx.callbacks)):
+            remaining = max(0.0, deadline - time.time())
+            if remaining <= 0:
+                logging.error("Shutdown timed out before running all callbacks")
+                break
+            t = threading.Thread(target=self._safe_call, args=(fn, i), daemon=True)
+            t.start()
+            t.join(timeout=remaining)
+
+    def _safe_call(self, fn: Callback, idx: int) -> None:
+        name = getattr(fn, "__name__", f"callback_{idx}")
         try:
-            # Phase 1: Execute shutdown callbacks
-            self.logger.info("Phase 1: Executing shutdown callbacks")
-            self._execute_shutdown_callbacks()
-            
-            # Phase 2: Stop accepting new connections
-            self.logger.info("Phase 2: Stopping server instances")
-            success &= self._cleanup_servers()
-            
-            # Phase 3: Cleanup registered handlers
-            self.logger.info("Phase 3: Executing cleanup handlers")
-            success &= self._execute_cleanup_handlers()
-            
-            # Phase 4: Close database connections
-            self.logger.info("Phase 4: Closing database connections")
-            success &= self._cleanup_database_connections()
-            
-            # Phase 5: Terminate background threads
-            self.logger.info("Phase 5: Terminating background threads")
-            success &= self._cleanup_background_threads()
-            
-            # Phase 6: Final cleanup
-            self.logger.info("Phase 6: Final cleanup")
-            self._final_cleanup()
-            
-            shutdown_duration = (
-                datetime.now(timezone.utc) - self.context.shutdown_start_time
-            ).total_seconds()
-            
-            status = "completed successfully" if success else "completed with errors"
-            self.logger.info(f"Graceful shutdown {status} in {shutdown_duration:.2f}s")
-            
-            if LOGGING_AVAILABLE and application_logger:
-                application_logger.log_system_event(
-                    "shutdown_completed",
-                    f"Graceful shutdown {status}",
-                    {
-                        "success": success,
-                        "duration_seconds": shutdown_duration,
-                        "cleanup_results": self.context.cleanup_results
-                    }
-                )
-            
-            return success
-            
+            fn()
+            ok = True
         except Exception as e:
-            self.logger.error(f"Error during graceful shutdown: {e}", exc_info=True)
-            return False
-    
-    def _execute_shutdown_callbacks(self):
-        """Execute all registered shutdown callbacks."""
-        for callback in self.context.shutdown_callbacks:
-            try:
-                self.logger.debug(f"Executing shutdown callback: {callback.__name__}")
-                callback()
-            except Exception as e:
-                self.logger.error(f"Error in shutdown callback {callback.__name__}: {e}")
-    
-    def _cleanup_servers(self) -> bool:
-        """Cleanup server instances."""
-        success = True
-        servers_to_cleanup = list(self.server_instances)
-        
-        for server in servers_to_cleanup:
+            logging.exception("Shutdown callback %s failed: %s", name, e)
+            ok = False
+        with self._lock:
+            self.ctx.results[name] = ok
+
+    def install_signal_handlers(self) -> None:
+        if self._installed:
+            return
+        self._installed = True
+        for sig in (getattr(signal, "SIGINT", None), getattr(signal, "SIGTERM", None), getattr(signal, "SIGHUP", None)):
+            if sig is None:
+                continue
             try:
-                self.logger.debug(f"Stopping server: {type(server).__name__}")
-                
-                if hasattr(server, 'stop'):
-                    server.stop()
-                elif hasattr(server, 'shutdown'):
-                    server.shutdown()
-                elif hasattr(server, 'close'):
-                    server.close()
-                else:
-                    self.logger.warning(f"Server {type(server).__name__} has no stop method")
-                
-                self.context.cleanup_results[f"server_{type(server).__name__}"] = True
-                
-            except Exception as e:
-                self.logger.error(f"Error stopping server {type(server).__name__}: {e}")
-                self.context.cleanup_results[f"server_{type(server).__name__}"] = False
-                success = False
-        
-        # Clear the set after all cleanup attempts
-        self.server_instances.clear()
-        return success
-    
-    def _execute_cleanup_handlers(self) -> bool:
-        """Execute all registered cleanup handlers."""
-        success = True
-        
-        for name, handler in self.cleanup_handlers.items():
-            try:
-                self.logger.debug(f"Executing cleanup handler: {name}")
-                handler()
-                self.context.cleanup_results[name] = True
-                self.context.active_resources.discard(name)
-                
-            except Exception as e:
-                self.logger.error(f"Error in cleanup handler {name}: {e}")
-                self.context.cleanup_results[name] = False
-                success = False
-        
-        return success
-    
-    def _cleanup_database_connections(self) -> bool:
-        """Cleanup database connections."""
-        success = True
-        connections_to_cleanup = list(self.database_connections)
-        
-        # Cleanup registered connections
-        for connection in connections_to_cleanup:
-            try:
-                self.logger.debug(f"Closing database connection: {id(connection)}")
-                
-                if hasattr(connection, 'close'):
-                    connection.close()
-                elif hasattr(connection, 'disconnect'):
-                    connection.disconnect()
-                
-                self.context.cleanup_results[f"db_connection_{id(connection)}"] = True
-                
-            except Exception as e:
-                self.logger.error(f"Error closing database connection {id(connection)}: {e}")
-                self.context.cleanup_results[f"db_connection_{id(connection)}"] = False
-                success = False
-        
-        # Clear the set after all cleanup attempts
-        self.database_connections.clear()
-        
-        # Cleanup DatabaseManager if available
-        if DATABASE_AVAILABLE:
-            try:
-                # This would cleanup the global DatabaseManager instance
-                self.logger.debug("Cleaning up DatabaseManager connections")
-                # Note: DatabaseManager cleanup would be implemented here
-                self.context.cleanup_results["database_manager"] = True
-                
-            except Exception as e:
-                self.logger.error(f"Error cleaning up DatabaseManager: {e}")
-                self.context.cleanup_results["database_manager"] = False
-                success = False
-        
-        return success
-    
-    def _cleanup_background_threads(self) -> bool:
-        """Cleanup background threads."""
-        success = True
-        timeout = max(1, self.context.timeout_seconds // 4)  # 25% of total timeout
-        threads_to_cleanup = list(self.background_threads)
-        
-        for thread in threads_to_cleanup:
-            try:
-                if thread.is_alive():
-                    self.logger.debug(f"Waiting for thread to finish: {thread.name}")
-                    thread.join(timeout=timeout)
-                    
-                    if thread.is_alive():
-                        self.logger.warning(f"Thread {thread.name} did not finish within timeout")
-                        self.context.cleanup_results[f"thread_{thread.name}"] = False
-                        success = False
-                    else:
-                        self.context.cleanup_results[f"thread_{thread.name}"] = True
-                else:
-                    self.context.cleanup_results[f"thread_{thread.name}"] = True
-                
-            except Exception as e:
-                self.logger.error(f"Error cleaning up thread {thread.name}: {e}")
-                self.context.cleanup_results[f"thread_{thread.name}"] = False
-                success = False
-        
-        # Clear the set after all cleanup attempts
-        self.background_threads.clear()
-        return success
-    
-    def _final_cleanup(self):
-        """Perform final cleanup operations."""
-        try:
-            # Restore original signal handlers
-            for sig, handler in self._original_handlers.items():
-                signal.signal(sig, handler)
-            
-            # Clear collections
-            self.cleanup_handlers.clear()
-            self.database_connections.clear()
-            self.server_instances.clear()
-            self.background_threads.clear()
-            
-            self.logger.debug("Final cleanup completed")
-            
-        except Exception as e:
-            self.logger.error(f"Error in final cleanup: {e}")
-    
-    def get_shutdown_status(self) -> Dict[str, Any]:
-        """Get current shutdown status."""
-        if not self.context.shutdown_initiated:
-            return {
-                "shutdown_initiated": False,
-                "status": "running"
-            }
-        
-        elapsed_time = 0
-        if self.context.shutdown_start_time:
-            elapsed_time = (
-                datetime.now(timezone.utc) - self.context.shutdown_start_time
-            ).total_seconds()
-        
-        return {
-            "shutdown_initiated": True,
-            "reason": self.context.shutdown_reason,
-            "start_time": self.context.shutdown_start_time.isoformat() if self.context.shutdown_start_time else None,
-            "elapsed_seconds": elapsed_time,
-            "timeout_seconds": self.context.timeout_seconds,
-            "active_resources": list(self.context.active_resources),
-            "cleanup_results": self.context.cleanup_results,
-            "status": "shutting_down" if elapsed_time < self.context.timeout_seconds else "timeout_exceeded"
-        }
-
+                signal.signal(sig, self._on_signal)  # type: ignore[arg-type]
+            except Exception:  # pragma: no cover
+                pass
 
-# Global shutdown manager instance
-_shutdown_manager: Optional[GracefulShutdownManager] = None
+    def _on_signal(self, signum, frame) -> None:  # pragma: no cover (depends on OS)
+        names = {getattr(signal, n): n for n in dir(signal) if n.startswith("SIG")}
+        self.initiate(reason=f"signal:{names.get(signum, signum)}")
 
+_manager: Optional[GracefulShutdownManager] = None
+_manager_lock = threading.Lock()
 
-def get_shutdown_manager(timeout_seconds: int = 30) -> GracefulShutdownManager:
-    """Get the global shutdown manager instance."""
-    global _shutdown_manager
-    
-    if _shutdown_manager is None:
-        _shutdown_manager = GracefulShutdownManager(timeout_seconds)
-    
-    return _shutdown_manager
+def get_manager() -> GracefulShutdownManager:
+    global _manager
+    with _manager_lock:
+        if _manager is None:
+            _manager = GracefulShutdownManager(timeout_seconds=int(float((__import__('os').getenv('SHUTDOWN_TIMEOUT','30')))))
+            _manager.install_signal_handlers()
+        return _manager
 
+# Convenience wrappers
+def register_cleanup(fn: Callback) -> None:
+    get_manager().register(fn)
 
-@contextmanager
-def shutdown_handler(timeout_seconds: int = 30):
-    """Context manager for graceful shutdown handling."""
-    # Create a fresh manager instance for this context
-    manager = GracefulShutdownManager(timeout_seconds)
-    
-    try:
-        yield manager
-    finally:
-        if not manager.is_shutdown_initiated():
-            manager.shutdown("Context exit")
-
-
-def register_for_shutdown(resource_name: str, cleanup_func: Callable):
-    """Convenience function to register a resource for cleanup."""
-    manager = get_shutdown_manager()
-    manager.register_cleanup_handler(resource_name, cleanup_func)
-
-
-def shutdown_application(reason: str = "Application shutdown", timeout_seconds: int = 30) -> bool:
-    """Shutdown the application gracefully."""
-    manager = get_shutdown_manager(timeout_seconds)
-    return manager.shutdown(reason)
-
+def initiate_shutdown(reason: str = "manual") -> None:
+    get_manager().initiate(reason)
 
 def is_shutting_down() -> bool:
-    """Check if the application is currently shutting down."""
-    global _shutdown_manager
-    
-    if _shutdown_manager is None:
-        return False
-    
-    return _shutdown_manager.is_shutdown_initiated()
-
+    return get_manager().is_shutting_down()
 
 if __name__ == "__main__":
-    # Test graceful shutdown system
-    print("🛑 Testing Graceful Shutdown System")
-    print("=" * 50)
-    
-    # Setup logging
-    logging.basicConfig(level=logging.INFO)
-    
-    # Create shutdown manager
-    manager = GracefulShutdownManager(timeout_seconds=10)
-    
-    # Register test resources
-    def cleanup_test_resource():
-        print("   ✅ Test resource cleaned up")
-    
-    manager.register_cleanup_handler("test_resource", cleanup_test_resource)
-    
-    # Create test thread
-    def test_thread_func():
-        print("   🧵 Test thread running")
+    logging.basicConfig(level="INFO")
+    def _demo():
+        logging.info("Cleaning up resource...")
+        time.sleep(1)
+        logging.info("Cleanup done.")
+    register_cleanup(_demo)
+    logging.info("Press Ctrl+C to trigger graceful shutdown.")
+    while True:
         time.sleep(2)
-        print("   🧵 Test thread finished")
-    
-    test_thread = threading.Thread(target=test_thread_func, name="test_thread")
-    test_thread.start()
-    manager.register_background_thread(test_thread)
-    
-    # Test shutdown
-    print("🔄 Initiating test shutdown...")
-    success = manager.shutdown("Test shutdown")
-    
-    if success:
-        print("✅ Graceful shutdown test completed successfully")
-    else:
-        print("❌ Graceful shutdown test failed")
-    
-    print("\n📊 Shutdown Status:")
-    status = manager.get_shutdown_status()
-    for key, value in status.items():
-        print(f"   {key}: {value}")
\ No newline at end of file
diff --git a/monitoring/grafana_dashboards.json b/monitoring/grafana_dashboards.json
index a871ada26bfad292f1351eafacb35aa5b8e9e671..a813a4967fce52ac2976126930ccf68de7af4644 100644
--- a/monitoring/grafana_dashboards.json
+++ b/monitoring/grafana_dashboards.json
@@ -1,273 +1,43 @@
 {
+  "overwrite": true,
   "dashboard": {
     "id": null,
-    "title": "TDD Framework - Performance & Security Monitoring",
-    "tags": ["tdd-framework", "performance", "security"],
+    "uid": "tdd-observability",
+    "title": "TDD Framework - Health & Performance",
+    "tags": ["tdd-framework", "observability"],
     "timezone": "browser",
+    "schemaVersion": 39,
+    "version": 1,
+    "refresh": "10s",
+    "time": {"from": "now-6h", "to": "now"},
     "panels": [
       {
         "id": 1,
-        "title": "Request Rate",
         "type": "stat",
+        "title": "Overall Health",
         "targets": [
-          {
-            "expr": "rate(tdd_framework_requests_total[5m])",
-            "legendFormat": "{{method}} {{endpoint}}"
-          }
+          {"expr": "avg(tdd_framework_health_status)", "legendFormat": "health"}
         ],
-        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0},
-        "fieldConfig": {
-          "defaults": {
-            "unit": "reqps",
-            "color": {"mode": "palette-classic"}
-          }
-        }
+        "options": {"reduceOptions": {"calcs": ["lastNotNull"], "fields": ""}, "orientation": "horizontal"}
       },
       {
         "id": 2,
-        "title": "Response Time",
-        "type": "timeseries",
+        "type": "bargauge",
+        "title": "Component Health",
         "targets": [
-          {
-            "expr": "histogram_quantile(0.50, rate(tdd_framework_request_duration_seconds_bucket[5m]))",
-            "legendFormat": "50th percentile"
-          },
-          {
-            "expr": "histogram_quantile(0.95, rate(tdd_framework_request_duration_seconds_bucket[5m]))",
-            "legendFormat": "95th percentile"
-          },
-          {
-            "expr": "histogram_quantile(0.99, rate(tdd_framework_request_duration_seconds_bucket[5m]))",
-            "legendFormat": "99th percentile"
-          }
+          {"expr": "tdd_framework_component_up", "legendFormat": "{{component}}"}
         ],
-        "gridPos": {"h": 8, "w": 12, "x": 6, "y": 0},
-        "fieldConfig": {
-          "defaults": {
-            "unit": "s",
-            "color": {"mode": "palette-classic"}
-          }
-        }
+        "options": {"displayMode": "lcd", "orientation": "horizontal"}
       },
       {
         "id": 3,
-        "title": "Error Rate",
-        "type": "stat",
-        "targets": [
-          {
-            "expr": "rate(tdd_framework_errors_total[5m])",
-            "legendFormat": "{{component}} - {{error_type}}"
-          }
-        ],
-        "gridPos": {"h": 8, "w": 6, "x": 18, "y": 0},
-        "fieldConfig": {
-          "defaults": {
-            "unit": "eps",
-            "color": {"mode": "thresholds"},
-            "thresholds": {
-              "steps": [
-                {"color": "green", "value": null},
-                {"color": "yellow", "value": 1},
-                {"color": "red", "value": 5}
-              ]
-            }
-          }
-        }
-      },
-      {
-        "id": 4,
-        "title": "Security Events",
-        "type": "barchart",
-        "targets": [
-          {
-            "expr": "increase(tdd_framework_security_events_total[1h])",
-            "legendFormat": "{{event_type}} - {{severity}}"
-          }
-        ],
-        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
-        "fieldConfig": {
-          "defaults": {
-            "color": {"mode": "palette-classic"}
-          }
-        }
-      },
-      {
-        "id": 5,
-        "title": "Authentication Attempts",
-        "type": "timeseries",
-        "targets": [
-          {
-            "expr": "rate(tdd_framework_auth_attempts_total{status=\"success\"}[5m])",
-            "legendFormat": "Successful"
-          },
-          {
-            "expr": "rate(tdd_framework_auth_attempts_total{status=\"failed\"}[5m])",
-            "legendFormat": "Failed"
-          }
-        ],
-        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
-        "fieldConfig": {
-          "defaults": {
-            "unit": "ops",
-            "color": {"mode": "palette-classic"}
-          }
-        }
-      },
-      {
-        "id": 6,
-        "title": "System Memory Usage",
-        "type": "timeseries",
-        "targets": [
-          {
-            "expr": "tdd_framework_memory_usage_bytes / 1024 / 1024",
-            "legendFormat": "Memory Usage (MB)"
-          }
-        ],
-        "gridPos": {"h": 8, "w": 8, "x": 0, "y": 16},
-        "fieldConfig": {
-          "defaults": {
-            "unit": "decbytes",
-            "color": {"mode": "palette-classic"}
-          }
-        }
-      },
-      {
-        "id": 7,
-        "title": "CPU Usage",
-        "type": "timeseries",
-        "targets": [
-          {
-            "expr": "tdd_framework_cpu_usage_percent",
-            "legendFormat": "CPU Usage"
-          }
-        ],
-        "gridPos": {"h": 8, "w": 8, "x": 8, "y": 16},
-        "fieldConfig": {
-          "defaults": {
-            "unit": "percent",
-            "color": {"mode": "palette-classic"},
-            "max": 100
-          }
-        }
-      },
-      {
-        "id": 8,
-        "title": "Active Sessions",
-        "type": "stat",
-        "targets": [
-          {
-            "expr": "tdd_framework_active_sessions",
-            "legendFormat": "Sessions"
-          }
-        ],
-        "gridPos": {"h": 8, "w": 8, "x": 16, "y": 16},
-        "fieldConfig": {
-          "defaults": {
-            "color": {"mode": "thresholds"},
-            "thresholds": {
-              "steps": [
-                {"color": "green", "value": null},
-                {"color": "yellow", "value": 50},
-                {"color": "red", "value": 100}
-              ]
-            }
-          }
-        }
-      },
-      {
-        "id": 9,
-        "title": "Database Connections",
         "type": "timeseries",
+        "title": "Uptime (seconds)",
         "targets": [
-          {
-            "expr": "tdd_framework_database_connections",
-            "legendFormat": "Active Connections"
-          }
-        ],
-        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 24},
-        "fieldConfig": {
-          "defaults": {
-            "unit": "short",
-            "color": {"mode": "palette-classic"}
-          }
-        }
-      },
-      {
-        "id": 10,
-        "title": "Cache Hit Ratio",
-        "type": "stat",
-        "targets": [
-          {
-            "expr": "tdd_framework_cache_hit_ratio * 100",
-            "legendFormat": "Hit Ratio"
-          }
-        ],
-        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 24},
-        "fieldConfig": {
-          "defaults": {
-            "unit": "percent",
-            "color": {"mode": "thresholds"},
-            "thresholds": {
-              "steps": [
-                {"color": "red", "value": null},
-                {"color": "yellow", "value": 70},
-                {"color": "green", "value": 90}
-              ]
-            }
-          }
-        }
-      },
-      {
-        "id": 11,
-        "title": "Top Error Components",
-        "type": "table",
-        "targets": [
-          {
-            "expr": "topk(10, sum by (component) (rate(tdd_framework_errors_total[1h])))",
-            "format": "table",
-            "instant": true
-          }
-        ],
-        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 32},
-        "fieldConfig": {
-          "defaults": {
-            "custom": {
-              "displayMode": "list"
-            }
-          }
-        }
-      },
-      {
-        "id": 12,
-        "title": "Request Status Codes",
-        "type": "piechart",
-        "targets": [
-          {
-            "expr": "sum by (status) (rate(tdd_framework_requests_total[5m]))",
-            "legendFormat": "{{status}}"
-          }
-        ],
-        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 32},
-        "fieldConfig": {
-          "defaults": {
-            "color": {"mode": "palette-classic"}
-          }
-        }
+          {"expr": "tdd_framework_uptime_seconds", "legendFormat": "uptime"}
+        ]
       }
     ],
-    "time": {
-      "from": "now-1h",
-      "to": "now"
-    },
-    "refresh": "30s",
-    "schemaVersion": 30,
-    "version": 1,
-    "links": [
-      {
-        "title": "TDD Framework Logs",
-        "url": "/d/logs-dashboard",
-        "type": "dashboards"
-      }
-    ]
+    "templating": { "list": [] }
   }
-}
\ No newline at end of file
+}
diff --git a/monitoring/health_check.py b/monitoring/health_check.py
index 2fafdefdb1df01be8e7c0749670c743915f0e912..65041c2af7df7304a415dbe4f1b23d22ee11cacc 100644
--- a/monitoring/health_check.py
+++ b/monitoring/health_check.py
@@ -1,602 +1,267 @@
 #!/usr/bin/env python3
+# -*- coding: utf-8 -*-
 """
-🩺 Health Check Endpoint System
+🩺 Health Check & Metrics Server (Prometheus-ready)
 
-Addresses report.md requirement: "Implement health-check endpoint for orchestration"
+Provides:
+- /health  (liveness)
+- /ready   (readiness)
+- /metrics (Prometheus exposition format)
 
-This module provides:
-- RESTful health check endpoint
-- Database connectivity verification
-- System resource monitoring
-- Service dependency checks
-- Kubernetes/Docker readiness probe support
+Design goals:
+- Thread-safe, no use of signal.alarm (works on Windows & non-main thread)
+- Soft dependency on project database layer (modular API preferred, legacy supported)
+- Small, dependency-free core (psutil optional)
 """
+from __future__ import annotations
 
-import os
-import sys
-import time
+import argparse
 import json
+import logging
+import os
+import socket
 import sqlite3
 import threading
+import time
+from concurrent.futures import ThreadPoolExecutor, TimeoutError
+from dataclasses import dataclass, field
 from datetime import datetime, timezone
-from typing import Dict, Any, List, Optional, Tuple
-from pathlib import Path
-from http.server import HTTPServer, BaseHTTPRequestHandler
-from urllib.parse import urlparse, parse_qs
-import logging
+from http.server import BaseHTTPRequestHandler, HTTPServer
+from typing import Any, Callable, Dict, List, Optional
 
-# Add project root to path
-sys.path.append(str(Path(__file__).parent.parent))
+START_TIME = time.time()
 
+# Optional psutil
 try:
-    from config.environment import get_config, is_production
-    CONFIG_AVAILABLE = True
-except ImportError:
-    CONFIG_AVAILABLE = False
-    get_config = None
-    is_production = lambda: False
+    import psutil  # type: ignore
+    PSUTIL_AVAILABLE = True
+except Exception:
+    PSUTIL_AVAILABLE = False
 
+# Optional graceful shutdown
 try:
-    from streamlit_extension.utils.database import DatabaseManager
-    DATABASE_AVAILABLE = True
-except ImportError:
-    DATABASE_AVAILABLE = False
-    DatabaseManager = None
-
-logger = logging.getLogger(__name__)
+    from graceful_shutdown import is_shutting_down  # type: ignore
+except Exception:
+    def is_shutting_down() -> bool:  # pragma: no cover - safe fallback
+        return False
 
+# Database adapters (prefer modular API, fallback to legacy)
+def _db_health_probe() -> Dict[str, Any]:
+    # Try modular API
+    try:
+        from streamlit_extension.database import check_health as check_health_mod  # type: ignore
+        result = check_health_mod()
+        # Normalize
+        ok = str(result.get("status", "")).lower() in {"ok", "healthy", "true", "1"}
+        return {"ok": bool(ok), "details": result}
+    except Exception:
+        pass
 
-class HealthStatus:
-    """Health status enumeration."""
-    HEALTHY = "healthy"
-    DEGRADED = "degraded"
-    UNHEALTHY = "unhealthy"
+    # Try legacy DatabaseManager
+    try:
+        from streamlit_extension.utils.database import DatabaseManager  # type: ignore
+        db = DatabaseManager()
+        if hasattr(db, "check_database_health"):
+            result = db.check_database_health()
+            ok = str(result.get("status", "")).lower() in {"ok", "healthy", "true", "1"}
+            return {"ok": bool(ok), "details": result}
+        # Fallback: open a connection
+        conn = db.get_connection()  # type: ignore
+        try:
+            cur = conn.cursor()
+            cur.execute("SELECT 1")
+            _ = cur.fetchone()
+            return {"ok": True, "details": {"status": "ok"}}
+        finally:
+            try:
+                release = getattr(db, 'release_connection', None)
+                if callable(release):
+                    release(conn)
+                else:
+                    conn.close()
+            except Exception:
+                try:
+                    conn.close()
+                except Exception:
+                    pass
+    except Exception as e:
+        # Final fallback: attempt to open sqlite file if provided
+        db_path = os.getenv("FRAMEWORK_DB_PATH", "framework.db")
+        try:
+            conn = sqlite3.connect(db_path, timeout=1)
+            conn.execute("SELECT 1")
+            conn.close()
+            return {"ok": True, "details": {"status": "ok", "driver": "sqlite3", "path": db_path}}
+        except Exception as e2:
+            return {"ok": False, "error": f"{e.__class__.__name__}: {e}", "details": {"driver": "unknown", "path": db_path}}
 
+@dataclass
+class ComponentCheck:
+    name: str
+    fn: Callable[[], Dict[str, Any]]
+    timeout: float = 2.0
 
-class HealthCheck:
-    """Individual health check component."""
-    
-    def __init__(self, name: str, check_func, timeout: float = 5.0, critical: bool = True):
-        self.name = name
-        self.check_func = check_func
-        self.timeout = timeout
-        self.critical = critical
-        self.last_check_time = None
-        self.last_result = None
-        self.last_error = None
-    
     def run(self) -> Dict[str, Any]:
-        """Run health check with timeout."""
-        start_time = time.time()
-        
-        try:
-            # Run check with timeout
-            result = self._run_with_timeout()
-            self.last_check_time = datetime.now(timezone.utc)
-            self.last_result = result
-            self.last_error = None
-            
-            return {
-                "name": self.name,
-                "status": result.get("status", HealthStatus.HEALTHY),
-                "message": result.get("message", "Check passed"),
-                "details": result.get("details", {}),
-                "critical": self.critical,
-                "duration_ms": round((time.time() - start_time) * 1000, 2),
-                "timestamp": self.last_check_time.isoformat()
-            }
-            
-        except Exception as e:
-            self.last_check_time = datetime.now(timezone.utc)
-            self.last_error = str(e)
-            self.last_result = None
-            
-            return {
-                "name": self.name,
-                "status": HealthStatus.UNHEALTHY,
-                "message": f"Health check failed: {e}",
-                "details": {"error": str(e)},
-                "critical": self.critical,
-                "duration_ms": round((time.time() - start_time) * 1000, 2),
-                "timestamp": self.last_check_time.isoformat()
-            }
-    
-    def _run_with_timeout(self) -> Dict[str, Any]:
-        """Run check function with timeout."""
-        import signal
-        
-        def timeout_handler(signum, frame):
-            raise TimeoutError(f"Health check '{self.name}' timed out after {self.timeout}s")
-        
-        # Set timeout (Unix only)
-        if hasattr(signal, 'SIGALRM'):
-            old_handler = signal.signal(signal.SIGALRM, timeout_handler)
-            signal.alarm(int(self.timeout))
-        
-        try:
-            result = self.check_func()
-            return result if isinstance(result, dict) else {"status": HealthStatus.HEALTHY}
-        finally:
-            if hasattr(signal, 'SIGALRM'):
-                signal.alarm(0)
-                signal.signal(signal.SIGALRM, old_handler)
+        with ThreadPoolExecutor(max_workers=1) as ex:
+            fut = ex.submit(self.fn)
+            try:
+                return fut.result(timeout=self.timeout)
+            except TimeoutError:
+                return {"ok": False, "error": "timeout"}
+            except Exception as e:
+                return {"ok": False, "error": f"{e.__class__.__name__}: {e}"}
+
+@dataclass
+class HealthState:
+    last_run: Optional[datetime] = None
+    last_status: str = "unknown"
+    last_details: Dict[str, Any] = field(default_factory=dict)
+    total_checks: int = 0
+    total_failures: int = 0
 
+class HealthService:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self.state = HealthState()
+        self.checks: List[ComponentCheck] = [
+            ComponentCheck("database", _db_health_probe, timeout=float(os.getenv("HEALTH_DB_TIMEOUT", "2.0"))),
+            ComponentCheck("system", self._system_probe, timeout=float(os.getenv("HEALTH_SYS_TIMEOUT", "1.0"))),
+        ]
 
-class HealthCheckManager:
-    """Manages multiple health checks and provides endpoints."""
-    
-    def __init__(self):
-        self.checks: List[HealthCheck] = []
-        self.start_time = datetime.now(timezone.utc)
-        self.version = "1.0.0"
-        self.environment = "unknown"
-        
-        if CONFIG_AVAILABLE:
+    def _system_probe(self) -> Dict[str, Any]:
+        data: Dict[str, Any] = {"hostname": socket.gethostname()}
+        if PSUTIL_AVAILABLE:
             try:
-                config = get_config()
-                self.environment = config.environment
-                self.version = config.version
-            except Exception:
-                pass
-        
-        self._register_default_checks()
-    
-    def _register_default_checks(self):
-        """Register default health checks."""
-        # Database connectivity check
-        if DATABASE_AVAILABLE:
-            self.add_check("database", self._check_database, timeout=3.0, critical=True)
-        
-        # Configuration check
-        if CONFIG_AVAILABLE:
-            self.add_check("configuration", self._check_configuration, timeout=1.0, critical=True)
-        
-        # Disk space check
-        self.add_check("disk_space", self._check_disk_space, timeout=2.0, critical=False)
-        
-        # Memory check
-        self.add_check("memory", self._check_memory, timeout=1.0, critical=False)
-        
-        # System uptime
-        self.add_check("uptime", self._check_uptime, timeout=0.5, critical=False)
-    
-    def add_check(self, name: str, check_func, timeout: float = 5.0, critical: bool = True):
-        """Add a custom health check."""
-        check = HealthCheck(name, check_func, timeout, critical)
-        self.checks.append(check)
-        logger.info(f"Added health check: {name}")
-    
-    def run_checks(self) -> Dict[str, Any]:
-        """Run all health checks and return aggregated results."""
-        start_time = time.time()
-        check_results = []
-        
-        # Run all checks
-        for check in self.checks:
-            result = check.run()
-            check_results.append(result)
-        
-        # Calculate overall status
-        overall_status = self._calculate_overall_status(check_results)
-        
-        # Build response
-        response = {
-            "status": overall_status,
-            "timestamp": datetime.now(timezone.utc).isoformat(),
-            "environment": self.environment,
-            "version": self.version,
-            "uptime_seconds": round((datetime.now(timezone.utc) - self.start_time).total_seconds()),
-            "duration_ms": round((time.time() - start_time) * 1000, 2),
-            "checks": check_results
-        }
-        
-        return response
-    
-    def _calculate_overall_status(self, check_results: List[Dict[str, Any]]) -> str:
-        """Calculate overall health status from individual checks."""
-        critical_failed = any(
-            result["status"] == HealthStatus.UNHEALTHY and result["critical"]
-            for result in check_results
-        )
-        
-        any_failed = any(
-            result["status"] in [HealthStatus.UNHEALTHY, HealthStatus.DEGRADED]
-            for result in check_results
-        )
-        
-        if critical_failed:
-            return HealthStatus.UNHEALTHY
-        elif any_failed:
-            return HealthStatus.DEGRADED
-        else:
-            return HealthStatus.HEALTHY
-    
-    def _check_database(self) -> Dict[str, Any]:
-        """Check database connectivity."""
-        if not DATABASE_AVAILABLE:
-            return {
-                "status": HealthStatus.DEGRADED,
-                "message": "Database manager not available"
-            }
-        
-        try:
-            db_manager = DatabaseManager()
-            
-            # Test framework database
-            with db_manager.get_connection("framework") as conn:
-                cursor = conn.cursor()
-                cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table'")
-                table_count = cursor.fetchone()[0]
-            
-            return {
-                "status": HealthStatus.HEALTHY,
-                "message": "Database connectivity verified",
-                "details": {
-                    "tables": table_count,
-                    "framework_db": str(db_manager.framework_db_path),
-                    "timer_db": str(db_manager.timer_db_path)
-                }
-            }
-            
-        except Exception as e:
-            return {
-                "status": HealthStatus.UNHEALTHY,
-                "message": f"Database check failed: {e}",
-                "details": {"error": str(e)}
-            }
-    
-    def _check_configuration(self) -> Dict[str, Any]:
-        """Check configuration loading."""
-        if not CONFIG_AVAILABLE:
-            return {
-                "status": HealthStatus.DEGRADED,
-                "message": "Configuration system not available"
-            }
-        
-        try:
-            config = get_config()
-            
-            # Validate critical settings
-            issues = []
-            
-            if is_production():
-                if not config.google_oauth.client_id:
-                    issues.append("Missing Google OAuth client ID")
-                if not config.google_oauth.client_secret:
-                    issues.append("Missing Google OAuth client secret")
-            
-            if issues:
-                return {
-                    "status": HealthStatus.DEGRADED,
-                    "message": "Configuration issues detected",
-                    "details": {"issues": issues}
-                }
-            
-            return {
-                "status": HealthStatus.HEALTHY,
-                "message": "Configuration loaded successfully",
-                "details": {
-                    "environment": config.environment,
-                    "debug": config.debug,
-                    "auth_required": config.security.require_auth
-                }
-            }
-            
-        except Exception as e:
-            return {
-                "status": HealthStatus.UNHEALTHY,
-                "message": f"Configuration check failed: {e}",
-                "details": {"error": str(e)}
-            }
-    
-    def _check_disk_space(self) -> Dict[str, Any]:
-        """Check available disk space."""
-        try:
-            import shutil
-            
-            # Check current directory disk space
-            total, used, free = shutil.disk_usage(".")
-            free_percent = (free / total) * 100
-            
-            # Warning thresholds
-            if free_percent < 5:
-                status = HealthStatus.UNHEALTHY
-                message = f"Critical: Only {free_percent:.1f}% disk space remaining"
-            elif free_percent < 15:
-                status = HealthStatus.DEGRADED
-                message = f"Warning: Only {free_percent:.1f}% disk space remaining"
-            else:
-                status = HealthStatus.HEALTHY
-                message = f"Disk space: {free_percent:.1f}% available"
-            
-            return {
-                "status": status,
-                "message": message,
-                "details": {
-                    "total_gb": round(total / (1024**3), 2),
-                    "used_gb": round(used / (1024**3), 2),
-                    "free_gb": round(free / (1024**3), 2),
-                    "free_percent": round(free_percent, 1)
-                }
-            }
-            
-        except Exception as e:
-            return {
-                "status": HealthStatus.DEGRADED,
-                "message": f"Disk space check failed: {e}",
-                "details": {"error": str(e)}
-            }
-    
-    def _check_memory(self) -> Dict[str, Any]:
-        """Check memory usage."""
-        try:
-            import psutil
-            
-            memory = psutil.virtual_memory()
-            available_percent = memory.available / memory.total * 100
-            
-            # Memory thresholds
-            if available_percent < 10:
-                status = HealthStatus.UNHEALTHY
-                message = f"Critical: Only {available_percent:.1f}% memory available"
-            elif available_percent < 20:
-                status = HealthStatus.DEGRADED
-                message = f"Warning: Only {available_percent:.1f}% memory available"
-            else:
-                status = HealthStatus.HEALTHY
-                message = f"Memory: {available_percent:.1f}% available"
-            
-            return {
-                "status": status,
-                "message": message,
-                "details": {
-                    "total_gb": round(memory.total / (1024**3), 2),
-                    "available_gb": round(memory.available / (1024**3), 2),
-                    "used_percent": round(memory.percent, 1),
-                    "available_percent": round(available_percent, 1)
-                }
-            }
-            
-        except ImportError:
-            return {
-                "status": HealthStatus.DEGRADED,
-                "message": "psutil not available for memory monitoring",
-                "details": {"note": "Install psutil for memory monitoring"}
-            }
-        except Exception as e:
-            return {
-                "status": HealthStatus.DEGRADED,
-                "message": f"Memory check failed: {e}",
-                "details": {"error": str(e)}
-            }
-    
-    def _check_uptime(self) -> Dict[str, Any]:
-        """Check application uptime."""
-        try:
-            uptime_seconds = (datetime.now(timezone.utc) - self.start_time).total_seconds()
-            uptime_hours = uptime_seconds / 3600
-            
-            return {
-                "status": HealthStatus.HEALTHY,
-                "message": f"Application uptime: {uptime_hours:.1f} hours",
-                "details": {
-                    "uptime_seconds": round(uptime_seconds),
-                    "uptime_hours": round(uptime_hours, 2),
-                    "start_time": self.start_time.isoformat()
-                }
-            }
-            
-        except Exception as e:
-            return {
-                "status": HealthStatus.DEGRADED,
-                "message": f"Uptime check failed: {e}",
-                "details": {"error": str(e)}
-            }
+                data.update({
+                    "cpu_percent": psutil.cpu_percent(interval=0.05),
+                    "mem_percent": psutil.virtual_memory().percent,
+                    "disk_percent": psutil.disk_usage("/").percent,
+                })
+                return {"ok": True, "details": data}
+            except Exception as e:
+                return {"ok": False, "error": f"{e.__class__.__name__}: {e}"}
+        return {"ok": True, "details": data}
 
+    def evaluate(self) -> Dict[str, Any]:
+        results: Dict[str, Dict[str, Any]] = {}
+        healthy_components = True
+        for chk in self.checks:
+            res = chk.run()
+            results[chk.name] = res
+            if not res.get("ok", False):
+                healthy_components = False
 
-class HealthCheckRequestHandler(BaseHTTPRequestHandler):
-    """HTTP request handler for health check endpoints."""
-    
-    def __init__(self, health_manager: HealthCheckManager, *args, **kwargs):
-        self.health_manager = health_manager
-        super().__init__(*args, **kwargs)
-    
-    def do_GET(self):
-        """Handle GET requests."""
-        parsed_path = urlparse(self.path)
-        
-        if parsed_path.path == "/health":
-            self._handle_health_check()
-        elif parsed_path.path == "/ready":
-            self._handle_readiness_check()
-        elif parsed_path.path == "/live":
-            self._handle_liveness_check()
-        elif parsed_path.path == "/metrics":
-            self._handle_metrics()
-        else:
-            self._send_response(404, {"error": "Not found"})
-    
-    def _handle_health_check(self):
-        """Handle full health check."""
-        result = self.health_manager.run_checks()
-        status_code = 200 if result["status"] == HealthStatus.HEALTHY else 503
-        self._send_response(status_code, result)
-    
-    def _handle_readiness_check(self):
-        """Handle Kubernetes readiness probe."""
-        result = self.health_manager.run_checks()
-        
-        # Ready if no critical checks are failing
-        critical_failed = any(
-            check["status"] == HealthStatus.UNHEALTHY and check["critical"]
-            for check in result["checks"]
-        )
-        
-        if critical_failed:
-            self._send_response(503, {
-                "status": "not_ready",
-                "message": "Critical health checks failing"
-            })
-        else:
-            self._send_response(200, {
-                "status": "ready",
-                "message": "Service is ready to accept traffic"
-            })
-    
-    def _handle_liveness_check(self):
-        """Handle Kubernetes liveness probe."""
-        # Simple alive check - just return 200 if process is running
-        self._send_response(200, {
-            "status": "alive",
-            "timestamp": datetime.now(timezone.utc).isoformat()
-        })
-    
-    def _handle_metrics(self):
-        """Handle basic metrics endpoint."""
-        result = self.health_manager.run_checks()
-        
-        # Convert to simple metrics format
-        metrics = {
-            "health_status": 1 if result["status"] == HealthStatus.HEALTHY else 0,
-            "uptime_seconds": result["uptime_seconds"],
-            "total_checks": len(result["checks"]),
-            "failed_checks": sum(1 for check in result["checks"] 
-                               if check["status"] != HealthStatus.HEALTHY),
-            "check_duration_ms": result["duration_ms"]
+        status = "healthy" if healthy_components and not is_shutting_down() else ("unhealthy" if not healthy_components else "degraded")
+
+        with self._lock:
+            self.state.last_run = datetime.now(timezone.utc)
+            self.state.last_status = status
+            self.state.last_details = results
+            self.state.total_checks += 1
+            if status != "healthy":
+                self.state.total_failures += 1
+
+        return {
+            "status": status,
+            "shutting_down": is_shutting_down(),
+            "components": results,
+            "timestamp": datetime.now(timezone.utc).isoformat(),
+            "uptime_seconds": int(time.time() - START_TIME),
         }
-        
-        self._send_response(200, metrics)
-    
-    def _send_response(self, status_code: int, data: Dict[str, Any]):
-        """Send JSON response."""
-        self.send_response(status_code)
-        self.send_header("Content-Type", "application/json")
-        self.send_header("Cache-Control", "no-cache")
-        self.end_headers()
-        
-        response_json = json.dumps(data, indent=2)
-        self.wfile.write(response_json.encode('utf-8'))
-    
-    def log_message(self, format, *args):
-        """Override to use Python logging."""
-        logger.info(f"{self.address_string()} - {format % args}")
 
+    # Prometheus exposition
+    def render_metrics(self) -> str:
+        st = self.state
+        lines = []
+        lines.append("# HELP tdd_framework_health_status 1=healthy, 0=unhealthy (degraded flagged via label).")
+        lines.append("# TYPE tdd_framework_health_status gauge")
+        lines.append(f'tdd_framework_health_status{{service="tdd-framework",status="{st.last_status}"}} {1 if st.last_status=="healthy" else 0}')
+        # Components
+        if isinstance(st.last_details, dict):
+            lines.append("# TYPE tdd_framework_component_up gauge")
+            for comp, res in st.last_details.items():
+                ok = 1 if res.get("ok", False) else 0
+                lines.append(f'tdd_framework_component_up{{service="tdd-framework",component="{comp}"}} {ok}')
+        # Uptime & counters
+        lines.append("# TYPE tdd_framework_uptime_seconds counter")
+        lines.append(f"tdd_framework_uptime_seconds {int(time.time() - START_TIME)}")
+        lines.append("# TYPE tdd_framework_health_checks_total counter")
+        lines.append(f"tdd_framework_health_checks_total {self.state.total_checks}")
+        lines.append("# TYPE tdd_framework_health_failures_total counter")
+        lines.append(f"tdd_framework_health_failures_total {self.state.total_failures}")
+        return "\n".join(lines) + "\n"
 
-class HealthCheckServer:
-    """HTTP server for health check endpoints."""
-    
-    def __init__(self, host: str = "0.0.0.0", port: int = 8080):
-        self.host = host
-        self.port = port
-        self.health_manager = HealthCheckManager()
-        self.server = None
-        self.server_thread = None
-    
-    def start(self, blocking: bool = False):
-        """Start the health check server."""
-        # Create request handler with health manager
-        def handler_factory(*args, **kwargs):
-            return HealthCheckRequestHandler(self.health_manager, *args, **kwargs)
-        
-        try:
-            self.server = HTTPServer((self.host, self.port), handler_factory)
-            logger.info(f"Health check server starting on {self.host}:{self.port}")
-            
-            if blocking:
-                self.server.serve_forever()
-            else:
-                self.server_thread = threading.Thread(target=self.server.serve_forever)
-                self.server_thread.daemon = True
-                self.server_thread.start()
-                logger.info(f"Health check server running in background")
-                
-        except Exception as e:
-            logger.error(f"Failed to start health check server: {e}")
-            raise
-    
-    def stop(self):
-        """Stop the health check server."""
-        if self.server:
-            logger.info("Stopping health check server")
-            self.server.shutdown()
-            self.server.server_close()
-            
-        if self.server_thread:
-            self.server_thread.join(timeout=5)
+SERVICE = HealthService()
 
+class Handler(BaseHTTPRequestHandler):
+    server_version = "tdd-health/1.0"
 
-# Global health check server instance
-_health_server: Optional[HealthCheckServer] = None
+    def _json(self, code: int, payload: Dict[str, Any]):
+        data = json.dumps(payload, ensure_ascii=False).encode("utf-8")
+        self.send_response(code)
+        self.send_header("Content-Type", "application/json; charset=utf-8")
+        self.send_header("Content-Length", str(len(data)))
+        self.end_headers()
+        self.wfile.write(data)
 
+    def _text(self, code: int, text: str):
+        data = text.encode("utf-8")
+        self.send_response(code)
+        self.send_header("Content-Type", "text/plain; version=0.0.4")
+        self.send_header("Content-Length", str(len(data)))
+        self.end_headers()
+        self.wfile.write(data)
 
-def start_health_check_server(host: str = "0.0.0.0", port: int = 8080, blocking: bool = False) -> HealthCheckServer:
-    """Start the global health check server."""
-    global _health_server
-    
-    if _health_server is None:
-        _health_server = HealthCheckServer(host, port)
-    
-    _health_server.start(blocking=blocking)
-    return _health_server
+    def do_GET(self):
+        path = self.path.split("?", 1)[0]
+        if path in ("/health", "/live", "/liveness"):
+            result = SERVICE.evaluate()
+            code = 200 if result["status"] == "healthy" else 503
+            self._json(code, result)
+            return
+        if path in ("/ready", "/readiness"):
+            result = SERVICE.evaluate()
+            # readiness fails if shutting down or any component unhealthy
+            ready = (result["status"] == "healthy") and not result.get("shutting_down", False)
+            code = 200 if ready else 503
+            self._json(code, {"ready": ready, **result})
+            return
+        if path == "/metrics":
+            # ensure state updated at least every scrape
+            _ = SERVICE.evaluate()
+            self._text(200, SERVICE.render_metrics())
+            return
+        self._json(404, {"error": "not found", "path": path})
 
+    def log_message(self, format: str, *args):
+        # Quieter HTTP logs by default
+        if os.getenv("HEALTH_HTTP_LOG", "0") == "1":
+            super().log_message(format, *args)
 
-def stop_health_check_server():
-    """Stop the global health check server."""
-    global _health_server
-    
-    if _health_server:
-        _health_server.stop()
-        _health_server = None
+def main(argv: Optional[List[str]] = None) -> int:
+    parser = argparse.ArgumentParser(description="Health check & metrics server")
+    parser.add_argument("--host", default=os.getenv("HEALTH_HOST", "127.0.0.1"))
+    parser.add_argument("--port", type=int, default=int(os.getenv("HEALTH_PORT", "8599")))
+    parser.add_argument("--check-only", action="store_true", help="Run checks once and exit with code")
+    parser.add_argument("--timeout", type=float, default=float(os.getenv("HEALTH_TIMEOUT", "2.0")))
+    args = parser.parse_args(argv)
 
+    logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))
 
-def get_health_status() -> Dict[str, Any]:
-    """Get current health status without starting server."""
-    health_manager = HealthCheckManager()
-    return health_manager.run_checks()
+    if args.check_only:
+        res = SERVICE.evaluate()
+        print(json.dumps(res, indent=2, ensure_ascii=False))
+        return 0 if res["status"] == "healthy" else 1
 
+    server = HTTPServer((args.host, args.port), Handler)
+    logging.info("Health server listening on http://%s:%d", args.host, args.port)
+    try:
+        server.serve_forever()
+    except KeyboardInterrupt:
+        pass
+    finally:
+        server.server_close()
+    return 0
 
 if __name__ == "__main__":
-    # CLI interface for health checks
-    import argparse
-    
-    parser = argparse.ArgumentParser(description="TDD Framework Health Check System")
-    parser.add_argument("--port", type=int, default=8080, help="Server port")
-    parser.add_argument("--host", default="0.0.0.0", help="Server host")
-    parser.add_argument("--check-only", action="store_true", help="Run checks and exit")
-    
-    args = parser.parse_args()
-    
-    if args.check_only:
-        # Run health checks and print results
-        print("🩺 TDD Framework Health Check")
-        print("=" * 50)
-        
-        result = get_health_status()
-        
-        print(f"Overall Status: {result['status'].upper()}")
-        print(f"Environment: {result['environment']}")
-        print(f"Version: {result['version']}")
-        print(f"Uptime: {result['uptime_seconds']}s")
-        print()
-        
-        for check in result['checks']:
-            status_emoji = {
-                HealthStatus.HEALTHY: "✅",
-                HealthStatus.DEGRADED: "⚠️",
-                HealthStatus.UNHEALTHY: "❌"
-            }.get(check['status'], "❓")
-            
-            critical_mark = " (CRITICAL)" if check['critical'] else ""
-            print(f"{status_emoji} {check['name']}{critical_mark}: {check['message']}")
-        
-        # Exit with appropriate code
-        sys.exit(0 if result['status'] == HealthStatus.HEALTHY else 1)
-    
-    else:
-        # Start server
-        try:
-            logging.basicConfig(level=logging.INFO)
-            server = start_health_check_server(args.host, args.port, blocking=True)
-        except KeyboardInterrupt:
-            print("\nShutting down health check server...")
-            stop_health_check_server()
\ No newline at end of file
+    raise SystemExit(main())
diff --git a/monitoring/prometheus.yml b/monitoring/prometheus.yml
index d31d55602986ee6604ef291ac8e83ca2adfc55a0..0ee04a84853cbb993674bf766099e59c3d688a3c 100644
--- a/monitoring/prometheus.yml
+++ b/monitoring/prometheus.yml
@@ -1,94 +1,19 @@
-# Prometheus Configuration for TDD Framework Monitoring
-# 
-# This configuration sets up Prometheus to scrape metrics from:
-# - TDD Framework application
-# - System metrics (node_exporter)
-# - Database metrics
-# - Custom application metrics
-
+# Prometheus configuration for TDD Framework
 global:
   scrape_interval: 15s
   evaluation_interval: 15s
-  external_labels:
-    monitor: 'tdd-framework'
-    environment: 'production'
 
-# Rule files for alerting
 rule_files:
-  - "alert_rules.yml"
-
-# Alertmanager configuration
-alerting:
-  alertmanagers:
-    - static_configs:
-        - targets:
-          - alertmanager:9093
+  - alert_rules.yml
 
-# Scrape configurations
 scrape_configs:
-  # TDD Framework Application Metrics
-  - job_name: 'tdd-framework'
-    static_configs:
-      - targets: ['localhost:8000']
-    metrics_path: /metrics
-    scrape_interval: 15s
-    scrape_timeout: 10s
-    honor_labels: true
-    params:
-      format: ['prometheus']
-
-  # System Metrics (Node Exporter)
-  - job_name: 'node-exporter'
-    static_configs:
-      - targets: ['localhost:9100']
-    metrics_path: /metrics
-    scrape_interval: 30s
-
-  # Streamlit Application Metrics
-  - job_name: 'streamlit-app'
-    static_configs:
-      - targets: ['localhost:8501']
-    metrics_path: /metrics
-    scrape_interval: 30s
-    honor_labels: true
-
-  # Database Metrics (if using external monitoring)
-  - job_name: 'sqlite-exporter'
-    static_configs:
-      - targets: ['localhost:9102']
-    metrics_path: /metrics
-    scrape_interval: 60s
-
-  # Redis Metrics (if Redis cache is enabled)
-  - job_name: 'redis-exporter'
+  - job_name: 'tdd-framework-health'
     static_configs:
-      - targets: ['localhost:9121']
+      - targets: ['localhost:8599']
     metrics_path: /metrics
-    scrape_interval: 30s
-
-  # Custom Performance Metrics
-  - job_name: 'performance-metrics'
-    static_configs:
-      - targets: ['localhost:8001']
-    metrics_path: /performance-metrics
-    scrape_interval: 30s
-
-# Storage configuration
-storage:
-  tsdb:
-    path: /prometheus/data
-    retention.time: 30d
-    retention.size: 10GB
-
-# Remote write configuration (for external systems)
-# remote_write:
-#   - url: "https://prometheus-remote-write-endpoint"
-#     write_relabel_configs:
-#       - source_labels: [__name__]
-#         regex: 'tdd_framework_.*'
-#         action: keep
 
-# Remote read configuration (for external systems)
-# remote_read:
-#   - url: "https://prometheus-remote-read-endpoint"
-#     read_recent: true
\ No newline at end of file
+  # Add exporters as needed:
+  # - job_name: 'node'
+  #   static_configs:
+  #     - targets: ['localhost:9100']
+  #   metrics_path: /metrics
diff --git a/monitoring/structured_logging.py b/monitoring/structured_logging.py
index 5cc748dadc6e13a0132bf8ab42b09794290acedb..bcb08e2fe9ce3f7f96f0c235b35211da7883639d 100644
--- a/monitoring/structured_logging.py
+++ b/monitoring/structured_logging.py
@@ -1,640 +1,115 @@
 #!/usr/bin/env python3
+# -*- coding: utf-8 -*-
 """
-📊 Structured Logging with Correlation IDs
+📊 Structured JSON Logging with Correlation IDs
 
-Addresses report.md requirement: "Set up structured logging and monitoring"
-and "Introduce comprehensive logging with correlation IDs for multi-user tracing"
-
-This module provides:
-- Structured JSON logging
-- Correlation ID tracking across requests
-- Context management for request tracing
-- Performance metrics logging
-- Security event logging
-- Integration with monitoring systems
+- JSON logs by default
+- Correlation ID propagation via contextvars
+- Helper context managers and filters
 """
+from __future__ import annotations
 
+import json
+import logging
 import os
 import sys
-import json
 import time
 import uuid
-import logging
-import threading
-from datetime import datetime, timezone
-from typing import Dict, Any, Optional, List, Union
-from pathlib import Path
-from contextlib import contextmanager
-from dataclasses import dataclass, asdict, field
-
-# Add project root to path
-sys.path.append(str(Path(__file__).parent.parent))
-
-try:
-    from config.environment import get_config, is_production
-    CONFIG_AVAILABLE = True
-except ImportError:
-    CONFIG_AVAILABLE = False
-    get_config = None
-    is_production = lambda: False
-
-try:
-    from duration_system.log_sanitization import sanitize_log_message
-    LOG_SANITIZATION_AVAILABLE = True
-except ImportError:
-    LOG_SANITIZATION_AVAILABLE = False
-    sanitize_log_message = lambda msg, level: msg
-
-
-# Thread-local storage for correlation context
-_correlation_context = threading.local()
+from dataclasses import dataclass, asdict
+from typing import Any, Dict, Optional
+import contextvars
 
+# correlation context
+_correlation_id: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar("correlation_id", default=None)
+_user_id: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar("user_id", default=None)
+_session_id: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar("session_id", default=None)
+_request_id: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar("request_id", default=None)
+_operation: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar("operation", default=None)
 
 @dataclass
 class CorrelationContext:
-    """Correlation context for request tracing."""
-    correlation_id: str
+    correlation_id: Optional[str] = None
     user_id: Optional[str] = None
     session_id: Optional[str] = None
     request_id: Optional[str] = None
     operation: Optional[str] = None
-    client_ip: Optional[str] = None
-    user_agent: Optional[str] = None
-    start_time: float = field(default_factory=time.time)
-    metadata: Dict[str, Any] = field(default_factory=dict)
-    
-    def to_dict(self) -> Dict[str, Any]:
-        """Convert to dictionary for logging."""
-        return {k: v for k, v in asdict(self).items() if v is not None}
 
+def get_correlation_context() -> CorrelationContext:
+    return CorrelationContext(
+        correlation_id=_correlation_id.get(),
+        user_id=_user_id.get(),
+        session_id=_session_id.get(),
+        request_id=_request_id.get(),
+        operation=_operation.get(),
+    )
 
-class StructuredLogFormatter(logging.Formatter):
-    """Custom formatter for structured JSON logging."""
-    
-    def __init__(self, include_correlation: bool = True, include_performance: bool = True):
-        super().__init__()
-        self.include_correlation = include_correlation
-        self.include_performance = include_performance
-    
+class Correlation:
+    """Context manager to set correlation data."""
+    def __init__(self, correlation_id: Optional[str] = None, **kwargs):
+        self.token_vals = []
+        if correlation_id is None:
+            correlation_id = str(uuid.uuid4())
+        self.values = {"correlation_id": correlation_id, **kwargs}
+
+    def __enter__(self):
+        self.token_vals = [
+            (_correlation_id, _correlation_id.set(self.values.get("correlation_id"))),
+            (_user_id, _user_id.set(self.values.get("user_id"))),
+            (_session_id, _session_id.set(self.values.get("session_id"))),
+            (_request_id, _request_id.set(self.values.get("request_id"))),
+            (_operation, _operation.set(self.values.get("operation"))),
+        ]
+        return self
+
+    def __exit__(self, exc_type, exc, tb):
+        for var, tok in reversed(self.token_vals):
+            var.reset(tok)
+
+class JsonFormatter(logging.Formatter):
     def format(self, record: logging.LogRecord) -> str:
-        """Format log record as structured JSON."""
-        # Base log entry
-        log_entry = {
-            "timestamp": datetime.fromtimestamp(record.created, timezone.utc).isoformat(),
+        ctx = get_correlation_context()
+        payload: Dict[str, Any] = {
+            "ts": time.strftime("%Y-%m-%dT%H:%M:%S%z", time.localtime(record.created)),
             "level": record.levelname,
             "logger": record.name,
             "message": record.getMessage(),
-            "module": record.module,
-            "function": record.funcName,
-            "line": record.lineno,
+            "correlation_id": ctx.correlation_id,
+            "user_id": ctx.user_id,
+            "session_id": ctx.session_id,
+            "request_id": ctx.request_id,
+            "operation": ctx.operation,
         }
-        
-        # Add correlation context if available
-        if self.include_correlation:
-            correlation = get_correlation_context()
-            if correlation:
-                log_entry["correlation"] = correlation.to_dict()
-        
-        # Add performance metrics if available
-        if self.include_performance and hasattr(record, 'performance'):
-            log_entry["performance"] = record.performance
-        
-        # Add custom fields from record
-        for attr in ['user_id', 'operation', 'error_code', 'request_size', 'response_size']:
-            if hasattr(record, attr):
-                log_entry[attr] = getattr(record, attr)
-        
-        # Add exception information
+        # extra dict
+        for key, val in record.__dict__.items():
+            if key in ("args", "msg", "exc_info", "exc_text", "stack_info", "lineno", "msecs",
+                        "relativeCreated", "created", "levelno", "pathname", "filename", "module",
+                        "funcName", "thread", "threadName", "processName", "process"):
+                continue
+            if key not in payload:
+                payload[key] = val
         if record.exc_info:
-            import traceback
-            log_entry["exception"] = {
-                "type": record.exc_info[0].__name__ if record.exc_info[0] else None,
-                "message": str(record.exc_info[1]) if record.exc_info[1] else None,
-                "traceback": traceback.format_exception(*record.exc_info)
-            }
-        
-        # Add extra fields
-        if hasattr(record, 'extra_fields'):
-            log_entry["extra_fields"] = record.extra_fields
-        
-        # Sanitize sensitive data if available
-        if LOG_SANITIZATION_AVAILABLE:
-            log_entry["message"] = sanitize_log_message(log_entry["message"], record.levelname)
-        
-        return json.dumps(log_entry, ensure_ascii=False)
-
-
-class CorrelationIDFilter(logging.Filter):
-    """Filter to add correlation ID to log records."""
-    
-    def filter(self, record: logging.LogRecord) -> bool:
-        """Add correlation context to log record."""
-        correlation = get_correlation_context()
-        if correlation:
-            record.correlation_id = correlation.correlation_id
-            record.user_id = correlation.user_id
-            record.session_id = correlation.session_id
-            record.operation = correlation.operation
-        
-        return True
-
-
-class PerformanceLoggerMixin:
-    """Mixin to add performance logging capabilities."""
-    
-    def log_performance(self, operation: str, duration_ms: float, **kwargs):
-        """Log performance metrics."""
-        performance_data = {
-            "operation": operation,
-            "duration_ms": round(duration_ms, 2),
-            "timestamp": datetime.now(timezone.utc).isoformat(),
-            **kwargs
-        }
-        
-        # Use structured logger if available
-        logger = getattr(self, 'logger', logging.getLogger(__name__))
-        
-        # Create log record with performance data
-        record = logger.makeRecord(
-            logger.name, logging.INFO, "", 0,
-            f"Performance: {operation} completed in {duration_ms:.2f}ms",
-            (), None
-        )
-        record.performance = performance_data
-        
-        logger.handle(record)
-    
-    @contextmanager
-    def performance_timer(self, operation: str, **kwargs):
-        """Context manager for timing operations."""
-        start_time = time.time()
-        try:
-            yield
-        finally:
-            duration = (time.time() - start_time) * 1000
-            self.log_performance(operation, duration, **kwargs)
-
-
-class SecurityEventLogger:
-    """Logger for security-related events."""
-    
-    def __init__(self, logger_name: str = "security"):
-        self.logger = get_structured_logger(logger_name)
-    
-    def log_authentication_attempt(self, user_id: str, success: bool, method: str = "oauth", **kwargs):
-        """Log authentication attempt."""
-        self.logger.info(
-            f"Authentication {'succeeded' if success else 'failed'} for user {user_id}",
-            extra={
-                'extra_fields': {
-                    'event_type': 'authentication',
-                    'user_id': user_id,
-                    'success': success,
-                    'method': method,
-                    'timestamp': datetime.now(timezone.utc).isoformat(),
-                    **kwargs
-                }
-            }
-        )
-    
-    def log_authorization_failure(self, user_id: str, resource: str, action: str, **kwargs):
-        """Log authorization failure."""
-        self.logger.warning(
-            f"Authorization denied: user {user_id} attempted {action} on {resource}",
-            extra={
-                'extra_fields': {
-                    'event_type': 'authorization_failure',
-                    'user_id': user_id,
-                    'resource': resource,
-                    'action': action,
-                    'timestamp': datetime.now(timezone.utc).isoformat(),
-                    **kwargs
-                }
-            }
-        )
-    
-    def log_security_violation(self, violation_type: str, details: Dict[str, Any], severity: str = "medium"):
-        """Log security violation."""
-        level = {
-            "low": logging.INFO,
-            "medium": logging.WARNING,
-            "high": logging.ERROR,
-            "critical": logging.CRITICAL
-        }.get(severity, logging.WARNING)
-        
-        self.logger.log(
-            level,
-            f"Security violation detected: {violation_type}",
-            extra={
-                'extra_fields': {
-                    'event_type': 'security_violation',
-                    'violation_type': violation_type,
-                    'severity': severity,
-                    'details': details,
-                    'timestamp': datetime.now(timezone.utc).isoformat()
-                }
-            }
-        )
-    
-    def log_rate_limit_exceeded(self, identifier: str, limit_type: str, **kwargs):
-        """Log rate limit exceeded."""
-        self.logger.warning(
-            f"Rate limit exceeded: {identifier} for {limit_type}",
-            extra={
-                'extra_fields': {
-                    'event_type': 'rate_limit_exceeded',
-                    'identifier': identifier,
-                    'limit_type': limit_type,
-                    'timestamp': datetime.now(timezone.utc).isoformat(),
-                    **kwargs
-                }
-            }
-        )
-
-
-def generate_correlation_id() -> str:
-    """Generate a unique correlation ID."""
-    return str(uuid.uuid4())
-
-
-def get_correlation_context() -> Optional[CorrelationContext]:
-    """Get current correlation context."""
-    return getattr(_correlation_context, 'current', None)
-
-
-def set_correlation_context(context: CorrelationContext):
-    """Set correlation context."""
-    _correlation_context.current = context
-
-
-def clear_correlation_context():
-    """Clear correlation context."""
-    _correlation_context.current = None
-
-
-@contextmanager
-def correlation_context(correlation_id: Optional[str] = None, **kwargs):
-    """Context manager for correlation tracking."""
-    if correlation_id is None:
-        correlation_id = generate_correlation_id()
-    
-    # Create new context
-    context = CorrelationContext(
-        correlation_id=correlation_id,
-        **kwargs
-    )
-    
-    # Store previous context
-    previous_context = get_correlation_context()
-    
-    try:
-        set_correlation_context(context)
-        yield context
-    finally:
-        # Restore previous context
-        if previous_context:
-            set_correlation_context(previous_context)
-        else:
-            clear_correlation_context()
-
-
-def update_correlation_context(**kwargs):
-    """Update current correlation context."""
-    context = get_correlation_context()
-    if context:
-        for key, value in kwargs.items():
-            if hasattr(context, key):
-                setattr(context, key, value)
-            else:
-                context.metadata[key] = value
-
-
-def get_structured_logger(name: str, level: Optional[str] = None) -> logging.Logger:
-    """Get a structured logger with correlation ID support."""
-    logger = logging.getLogger(name)
-    
-    # Configure logger if not already configured
-    if not logger.handlers:
-        setup_structured_logging(logger, level)
-    
-    return logger
-
-
-def setup_structured_logging(logger: Optional[logging.Logger] = None, level: Optional[str] = None):
-    """Setup structured logging configuration."""
-    if logger is None:
-        logger = logging.getLogger()
-    
-    # Determine log level
-    if level is None:
-        if CONFIG_AVAILABLE:
-            try:
-                config = get_config()
-                level = config.security.log_level
-            except Exception:
-                level = "INFO"
-        else:
-            level = "INFO"
-    
-    # Set log level
-    numeric_level = getattr(logging, level.upper(), logging.INFO)
-    logger.setLevel(numeric_level)
-    
-    # Remove existing handlers to avoid duplicates
-    for handler in logger.handlers[:]:
-        logger.removeHandler(handler)
-    
-    # Create handler
-    if CONFIG_AVAILABLE:
-        try:
-            config = get_config()
-            if config.monitoring.log_file_path:
-                handler = logging.FileHandler(config.monitoring.log_file_path)
-            else:
-                handler = logging.StreamHandler()
-        except Exception:
-            handler = logging.StreamHandler()
-    else:
-        handler = logging.StreamHandler()
-    
-    # Setup formatter
-    if CONFIG_AVAILABLE:
+            payload["exc_info"] = self.formatException(record.exc_info)
+        return json.dumps(payload, ensure_ascii=False)
+
+def setup_logging(level: Optional[str] = None, to_stdout: bool = True) -> None:
+    """Configure root logger for JSON output."""
+    lvl = (level or os.getenv("LOG_LEVEL", "INFO")).upper()
+    handler = logging.StreamHandler(sys.stdout if to_stdout else sys.stderr)
+    handler.setFormatter(JsonFormatter())
+    root = logging.getLogger()
+    root.handlers[:] = [handler]
+    root.setLevel(lvl)
+
+def get_logger(name: str) -> logging.Logger:
+    return logging.getLogger(name)
+
+# Example usage
+if __name__ == "__main__":
+    setup_logging("DEBUG")
+    log = get_logger("demo")
+    with Correlation(operation="startup"):
+        log.info("Application starting")
         try:
-            config = get_config()
-            use_json = config.monitoring.log_format == "json"
+            raise ValueError("Example error")
         except Exception:
-            use_json = is_production()
-    else:
-        use_json = False
-    
-    if use_json:
-        formatter = StructuredLogFormatter()
-    else:
-        formatter = logging.Formatter(
-            '%(asctime)s - %(name)s - %(levelname)s - [%(correlation_id)s] - %(message)s',
-            defaults={'correlation_id': 'no-correlation'}
-        )
-    
-    handler.setFormatter(formatter)
-    
-    # Add correlation ID filter
-    correlation_filter = CorrelationIDFilter()
-    handler.addFilter(correlation_filter)
-    
-    logger.addHandler(handler)
-    
-    return logger
-
-
-def log_request_start(operation: str, **kwargs) -> str:
-    """Log request start and return correlation ID."""
-    correlation_id = generate_correlation_id()
-    
-    with correlation_context(
-        correlation_id=correlation_id,
-        operation=operation,
-        **kwargs
-    ):
-        logger = get_structured_logger("request")
-        logger.info(f"Request started: {operation}", extra={
-            'extra_fields': {
-                'event_type': 'request_start',
-                'operation': operation,
-                **kwargs
-            }
-        })
-    
-    return correlation_id
-
-
-def log_request_end(correlation_id: str, success: bool = True, **kwargs):
-    """Log request completion."""
-    with correlation_context(correlation_id=correlation_id):
-        logger = get_structured_logger("request")
-        
-        context = get_correlation_context()
-        duration = None
-        if context:
-            duration = (time.time() - context.start_time) * 1000
-        
-        logger.info(
-            f"Request {'completed' if success else 'failed'}: {context.operation if context else 'unknown'}",
-            extra={
-                'extra_fields': {
-                    'event_type': 'request_end',
-                    'success': success,
-                    'duration_ms': round(duration, 2) if duration else None,
-                    **kwargs
-                }
-            }
-        )
-
-
-class DatabaseLogger(PerformanceLoggerMixin):
-    """Logger for database operations."""
-    
-    def __init__(self):
-        self.logger = get_structured_logger("database")
-    
-    def log_query(self, query_type: str, table: str, duration_ms: float, rows_affected: int = 0):
-        """Log database query."""
-        self.logger.info(
-            f"Database {query_type} on {table}: {rows_affected} rows in {duration_ms:.2f}ms",
-            extra={
-                'extra_fields': {
-                    'event_type': 'database_query',
-                    'query_type': query_type,
-                    'table': table,
-                    'duration_ms': round(duration_ms, 2),
-                    'rows_affected': rows_affected
-                }
-            }
-        )
-    
-    def log_connection_event(self, event_type: str, details: Dict[str, Any]):
-        """Log database connection events."""
-        self.logger.info(
-            f"Database connection {event_type}",
-            extra={
-                'extra_fields': {
-                    'event_type': f'database_{event_type}',
-                    'details': details
-                }
-            }
-        )
-
-
-class ApplicationLogger(PerformanceLoggerMixin):
-    """Logger for general application events."""
-    
-    def __init__(self):
-        self.logger = get_structured_logger("application")
-    
-    def log_user_action(self, action: str, user_id: str, details: Dict[str, Any] = None):
-        """Log user action."""
-        self.logger.info(
-            f"User action: {action} by {user_id}",
-            extra={
-                'extra_fields': {
-                    'event_type': 'user_action',
-                    'action': action,
-                    'user_id': user_id,
-                    'details': details or {}
-                }
-            }
-        )
-    
-    def log_system_event(self, event_type: str, message: str, details: Dict[str, Any] = None):
-        """Log system event."""
-        self.logger.info(
-            f"System event: {message}",
-            extra={
-                'extra_fields': {
-                    'event_type': f'system_{event_type}',
-                    'details': details or {}
-                }
-            }
-        )
-    
-    def log_error(self, error: Exception, context: str = "", **kwargs):
-        """Log application error with context."""
-        self.logger.error(
-            f"Application error in {context}: {error}",
-            exc_info=True,
-            extra={
-                'extra_fields': {
-                    'event_type': 'application_error',
-                    'error_type': type(error).__name__,
-                    'context': context,
-                    **kwargs
-                }
-            }
-        )
-
-
-# Global logger instances
-security_logger = SecurityEventLogger()
-database_logger = DatabaseLogger()
-application_logger = ApplicationLogger()
-
-
-def setup_monitoring_integration():
-    """Setup integration with monitoring systems."""
-    if not CONFIG_AVAILABLE:
-        return
-    
-    try:
-        config = get_config()
-        
-        # Setup file logging for production
-        if config.environment == "production" and not config.monitoring.log_file_path:
-            log_dir = Path("/app/logs")
-            log_dir.mkdir(exist_ok=True)
-            config.monitoring.log_file_path = str(log_dir / "tdd_framework.log")
-        
-        # Setup root logger
-        setup_structured_logging()
-        
-        logging.info("Structured logging initialized", extra={
-            'extra_fields': {
-                'event_type': 'logging_initialized',
-                'environment': config.environment,
-                'log_format': config.monitoring.log_format,
-                'correlation_enabled': config.monitoring.enable_correlation_ids
-            }
-        })
-        
-    except Exception as e:
-        logging.error(f"Failed to setup monitoring integration: {e}")
-
-
-# Decorators for easy integration
-def with_correlation(operation: str = None):
-    """Decorator to add correlation tracking to functions."""
-    def decorator(func):
-        def wrapper(*args, **kwargs):
-            op_name = operation or f"{func.__module__}.{func.__name__}"
-            
-            # Check if we're already in a correlation context
-            existing_context = get_correlation_context()
-            if existing_context:
-                # Update operation if not set
-                if not existing_context.operation:
-                    update_correlation_context(operation=op_name)
-                return func(*args, **kwargs)
-            
-            # Create new correlation context
-            with correlation_context(operation=op_name):
-                return func(*args, **kwargs)
-        
-        return wrapper
-    return decorator
-
-
-def log_performance(operation: str = None):
-    """Decorator to log function performance."""
-    def decorator(func):
-        def wrapper(*args, **kwargs):
-            op_name = operation or f"{func.__module__}.{func.__name__}"
-            
-            start_time = time.time()
-            try:
-                result = func(*args, **kwargs)
-                duration = (time.time() - start_time) * 1000
-                
-                logger = get_structured_logger("performance")
-                logger.info(f"Performance: {op_name} completed in {duration:.2f}ms", extra={
-                    'extra_fields': {
-                        'event_type': 'performance',
-                        'operation': op_name,
-                        'duration_ms': round(duration, 2),
-                        'success': True
-                    }
-                })
-                
-                return result
-            except Exception as e:
-                duration = (time.time() - start_time) * 1000
-                
-                logger = get_structured_logger("performance")
-                logger.error(f"Performance: {op_name} failed after {duration:.2f}ms", extra={
-                    'extra_fields': {
-                        'event_type': 'performance',
-                        'operation': op_name,
-                        'duration_ms': round(duration, 2),
-                        'success': False,
-                        'error': str(e)
-                    }
-                })
-                raise
-        
-        return wrapper
-    return decorator
-
-
-if __name__ == "__main__":
-    # Test structured logging
-    print("📊 Testing Structured Logging System")
-    print("=" * 50)
-    
-    # Setup logging
-    setup_structured_logging()
-    
-    # Test basic logging
-    logger = get_structured_logger("test")
-    
-    with correlation_context(operation="test_operation", user_id="test_user"):
-        logger.info("Test structured log message")
-        logger.warning("Test warning with correlation")
-        
-        # Test performance logging
-        with application_logger.performance_timer("test_timer"):
-            time.sleep(0.1)
-        
-        # Test security logging
-        security_logger.log_authentication_attempt("test_user", True)
-        
-        # Test database logging
-        database_logger.log_query("SELECT", "framework_epics", 15.5, 10)
-    
-    print("✅ Structured logging system test completed")
\ No newline at end of file
+            log.exception("Something went wrong")
 
EOF
)