 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/streamlit_extension/utils/performance_tester.py b/streamlit_extension/utils/performance_tester.py
index 2a815854668547204b6ac45066db31a64ce99e68..2c41da99d296a02204dc4fdf89f7db80bf8181a5 100644
--- a/streamlit_extension/utils/performance_tester.py
+++ b/streamlit_extension/utils/performance_tester.py
@@ -1,63 +1,75 @@
 """
 ðŸš€ Performance Testing System - Enterprise Load Testing
 
 Comprehensive performance testing and benchmarking system addressing report.md items:
 - Performance bottleneck identification
 - Load testing capabilities 
 - Database query optimization
 - Real-time performance monitoring
 - Automated performance regression detection
 
 Features:
 - Multi-threaded load simulation
 - Database performance profiling
 - Memory usage analysis
 - Response time measurement
 - Throughput analysis
 - Bottleneck detection
 - Performance regression alerts
 """
 
 import time
 import threading
 import sqlite3
+import logging
 try:
     import psutil
 except Exception:  # pragma: no cover
     psutil = None
 import tracemalloc
 from concurrent.futures import ThreadPoolExecutor, as_completed
 from typing import Dict, List, Optional, Any, Callable
 from dataclasses import dataclass
 from contextlib import contextmanager
 import statistics
 import json
 from pathlib import Path
 import datetime
 import gc
 
+# Legacy import - keeping for hybrid compatibility
+from streamlit_extension.utils.database import DatabaseManager  # Legacy compatibility
+from streamlit_extension.database import get_connection, list_epics, list_tasks
+from streamlit_extension.services import ServiceContainer
+# Performance testing imports
+try:  # pragma: no cover
+    from streamlit_extension.utils.performance_monitor import PerformanceTester
+except ImportError:  # pragma: no cover
+    PerformanceTester = None  # type: ignore
+from streamlit_extension.database import get_connection
+
 
 @dataclass
 class PerformanceMetrics:
     """Performance measurement data structure."""
     operation_name: str
     response_time: float
     memory_usage: float
     cpu_usage: float
     threads_count: int
     timestamp: datetime.datetime
     success: bool
     error_message: Optional[str] = None
 
 
 @dataclass
 class LoadTestConfig:
     """Load test configuration."""
     concurrent_users: int = 10
     duration_seconds: int = 60
     operations_per_second: int = 100
     ramp_up_seconds: int = 10
     test_data_size: int = 1000
 
 
 class PerformanceProfiler:
@@ -152,108 +164,147 @@ class PerformanceProfiler:
                 "max": max(response_times),
             },
             "memory": {
                 "avg_delta_mb": float(sum(mem_deltas)/len(mem_deltas)),
                 "max": max(mem_deltas) if mem_deltas else 0.0
             },
         }
         return stats
     
     def _percentile(self, data: List[float], percentile: int) -> float:
         """Calculate percentile."""
         if not data:
             return 0.0
         sorted_data = sorted(data)
         index = (percentile / 100) * (len(sorted_data) - 1)
         if index.is_integer():
             return sorted_data[int(index)]
         else:
             lower = sorted_data[int(index)]
             upper = sorted_data[int(index) + 1]
             return lower + (upper - lower) * (index - int(index))
 
 
 class DatabasePerformanceTester:
     """Specialized database performance testing."""
-    
-    def __init__(self, db_manager):
-        self.db_manager = db_manager
+
+    def __init__(self, db_manager: Optional[DatabaseManager] = None):
+        self.db_manager = db_manager or DatabaseManager()  # Legacy fallback
+        # Performance testing with direct connection
+        self.performance_connection = get_connection()
+        # Service container for business logic testing  
+        self.service_container = ServiceContainer(self.db_manager)
         self.profiler = PerformanceProfiler()
         
     def benchmark_crud_operations(self, iterations: int = 1000) -> Dict[str, Any]:
         """Benchmark CRUD operations performance."""
         results = {}
         
         # Test project operations
         with self.profiler.profile_operation("project_create"):
             for i in range(iterations):
                 self.db_manager.create_project(
                     project_key=f"test_project_{i}",
                     name=f"Test Project {i}",
                     description="Performance test project"
                 )
         
         with self.profiler.profile_operation("project_read"):
             for i in range(iterations):
                 self.db_manager.get_projects(limit=10)
         
         # Collect statistics
         results["project_create"] = self.profiler.get_statistics("project_create")
         results["project_read"] = self.profiler.get_statistics("project_read")
         
         return results
     
     def test_query_performance(self) -> Dict[str, Any]:
         """Test complex query performance."""
         test_queries = {
             "simple_select": "SELECT * FROM framework_projects LIMIT 100",
             "complex_join": """
                 SELECT p.name, COUNT(e.id) as epic_count 
                 FROM framework_projects p 
                 LEFT JOIN framework_epics e ON p.id = e.project_id 
                 GROUP BY p.id
             """,
             "aggregation": """
                 SELECT status, COUNT(*) as count, AVG(progress) as avg_progress
                 FROM framework_epics 
                 GROUP BY status
             """
         }
         
         results = {}
         for query_name, query_sql in test_queries.items():
             with self.profiler.profile_operation(f"query_{query_name}"):
-                with self.db_manager.get_connection() as conn:
+                with get_connection() as conn:  # Direct connection for performance testing
                     cursor = conn.cursor()
                     cursor.execute(query_sql)
                     rows = cursor.fetchall()
             
             results[query_name] = self.profiler.get_statistics(f"query_{query_name}")
             results[query_name]["rows_returned"] = len(rows)
         
         return results
 
+    def benchmark_hybrid_operations(self) -> Dict[str, Any]:
+        """Benchmark mix of legacy and modular operations."""
+        results = {}
+        with self.profiler.profile_operation("legacy_get_projects"):
+            self.db_manager.get_projects(limit=5)
+        with self.profiler.profile_operation("modular_list_epics"):
+            list_epics()
+        results["legacy_get_projects"] = self.profiler.get_statistics("legacy_get_projects")
+        results["modular_list_epics"] = self.profiler.get_statistics("modular_list_epics")
+        return results
+
+    def benchmark_modular_operations(self) -> Dict[str, Any]:
+        """Benchmark modular API operations."""
+        results = {}
+        with self.profiler.profile_operation("mod_list_epics"):
+            list_epics()
+        with self.profiler.profile_operation("mod_list_tasks"):
+            list_tasks(1)
+        results["mod_list_epics"] = self.profiler.get_statistics("mod_list_epics")
+        results["mod_list_tasks"] = self.profiler.get_statistics("mod_list_tasks")
+        return results
+
+    def benchmark_service_layer(self) -> Dict[str, Any]:
+        """Benchmark operations through the service layer."""
+        results = {}
+        epic_service = self.service_container.get_epic_service()
+        task_service = self.service_container.get_task_service()
+        with self.profiler.profile_operation("svc_list_epics"):
+            epic_service.list_epics()
+        with self.profiler.profile_operation("svc_list_tasks"):
+            task_service.list_tasks()
+        results["svc_list_epics"] = self.profiler.get_statistics("svc_list_epics")
+        results["svc_list_tasks"] = self.profiler.get_statistics("svc_list_tasks")
+        return results
+
 
 class LoadTester:
     """Multi-threaded load testing system."""
     
     def __init__(self, db_manager):
         self.db_manager = db_manager
         self.profiler = PerformanceProfiler()
         self.stop_event = threading.Event()
         
     def run_load_test(self, config: LoadTestConfig, target_function: Callable) -> Dict[str, Any]:
         """Execute load test with specified configuration."""
         self.stop_event.clear()
         
         # Prepare test data
         test_data = self._generate_test_data(config.test_data_size)
         
         # Start load test
         start_time = time.time()
         
         with ThreadPoolExecutor(max_workers=config.concurrent_users) as executor:
             # Submit tasks
             futures = []
             for i in range(config.concurrent_users):
                 future = executor.submit(
                     self._worker_thread,
@@ -548,34 +599,36 @@ def create_performance_test_suite(db_manager) -> Dict[str, Any]:
     logging.info("ðŸ“‹ Generating performance report...")
     report_file = reporter.generate_performance_report(results, "comprehensive_test")
     
     logging.info(f"âœ… Performance testing complete! Report saved to: {report_file}")
     
     return results
 
 
 # Example usage and integration functions
 def run_quick_performance_check(db_manager) -> Dict[str, Any]:
     """Run quick performance check for monitoring."""
     profiler = PerformanceProfiler()
     
     # Test basic operations
     with profiler.profile_operation("get_projects"):
         db_manager.get_projects(limit=10)
 
     return {
         "get_projects": profiler.get_statistics("get_projects"),
         "timestamp": datetime.datetime.now().isoformat()
     }
 
 
 if __name__ == "__main__":
     # Example usage
-    from streamlit_extension.utils.database import DatabaseManager
-    
-    db_manager = DatabaseManager("framework.db", "task_timer.db")
-    
+    db_manager = DatabaseManager("framework.db", "task_timer.db")  # Legacy fallback
+    # Performance testing with direct connection
+    performance_connection = get_connection()
+    # Service container for business logic testing  
+    service_container = ServiceContainer(db_manager)
+
     # Run comprehensive test suite
     results = create_performance_test_suite(db_manager)
-    
+
     logging.info("Performance testing completed!")
     logging.info(f"Results: {len(results)} test categories executed")
 
EOF
)