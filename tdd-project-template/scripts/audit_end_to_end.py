#!/usr/bin/env python3
"""
ğŸ”„ TDD Project Template - End-to-End User Workflow Audit
======================================================

Auditoria que simula um usuÃ¡rio real usando o template completo,
desde setup inicial atÃ© geraÃ§Ã£o de charts e deployment.

Uso:
    python scripts/audit_end_to_end.py
    python scripts/audit_end_to_end.py --verbose --full-simulation
"""

import argparse
import json
import os
import sys
import re
import subprocess
import tempfile
import shutil
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime, timedelta
import random


class EndToEndAuditor:
    """Auditor de workflow end-to-end do template TDD."""
    
    def __init__(self, verbose: bool = False, full_simulation: bool = False):
        self.verbose = verbose
        self.full_simulation = full_simulation
        self.project_root = Path.cwd()
        self.findings = []
        self.metrics = {}
        self.simulation_data = {}
        
    def log_finding(self, level: str, category: str, message: str, details: Optional[Dict] = None):
        """Log audit finding with structured data."""
        finding = {
            'timestamp': datetime.now().isoformat(),
            'level': level,
            'category': category,
            'message': message,
            'details': details or {}
        }
        self.findings.append(finding)
        
        icons = {'SUCCESS': 'âœ…', 'WARNING': 'âš ï¸', 'ERROR': 'âŒ', 'CRITICAL': 'ğŸ”¥', 'INFO': 'â„¹ï¸'}
        icon = icons.get(level, 'â„¹ï¸')
        print(f"{icon} [{category}] {message}")
        if self.verbose and details:
            for key, value in details.items():
                print(f"    {key}: {value}")
    
    def audit_template_initialization(self) -> Dict[str, Any]:
        """Audit template initialization and setup process."""
        print("\nğŸš€ AUDITING TEMPLATE INITIALIZATION")
        print("=" * 50)
        
        init_metrics = {
            'readme_accessibility': False,
            'setup_instructions_clear': False,
            'required_files_present': [],
            'configuration_templates': [],
            'example_data_quality': {}
        }
        
        init_score = 0
        
        # Check README accessibility and setup instructions
        readme_path = self.project_root / 'README.md'
        if readme_path.exists():
            init_metrics['readme_accessibility'] = True
            init_score += 20
            
            content = readme_path.read_text()
            
            # Check for clear setup instructions
            setup_indicators = [
                'install', 'setup', 'getting started', 'quick start',
                'dependencies', 'requirements', 'poetry install'
            ]
            
            found_setup_indicators = [indicator for indicator in setup_indicators
                                    if indicator.lower() in content.lower()]
            
            if len(found_setup_indicators) >= 4:
                init_metrics['setup_instructions_clear'] = True
                init_score += 25
                self.log_finding('SUCCESS', 'INIT', 
                               f"Clear setup instructions found: {', '.join(found_setup_indicators[:3])}")
            elif len(found_setup_indicators) >= 2:
                init_score += 15
                self.log_finding('WARNING', 'INIT', 
                               f"Basic setup instructions: {', '.join(found_setup_indicators)}")
            else:
                self.log_finding('ERROR', 'INIT', "Setup instructions unclear or missing")
            
            self.log_finding('SUCCESS', 'INIT', "README accessible for new users")
        else:
            self.log_finding('CRITICAL', 'INIT', "README.md missing - critical for new users")
        
        # Check for required files for new project setup
        required_files = {
            'pyproject.toml': 'Poetry dependency management',
            'requirements.txt': 'Pip fallback dependencies',
            '.gitignore': 'Git ignore patterns',
            'epics/epic_1.json': 'Example epic definition',
            'epics/template_epic.json': 'Epic template',
            'scripts/commit_helper.py': 'TDD commit helper',
            'scripts/test_setup.py': 'Setup validation script'
        }
        
        found_required = []
        for file_path, description in required_files.items():
            full_path = self.project_root / file_path
            if full_path.exists():
                found_required.append(file_path)
                init_score += 5
                self.log_finding('SUCCESS', 'INIT', f"Required file present: {file_path}")
            else:
                self.log_finding('WARNING', 'INIT', f"Missing required file: {file_path}")
        
        init_metrics['required_files_present'] = found_required
        
        # Check configuration templates
        config_templates = [
            'config/environment/.env.example',
            'docs/_config.yml',
            'docs/Gemfile'
        ]
        
        found_configs = []
        for config_path in config_templates:
            full_path = self.project_root / config_path
            if full_path.exists():
                found_configs.append(config_path)
                init_score += 5
        
        init_metrics['configuration_templates'] = found_configs
        
        if len(found_configs) >= 2:
            self.log_finding('SUCCESS', 'INIT', f"Configuration templates available: {len(found_configs)}")
        else:
            self.log_finding('WARNING', 'INIT', f"Limited configuration templates: {len(found_configs)}")
        
        # Check example data quality
        epic_1_path = self.project_root / 'epics' / 'epic_1.json'
        if epic_1_path.exists():
            try:
                with open(epic_1_path, 'r') as f:
                    epic_data = json.load(f)
                
                # Validate epic structure
                epic_quality = {
                    'has_id': 'epic_id' in epic_data,
                    'has_title': 'title' in epic_data and len(epic_data.get('title', '')) > 10,
                    'has_tasks': 'tasks' in epic_data and len(epic_data.get('tasks', [])) > 0,
                    'has_tdd_phases': False,
                    'realistic_data': epic_data.get('title', '') != 'Epic Title Here'
                }
                
                # Check TDD phases in tasks
                if epic_data.get('tasks'):
                    phases = [task.get('phase') for task in epic_data['tasks']]
                    tdd_phases = ['analysis', 'red', 'green', 'refactor']
                    found_phases = [phase for phase in tdd_phases if phase in phases]
                    epic_quality['has_tdd_phases'] = len(found_phases) >= 3
                
                quality_score = sum(1 for v in epic_quality.values() if v)
                if quality_score >= 4:
                    init_score += 15
                    self.log_finding('SUCCESS', 'INIT', f"High-quality example epic ({quality_score}/5)")
                elif quality_score >= 3:
                    init_score += 10
                    self.log_finding('WARNING', 'INIT', f"Decent example epic ({quality_score}/5)")
                else:
                    self.log_finding('ERROR', 'INIT', f"Poor example epic quality ({quality_score}/5)")
                
                init_metrics['example_data_quality'] = epic_quality
                
            except Exception as e:
                self.log_finding('ERROR', 'INIT', f"Failed to validate example epic: {str(e)}")
        
        self.metrics['template_initialization_score'] = init_score
        
        return {
            'score': init_score,
            'metrics': init_metrics
        }
    
    def audit_tdd_workflow_simulation(self) -> Dict[str, Any]:
        """Simulate complete TDD workflow from epic creation to commit."""
        print("\nğŸ”„ AUDITING TDD WORKFLOW SIMULATION")
        print("=" * 50)
        
        workflow_metrics = {
            'epic_creation': {},
            'commit_pattern_validation': {},
            'phase_progression': {},
            'tools_functionality': {}
        }
        
        workflow_score = 0
        
        # Simulate epic creation
        try:
            # Create a test epic based on template
            test_epic = {
                "epic_id": "TEST-E2E",
                "title": "End-to-End Audit Test Epic",
                "description": "Epic created during end-to-end testing to validate workflow",
                "section": "Quality Assurance",
                "priority": "high",
                "status": "active",
                "estimated_hours": 2.0,
                "planned_start": datetime.now().strftime("%Y-%m-%d"),
                "planned_end": (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d"),
                "assigned_developer": "e2e-auditor",
                "tags": ["test", "audit", "tdd"],
                "acceptance_criteria": [
                    "Epic can be created from template",
                    "TDD workflow tools function correctly",
                    "Commit patterns validate properly"
                ],
                "tasks": [
                    {
                        "task_id": "TEST-E2E.1",
                        "phase": "analysis",
                        "title": "Analyze audit requirements",
                        "description": "Define what needs to be tested in the workflow",
                        "estimated_time_minutes": 30,
                        "status": "completed",
                        "type": "docs"
                    },
                    {
                        "task_id": "TEST-E2E.2",
                        "phase": "red",
                        "title": "Write audit validation tests",
                        "description": "Create tests that validate the audit workflow",
                        "estimated_time_minutes": 45,
                        "status": "in_progress", 
                        "type": "test"
                    },
                    {
                        "task_id": "TEST-E2E.3",
                        "phase": "green",
                        "title": "Implement audit functionality",
                        "description": "Build the actual audit implementation",
                        "estimated_time_minutes": 60,
                        "status": "pending",
                        "type": "feat"
                    },
                    {
                        "task_id": "TEST-E2E.4",
                        "phase": "refactor",
                        "title": "Optimize audit code",
                        "description": "Clean up and optimize the audit implementation",
                        "estimated_time_minutes": 30,
                        "status": "pending",
                        "type": "refactor"
                    }
                ],
                "dependencies": [],
                "blockers": [],
                "tdd_metrics": {
                    "total_estimated_minutes": 165,
                    "phases_distribution": {
                        "analysis": 30,
                        "red": 45,
                        "green": 60,
                        "refactor": 30
                    },
                    "expected_tdd_cycles": 1,
                    "test_coverage_target": 100
                },
                "github_integration": {
                    "repository": "test-repo/e2e-audit",
                    "milestone": "E2E Testing",
                    "labels": ["epic-test-e2e", "audit"],
                    "project_board": "E2E Testing Board"
                },
                "notes": [
                    "This epic was created during end-to-end testing",
                    "Should validate all TDD workflow functionality"
                ],
                "created_date": datetime.now().isoformat(),
                "updated_date": datetime.now().isoformat(),
                "version": "1.0"
            }
            
            # Write test epic to temporary file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(test_epic, f, indent=2)
                test_epic_path = f.name
            
            workflow_metrics['epic_creation']['test_epic_created'] = True
            workflow_score += 20
            self.log_finding('SUCCESS', 'WORKFLOW', "Test epic created successfully")
            
            # Validate epic structure
            try:
                with open(test_epic_path, 'r') as f:
                    loaded_epic = json.load(f)
                
                if loaded_epic['epic_id'] == 'TEST-E2E' and len(loaded_epic['tasks']) == 4:
                    workflow_metrics['epic_creation']['validation_passed'] = True
                    workflow_score += 15
                    self.log_finding('SUCCESS', 'WORKFLOW', "Epic structure validation passed")
                
            except Exception as e:
                self.log_finding('ERROR', 'WORKFLOW', f"Epic validation failed: {str(e)}")
            
            finally:
                os.unlink(test_epic_path)
            
        except Exception as e:
            self.log_finding('ERROR', 'WORKFLOW', f"Epic creation failed: {str(e)}")
        
        # Test commit pattern validation with realistic scenarios
        commit_helper_path = self.project_root / 'scripts' / 'commit_helper.py'
        if commit_helper_path.exists():
            test_commits = [
                {
                    'message': '[EPIC-TEST] analysis: docs: analyze requirements [Task TEST.1 | 30min]',
                    'expected_valid': True,
                    'phase': 'analysis'
                },
                {
                    'message': '[EPIC-TEST] red: test: add validation tests [Task TEST.2 | 45min]',
                    'expected_valid': True,
                    'phase': 'red'
                },
                {
                    'message': '[EPIC-TEST] green: feat: implement feature [Task TEST.3 | 60min]',
                    'expected_valid': True,
                    'phase': 'green'
                },
                {
                    'message': '[EPIC-TEST] refactor: refactor: optimize code [Task TEST.4 | 30min]',
                    'expected_valid': True,
                    'phase': 'refactor'
                },
                {
                    'message': 'invalid commit message without pattern',
                    'expected_valid': False,
                    'phase': 'invalid'
                }
            ]
            
            validation_results = []
            for commit_test in test_commits:
                try:
                    result = subprocess.run([
                        sys.executable, str(commit_helper_path), 
                        '--validate', commit_test['message']
                    ], capture_output=True, text=True, timeout=10, cwd=self.project_root)
                    
                    is_valid = result.returncode == 0 and 'Valid' in result.stdout
                    expected = commit_test['expected_valid']
                    
                    test_result = {
                        'message': commit_test['message'],
                        'phase': commit_test['phase'],
                        'expected_valid': expected,
                        'actual_valid': is_valid,
                        'correct_validation': is_valid == expected
                    }
                    
                    validation_results.append(test_result)
                    
                    if test_result['correct_validation']:
                        workflow_score += 10
                        self.log_finding('SUCCESS', 'WORKFLOW', 
                                       f"Correct validation for {commit_test['phase']} phase")
                    else:
                        self.log_finding('ERROR', 'WORKFLOW',
                                       f"Incorrect validation for {commit_test['phase']} phase")
                    
                except subprocess.TimeoutExpired:
                    self.log_finding('ERROR', 'WORKFLOW', f"Validation timeout for {commit_test['phase']}")
                except Exception as e:
                    self.log_finding('ERROR', 'WORKFLOW', 
                                   f"Validation error for {commit_test['phase']}: {str(e)}")
            
            workflow_metrics['commit_pattern_validation']['test_results'] = validation_results
            
            # Calculate validation success rate
            correct_validations = sum(1 for r in validation_results if r['correct_validation'])
            validation_rate = (correct_validations / len(validation_results)) * 100 if validation_results else 0
            
            if validation_rate >= 80:
                workflow_score += 15
                self.log_finding('SUCCESS', 'WORKFLOW', f"Excellent validation accuracy: {validation_rate:.1f}%")
            elif validation_rate >= 60:
                workflow_score += 10
                self.log_finding('WARNING', 'WORKFLOW', f"Good validation accuracy: {validation_rate:.1f}%")
            else:
                self.log_finding('ERROR', 'WORKFLOW', f"Poor validation accuracy: {validation_rate:.1f}%")
        
        else:
            self.log_finding('ERROR', 'WORKFLOW', "Commit helper not available for testing")
        
        # Test TDD phase progression logic
        phase_progression_tests = [
            {'from': 'analysis', 'to': 'red', 'valid': True},
            {'from': 'red', 'to': 'green', 'valid': True},
            {'from': 'green', 'to': 'refactor', 'valid': True},
            {'from': 'refactor', 'to': 'red', 'valid': True},  # New cycle
            {'from': 'green', 'to': 'red', 'valid': False},  # Backwards not recommended
        ]
        
        progression_score = 0
        for test in phase_progression_tests:
            # This is a logical test - in real implementation, 
            # the tool should guide proper phase transitions
            if test['valid']:
                progression_score += 1
        
        workflow_metrics['phase_progression']['tests'] = phase_progression_tests
        workflow_score += progression_score * 5  # 5 points per valid progression
        
        # Test tools functionality
        tool_tests = [
            {'tool': 'commit_helper.py', 'test': '--help'},
            {'tool': 'commit_helper.py', 'test': '--guide'},
            {'tool': 'test_setup.py', 'test': None}
        ]
        
        working_tools = 0
        for tool_test in tool_tests:
            tool_path = self.project_root / 'scripts' / tool_test['tool']
            if tool_path.exists():
                try:
                    cmd = [sys.executable, str(tool_path)]
                    if tool_test['test']:
                        cmd.append(tool_test['test'])
                    
                    result = subprocess.run(cmd, capture_output=True, text=True, 
                                          timeout=15, cwd=self.project_root)
                    
                    if result.returncode in [0, 1]:  # 0 = success, 1 = acceptable with warnings
                        working_tools += 1
                        workflow_score += 10
                        self.log_finding('SUCCESS', 'WORKFLOW', f"Tool functional: {tool_test['tool']}")
                    else:
                        self.log_finding('ERROR', 'WORKFLOW', 
                                       f"Tool failed: {tool_test['tool']} - {result.returncode}")
                        
                except subprocess.TimeoutExpired:
                    self.log_finding('ERROR', 'WORKFLOW', f"Tool timeout: {tool_test['tool']}")
                except Exception as e:
                    self.log_finding('ERROR', 'WORKFLOW', f"Tool error: {tool_test['tool']} - {str(e)}")
            else:
                self.log_finding('WARNING', 'WORKFLOW', f"Tool missing: {tool_test['tool']}")
        
        workflow_metrics['tools_functionality']['working_tools'] = working_tools
        workflow_metrics['tools_functionality']['total_tools'] = len(tool_tests)
        
        self.metrics['tdd_workflow_score'] = workflow_score
        
        return {
            'score': workflow_score,
            'metrics': workflow_metrics
        }
    
    def audit_chart_generation_workflow(self) -> Dict[str, Any]:
        """Audit chart generation workflow with simulated data."""
        print("\nğŸ“Š AUDITING CHART GENERATION WORKFLOW")
        print("=" * 50)
        
        chart_metrics = {
            'gantt_script_availability': False,
            'sample_data_generation': False,
            'chart_generation_success': False,
            'output_file_creation': False
        }
        
        chart_score = 0
        
        # Check if Gantt tracker is available
        gantt_path = self.project_root / 'scripts' / 'visualization' / 'tdd_gantt_tracker.py'
        if gantt_path.exists():
            chart_metrics['gantt_script_availability'] = True
            chart_score += 25
            self.log_finding('SUCCESS', 'CHARTS', "Gantt tracker script available")
            
            # Test with simulated commit data
            if self.full_simulation:
                try:
                    # Create simulated git history
                    simulated_commits = [
                        "[EPIC-1] analysis: docs: analyze user requirements [Task 1.1 | 30min]",
                        "[EPIC-1] red: test: add user login tests [Task 1.2 | 45min]", 
                        "[EPIC-1] green: feat: implement user login [Task 1.3 | 60min]",
                        "[EPIC-1] refactor: refactor: extract auth utilities [Task 1.4 | 25min]",
                        "[EPIC-1] red: test: add password validation tests [Task 1.5 | 30min]",
                        "[EPIC-1] green: feat: implement password validation [Task 1.6 | 40min]",
                    ]
                    
                    # Create temporary git history simulation
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                        for i, commit_msg in enumerate(simulated_commits):
                            timestamp = datetime.now() - timedelta(hours=len(simulated_commits) - i)
                            f.write(f"{timestamp.isoformat()} {commit_msg}\n")
                        
                        commit_data_path = f.name
                    
                    chart_metrics['sample_data_generation'] = True
                    chart_score += 20
                    self.log_finding('SUCCESS', 'CHARTS', "Sample commit data generated")
                    
                    # Test chart generation (would need actual implementation)
                    # For now, test that the script can be executed
                    try:
                        result = subprocess.run([
                            sys.executable, str(gantt_path), '--help'
                        ], capture_output=True, text=True, timeout=30, cwd=self.project_root)
                        
                        if result.returncode in [0, 1]:  # May fail due to missing deps, but script loads
                            if 'ModuleNotFoundError' not in result.stderr:
                                chart_metrics['chart_generation_success'] = True
                                chart_score += 25
                                self.log_finding('SUCCESS', 'CHARTS', "Chart generation script functional")
                            else:
                                self.log_finding('WARNING', 'CHARTS', 
                                               "Chart generation script has dependency issues (expected)")
                                chart_score += 10
                        else:
                            self.log_finding('ERROR', 'CHARTS', 
                                           f"Chart generation script failed: {result.returncode}")
                        
                    except Exception as e:
                        self.log_finding('ERROR', 'CHARTS', f"Chart generation test failed: {str(e)}")
                    
                    finally:
                        os.unlink(commit_data_path)
                        
                except Exception as e:
                    self.log_finding('ERROR', 'CHARTS', f"Chart workflow simulation failed: {str(e)}")
            
            else:
                # Basic test without full simulation
                try:
                    result = subprocess.run([
                        sys.executable, str(gantt_path), '--help'
                    ], capture_output=True, text=True, timeout=15, cwd=self.project_root)
                    
                    if result.returncode in [0, 1]:
                        chart_score += 15
                        self.log_finding('SUCCESS', 'CHARTS', "Chart script loads successfully")
                    
                except Exception as e:
                    self.log_finding('WARNING', 'CHARTS', f"Basic chart test failed: {str(e)}")
        
        else:
            self.log_finding('ERROR', 'CHARTS', "Gantt tracker script missing")
        
        # Check for chart output locations
        expected_output_locations = [
            'docs/tdd_gantt_progress.html',
            'docs/dashboard.html'
        ]
        
        output_files_found = 0
        for output_path in expected_output_locations:
            full_path = self.project_root / output_path
            if full_path.exists():
                output_files_found += 1
                chart_score += 10
                
                # Check file size (should be substantial if it contains charts)
                file_size = full_path.stat().st_size
                if file_size > 5000:  # > 5KB indicates substantial content
                    chart_score += 5
                    self.log_finding('SUCCESS', 'CHARTS', f"Chart output file substantial: {output_path}")
                else:
                    self.log_finding('WARNING', 'CHARTS', f"Chart output file minimal: {output_path}")
            else:
                self.log_finding('WARNING', 'CHARTS', f"Expected chart output missing: {output_path}")
        
        if output_files_found >= 1:
            chart_metrics['output_file_creation'] = True
            self.log_finding('SUCCESS', 'CHARTS', f"Chart output files present: {output_files_found}")
        
        self.metrics['chart_generation_score'] = chart_score
        
        return {
            'score': chart_score,
            'metrics': chart_metrics
        }
    
    def audit_user_onboarding_experience(self) -> Dict[str, Any]:
        """Audit the complete user onboarding experience."""
        print("\nğŸ‘¥ AUDITING USER ONBOARDING EXPERIENCE")
        print("=" * 50)
        
        onboarding_metrics = {
            'documentation_clarity': {},
            'setup_process_smoothness': {},
            'first_success_achievability': {},
            'error_recovery_guidance': {}
        }
        
        onboarding_score = 0
        
        # Evaluate documentation clarity for new users
        readme_path = self.project_root / 'README.md'
        if readme_path.exists():
            content = readme_path.read_text()
            
            clarity_indicators = {
                'has_quick_start': 'quick start' in content.lower(),
                'has_step_by_step': re.search(r'^\s*\d+\.\s', content, re.MULTILINE) is not None,
                'has_code_examples': content.count('```') >= 5,
                'has_prerequisites': any(word in content.lower() for word in ['prerequisite', 'requirement', 'need']),
                'has_troubleshooting': any(word in content.lower() for word in ['troubleshoot', 'problem', 'issue', 'error']),
                'visual_appeal': len(re.findall(r'[ğŸ¯ğŸ“ŠğŸš€âœ¨ğŸ§ªğŸ“ˆğŸ¤–]', content)) >= 5
            }
            
            clarity_score = sum(1 for v in clarity_indicators.values() if v)
            onboarding_metrics['documentation_clarity'] = clarity_indicators
            
            if clarity_score >= 5:
                onboarding_score += 30
                self.log_finding('SUCCESS', 'ONBOARDING', f"Excellent documentation clarity ({clarity_score}/6)")
            elif clarity_score >= 3:
                onboarding_score += 20
                self.log_finding('WARNING', 'ONBOARDING', f"Good documentation clarity ({clarity_score}/6)")
            else:
                onboarding_score += 10
                self.log_finding('ERROR', 'ONBOARDING', f"Poor documentation clarity ({clarity_score}/6)")
        
        # Test setup process smoothness
        setup_files = {
            'pyproject.toml': 'Dependency management config present',
            'requirements.txt': 'Fallback dependency list present',
            'scripts/test_setup.py': 'Setup validation tool present',
            '.gitignore': 'Git ignore configuration present'
        }
        
        setup_readiness = 0
        for setup_file, description in setup_files.items():
            if (self.project_root / setup_file).exists():
                setup_readiness += 1
                onboarding_score += 5
                self.log_finding('SUCCESS', 'ONBOARDING', description)
            else:
                self.log_finding('WARNING', 'ONBOARDING', f"Missing setup file: {setup_file}")
        
        onboarding_metrics['setup_process_smoothness']['files_present'] = setup_readiness
        onboarding_metrics['setup_process_smoothness']['total_files'] = len(setup_files)
        
        # Test first success achievability
        # Can a user quickly get a "win" with the template?
        first_success_indicators = []
        
        # Check if there's a working example they can run immediately
        if (self.project_root / 'epics' / 'epic_1.json').exists():
            first_success_indicators.append('working_example_epic')
            onboarding_score += 10
        
        # Check if commit helper provides immediate feedback
        commit_helper_path = self.project_root / 'scripts' / 'commit_helper.py'
        if commit_helper_path.exists():
            try:
                result = subprocess.run([
                    sys.executable, str(commit_helper_path), '--guide'
                ], capture_output=True, text=True, timeout=10, cwd=self.project_root)
                
                if result.returncode == 0 and len(result.stdout) > 200:
                    first_success_indicators.append('helpful_guide_command')
                    onboarding_score += 15
                    self.log_finding('SUCCESS', 'ONBOARDING', "Commit helper provides helpful guidance")
                
            except Exception:
                pass
        
        # Check if setup validation gives clear feedback
        test_setup_path = self.project_root / 'scripts' / 'test_setup.py'
        if test_setup_path.exists():
            try:
                result = subprocess.run([
                    sys.executable, str(test_setup_path)
                ], capture_output=True, text=True, timeout=20, cwd=self.project_root)
                
                if result.returncode in [0, 1]:  # Success or warnings acceptable
                    output = result.stdout + result.stderr
                    if any(word in output.lower() for word in ['success', 'working', 'ready', 'ok']):
                        first_success_indicators.append('positive_setup_feedback')
                        onboarding_score += 15
                        self.log_finding('SUCCESS', 'ONBOARDING', "Setup validation provides positive feedback")
                
            except Exception:
                pass
        
        onboarding_metrics['first_success_achievability']['indicators'] = first_success_indicators
        
        if len(first_success_indicators) >= 2:
            self.log_finding('SUCCESS', 'ONBOARDING', "Multiple paths to first success")
        elif len(first_success_indicators) >= 1:
            self.log_finding('WARNING', 'ONBOARDING', "Limited paths to first success")
        else:
            self.log_finding('ERROR', 'ONBOARDING', "No clear path to first success")
        
        # Test error recovery guidance
        error_guidance_score = 0
        
        # Check for troubleshooting documentation
        troubleshooting_files = [
            'TROUBLESHOOTING.md',
            'docs/troubleshooting.md',
            'docs/github_pages_setup_guide.md'
        ]
        
        found_troubleshooting = 0
        for trouble_file in troubleshooting_files:
            if (self.project_root / trouble_file).exists():
                found_troubleshooting += 1
                error_guidance_score += 10
        
        # Check if tools provide helpful error messages
        try:
            # Test commit helper with invalid input
            result = subprocess.run([
                sys.executable, str(commit_helper_path), '--validate', 'invalid message'
            ], capture_output=True, text=True, timeout=10, cwd=self.project_root)
            
            if result.returncode != 0:  # Should fail
                error_output = result.stdout + result.stderr
                helpful_indicators = [
                    'should', 'try', 'example', 'format', 'pattern', 'help'
                ]
                found_help = sum(1 for indicator in helpful_indicators 
                               if indicator in error_output.lower())
                
                if found_help >= 2:
                    error_guidance_score += 15
                    self.log_finding('SUCCESS', 'ONBOARDING', "Tools provide helpful error messages")
                elif found_help >= 1:
                    error_guidance_score += 10
                    self.log_finding('WARNING', 'ONBOARDING', "Tools provide basic error guidance")
                else:
                    self.log_finding('ERROR', 'ONBOARDING', "Tools provide poor error guidance")
                    
        except Exception:
            pass
        
        onboarding_metrics['error_recovery_guidance']['troubleshooting_files'] = found_troubleshooting
        onboarding_metrics['error_recovery_guidance']['score'] = error_guidance_score
        
        onboarding_score += error_guidance_score
        
        self.metrics['user_onboarding_score'] = onboarding_score
        
        return {
            'score': onboarding_score,
            'metrics': onboarding_metrics
        }
    
    def generate_end_to_end_report(self) -> Dict[str, Any]:
        """Generate comprehensive end-to-end audit report."""
        print("\nğŸ“Š GENERATING END-TO-END REPORT")
        print("=" * 50)
        
        # Calculate overall end-to-end score
        e2e_weights = {
            'template_initialization_score': 0.25,
            'tdd_workflow_score': 0.35,
            'chart_generation_score': 0.20,
            'user_onboarding_score': 0.20
        }
        
        overall_score = 0
        for metric, weight in e2e_weights.items():
            score = self.metrics.get(metric, 0)
            overall_score += score * weight
        
        # Categorize findings
        critical_issues = [f for f in self.findings if f['level'] == 'CRITICAL']
        errors = [f for f in self.findings if f['level'] == 'ERROR']
        warnings = [f for f in self.findings if f['level'] == 'WARNING']
        successes = [f for f in self.findings if f['level'] == 'SUCCESS']
        
        report = {
            'audit_timestamp': datetime.now().isoformat(),
            'overall_score': round(overall_score, 2),
            'grade': self._calculate_grade(overall_score),
            'user_readiness': self._calculate_user_readiness(overall_score),
            'weighted_scores': {k: round(self.metrics.get(k, 0), 2) for k in e2e_weights.keys()},
            'weights_used': e2e_weights,
            'metrics': self.metrics,
            'simulation_data': self.simulation_data,
            'summary': {
                'total_findings': len(self.findings),
                'critical_issues': len(critical_issues),
                'errors': len(errors),
                'warnings': len(warnings),
                'successes': len(successes)
            },
            'findings': self.findings,
            'recommendations': self._generate_e2e_recommendations()
        }
        
        # Console summary
        grade_colors = {
            'A+': 'ğŸŸ¢', 'A': 'ğŸŸ¢', 'B+': 'ğŸ”µ', 'B': 'ğŸ”µ', 
            'C+': 'ğŸŸ¡', 'C': 'ğŸŸ¡', 'D': 'ğŸŸ ', 'F': 'ğŸ”´'
        }
        readiness_colors = {'USER_READY': 'ğŸŸ¢', 'MOSTLY_READY': 'ğŸ”µ', 'NEEDS_IMPROVEMENT': 'ğŸŸ¡', 'NOT_READY': 'ğŸ”´'}
        
        grade_color = grade_colors.get(report['grade'], 'âšª')
        readiness_color = readiness_colors.get(report['user_readiness'], 'âšª')
        
        print(f"\n{grade_color} OVERALL E2E GRADE: {report['grade']} ({overall_score:.1f}%)")
        print(f"{readiness_color} USER READINESS: {report['user_readiness']}")
        print(f"ğŸ“Š Component Scores:")
        for component, score in report['weighted_scores'].items():
            component_name = component.replace('_score', '').replace('_', ' ').title()
            print(f"    {component_name}: {score}%")
        print(f"ğŸ“‹ Total Findings: {len(self.findings)}")
        print(f"ğŸ”¥ Critical: {len(critical_issues)}")
        print(f"âŒ Errors: {len(errors)}")
        print(f"âš ï¸ Warnings: {len(warnings)}")
        print(f"âœ… Successes: {len(successes)}")
        
        return report
    
    def _calculate_grade(self, score: float) -> str:
        """Calculate letter grade based on score."""
        if score >= 95: return 'A+'
        elif score >= 90: return 'A'
        elif score >= 87: return 'B+'
        elif score >= 83: return 'B'
        elif score >= 80: return 'C+'
        elif score >= 75: return 'C'
        elif score >= 70: return 'D'
        else: return 'F'
    
    def _calculate_user_readiness(self, score: float) -> str:
        """Calculate user readiness level."""
        critical_issues = len([f for f in self.findings if f['level'] == 'CRITICAL'])
        
        if critical_issues > 0:
            return 'NOT_READY'
        elif score >= 85:
            return 'USER_READY'
        elif score >= 70:
            return 'MOSTLY_READY'
        else:
            return 'NEEDS_IMPROVEMENT'
    
    def _generate_e2e_recommendations(self) -> List[str]:
        """Generate actionable end-to-end recommendations."""
        recommendations = []
        
        critical_count = len([f for f in self.findings if f['level'] == 'CRITICAL'])
        if critical_count > 0:
            recommendations.append(f"ğŸ”¥ CRITICAL: Fix {critical_count} critical user experience issues")
        
        if self.metrics.get('template_initialization_score', 0) < 70:
            recommendations.append("ğŸš€ HIGH: Improve template initialization and setup documentation")
        
        if self.metrics.get('tdd_workflow_score', 0) < 70:
            recommendations.append("ğŸ”„ HIGH: Fix TDD workflow tools and commit pattern validation")
        
        if self.metrics.get('chart_generation_score', 0) < 60:
            recommendations.append("ğŸ“Š MEDIUM: Improve chart generation workflow and dependencies")
        
        if self.metrics.get('user_onboarding_score', 0) < 70:
            recommendations.append("ğŸ‘¥ HIGH: Enhance user onboarding experience and first success paths")
        
        error_count = len([f for f in self.findings if f['level'] == 'ERROR'])
        if error_count >= 5:
            recommendations.append(f"âŒ MEDIUM: Address {error_count} workflow errors")
        
        if not recommendations:
            recommendations.append("ğŸ‰ Excellent! Template provides smooth end-to-end user experience")
        
        return recommendations


def main():
    """Main end-to-end audit execution."""
    parser = argparse.ArgumentParser(description="ğŸ”„ TDD Template End-to-End User Workflow Audit")
    parser.add_argument('--verbose', '-v', action='store_true', 
                       help='Verbose output with detailed findings')
    parser.add_argument('--full-simulation', action='store_true',
                       help='Run full user workflow simulation (slower but comprehensive)')
    parser.add_argument('--report', choices=['console', 'json', 'both'], 
                       default='console', help='Report output format')
    parser.add_argument('--output', '-o', help='Output file for JSON report')
    
    args = parser.parse_args()
    
    print("ğŸ”„ TDD PROJECT TEMPLATE - END-TO-END USER WORKFLOW AUDIT")
    print("=" * 65)
    print(f"ğŸ• Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“‚ Project: {Path.cwd()}")
    print(f"ğŸ” Full Simulation: {'YES' if args.full_simulation else 'NO'}")
    
    auditor = EndToEndAuditor(verbose=args.verbose, full_simulation=args.full_simulation)
    
    try:
        # Run all end-to-end audits
        init_results = auditor.audit_template_initialization()
        workflow_results = auditor.audit_tdd_workflow_simulation()
        chart_results = auditor.audit_chart_generation_workflow()
        onboarding_results = auditor.audit_user_onboarding_experience()
        
        # Generate final report
        report = auditor.generate_end_to_end_report()
        
        # Output report
        if args.report in ['json', 'both']:
            output_file = args.output or f'end_to_end_audit_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
            with open(output_file, 'w') as f:
                json.dump(report, f, indent=2, default=str)
            print(f"\nğŸ’¾ JSON report saved: {output_file}")
        
        # Return exit code based on user readiness
        readiness_codes = {'USER_READY': 0, 'MOSTLY_READY': 1, 'NEEDS_IMPROVEMENT': 2, 'NOT_READY': 3}
        return readiness_codes.get(report['user_readiness'], 3)
        
    except KeyboardInterrupt:
        print("\nâŒ End-to-end audit cancelled by user")
        return 1
    except Exception as e:
        print(f"âŒ End-to-end audit failed: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())